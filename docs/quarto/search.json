[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Spatio-Temporal Statistics with R (1st edition)",
    "section": "",
    "text": "Preface\nWe live in a complex world, and clever people are continually coming up with new ways to observe and record increasingly large parts of it so we can comprehend it better (warts and all!). We are squarely in the midst of a “big data” era, and it seems that every day new methodologies and algorithms emerge that are designed to deal with the ever-increasing size of these data streams.\nIt so happens that the “big data” available to us are often spatio-temporal data. That is, they can be indexed by spatial locations and time stamps. The space might be geographic space, or socio-economic space, or more generally network space, and the time scales might range from microseconds to millennia. Although scientists have long been interested in spatio-temporal data (e.g., Kepler’s studies based on planetary observations several centuries ago), it is only relatively recently that statisticians have taken a keen interest in the topic. At the risk of two of us being found guilty of self-promotion, we believe that the book Statistics for Spatio-Temporal Data by Cressie & Wikle (2011) was perhaps the first dedicated and comprehensive statistical monograph on the topic. In the decade (almost) since the publication of that book, there has been an exponential increase in the number of papers dealing with spatio-temporal data analysis – not only in statistics, but also in many other branches of science. Although Cressie & Wikle (2011) is still extremely relevant, it was intended for a fairly advanced, technically trained audience, and it did not include software or coding examples. In contrast, the present book provides a more accessible introduction, with hands-on applications of the methods through the use of R Labs at the end of each chapter. At the time of writing, this unique aspect of the book fills a void in the literature that can provide a bridge for students and researchers alike who wish to learn the basics of spatio-temporal statistics.\nWhat level is expected of readers of this book? First, although each chapter is fairly self-contained and they can be read in any order, we ordered the book deliberately to “ease” the reader into more technical material in later chapters. Spatio-temporal data can be complex, and their representations in terms of mathematical and statistical models can be complex as well. They require a number of indices (e.g., for space, for time, for multiple variables). In addition, being able to account for dependent random processes requires a bit of statistical sophistication that cannot be completely avoided, even in an applications-based introductory book. We believe that a reader who has taken a class or two in calculus-based probability and inference, and who is comfortable with basic matrix-algebra representations of statistical models (e.g., a multiple regression or a multivariate time-series representation), could comfortably get through this book. For those who would like a brief refresher on matrix algebra, we provide an overview of the components that we use in an appendix. To make this a bit easier on readers with just a few statistics courses on their transcript, we have interspersed “technical notes” throughout the book that provide short, gentle reviews of methods and ideas from the broader statistical literature.\nChapter 1 is the place to start, to get you intrigued and perhaps even excited about what is to come. We organized the rest of the book to follow what we believe to be good statistical practice. First, look at your data and do exploratory analyses (Chapter 2), then fit simple statistical models to the data to indicate possible patterns and see if assumptions are violated (Chapter 3), and then use what you learned in these analyses to build a spatio-temporal model that allows valid inferences (Chapters 4 and 5). The end of the cycle is to evaluate your model formally to find areas of improvement and to help choose the best model possible (Chapter 6). Then, if needed, repeat with a better-informed spatio-temporal model.\nThe bulk of the material on spatio-temporal modeling appears in Chapters 4 and 5. Chapter 4 covers descriptive (marginal) models formed by characterizing the spatio-temporal dependence structure (mainly through spatio-temporal covariances), which in turn leads to models that are analogous to the ubiquitous geostatistical models used in kriging. Chapter 5 focuses on dynamic (conditional) models that characterize the dynamic evolution of spatial processes through time, analogous to multivariate time-series models. Like Cressie & Wikle (2011), both Chapters 4 and 5 are firmly rooted in the notion of hierarchical thinking (i.e., hierarchical statistical modeling), which makes a clear distinction between the data and the underlying latent process of interest. This is based on the very practical notion that “[w]hat you see (data) is not always what you want to get (process)” Cressie & Wikle (2011), p. xvi.\nSpatio-temporal statistics is such a vast field and this modestly sized book is necessarily not comprehensive. For example, we focus primarily on data whose spatial reference is a point, and we do not explore issues related to the “change-of-support” problem, nor do we deal with spatio-temporal point processes. Further, we mostly limit our discussion to models and methodologies that are relatively mature, understood, and widely used. Some of the applications our readers are confronted with will undoubtedly require cutting-edge methods beyond the scope of this book. In that regard, the book provides a down-to-earth introduction. We hope you find that the path is wide and the slope is gentle, ultimately giving you the confidence to explore the literature for new developments. For this reason, we have named our epilogical chapter Pergimus, Latin for “let us continue to progress.”\nA substantial portion of this book is devoted to “Labs,” which enable the reader to put his or her understanding into practice using the programming language R. There are several reasons why we chose R: it is one of the most versatile languages designed for statistics; it is open source; it enjoys a vibrant online community whose members post solutions to virtually any problem you will encounter when coding; and, most importantly, a large number of packages that can be used for spatio-temporal modeling, exploratory data analysis, and statistical inference (estimation, prediction, uncertainty quantification, and so forth) are written in R. The last point is crucial, as it was our aim right from the beginning to make use of as much tried-and-tested code as possible to reduce the analyst’s barrier to entry. Indeed, it is fair to say that this book would not have been possible without the excellent work, openness, and generosity of the R community as a whole.\nIn presenting the Labs, we intentionally use a “code-after-methodology” approach, since we firmly believe that the reader should have an understanding of the statistical methods being used before delving into the computational details. To facilitate the connections between methodology and computation, we have added “R Tips” where needed. The Labs themselves assume some prior knowledge of R and, in particular, of the tidyverse, which is built on an underlying philosophy of how to deal with data and graphics. Readers who would like to know more can consult the excellent book by Wickham & Grolemund (2016) for background reading (freely available online).\nFinally, our goal when we started this project was to help as many people as we could to start analyzing spatio-temporal data. Consequently, with the generous support of our editors at Chapman & Hall/CRC, we have made the .pdf file of this book and the accompanying R package, STRbook, freely available for download from the website listed below. In addition, this website is a place where users can post errata, comment on the code examples, post their own code for different problems, their own spatio-temporal data sets, and articles on spatio-temporal statistics. You are invited to go to:\n\n\n\n\nCressie, N., & Wikle, C. K. (2011). Statistics for spatio-temporal data. John Wiley & Sons.\n\n\nWickham, H., & Grolemund, G. (2016). R for data science: Import, tidy, transform, visualize, and model data. O’Reilly Media.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "Chapter1.html",
    "href": "Chapter1.html",
    "title": "1  Introduction to Spatio-Temporal Statistics",
    "section": "",
    "text": "1.1 Why Should Spatio-Temporal Models Be Statistical?\nWynn Bullock was an early pioneer of modern photography, and this quote captures the essence of what we are trying to get across in our book — except in our case the “photographs” are fuzzy and the pictures are incomplete! The top panel of Figure 1.1 shows the July 2014 launch of the US National Aeronautics and Space Administration (NASA) Orbiting Carbon Observatory-2 (OCO-2) satellite, and the bottom panel shows the photographer in action. OCO-2 reached orbit successfully and, at the time of writing, is taking pictures of the dynamic world below. They are taken every fraction of a second, and each “photograph” is made up of measurements of the sun’s energy in selected spectral bands, reflected from Earth’s surface.\nAfter NASA processes these measurements, an estimate is obtained of the fraction of carbon dioxide (CO\\(_2\\)) molecules in an atmospheric column between Earth’s surface and the OCO-2 satellite. The top panel of Figure 1.2 shows these estimates in the boreal winter at locations determined by the geometry of the satellite’s 16-day repeat cycle (the time interval after which the satellite retraces its orbital path). (They are color-coded according to their value in units of parts per million, or ppm.) Plainly, there are gaps caused by OCO-2’s orbit geometry, and notice that the higher northern latitudes have very few data (caused by the sun’s low angle at that time of the year). The bottom panel of Figure 1.2 shows 16 days of OCO-2 data obtained six months later, in the boreal summer, where the same comments about coverage apply, except that now the higher southern latitudes have very few data. Data incompleteness here is a moving target in both space and time. Furthermore, any color-coded “dot” on the map represents a datum that should not be totally believed, since it is an estimate obtained from measurements made through 700 km of atmosphere with clouds, water vapor, and dust getting in the way. That is, there is “noise” in the data.\nThere is a “+” on the global maps shown in Figure 1.2, which marks the location of the Mauna Loa volcano, Hawaii. Near the top of this volcano, at an altitude of 4.17 km, is the US National Oceanic and Atmospheric Administration (NOAA) Mauna Loa Observatory, which has been taking monthly measurements of CO\\(_2\\) since the late 1950s. The data are shown as a time series in Figure 1.3. Now, for the moment, put aside issues associated with measurements being taken with different instruments, on different parcels of air, at different locations, and for different blocks of time; these can be dealt with using quite advanced spatio-temporal statistical methodology found in, for example, Cressie & Wikle (2011). What is fundamental here is that underlying these imperfect observations is a spatio-temporal process that itself is not perfectly understood, and we propose to capture this uncertainty in the process with a spatio-temporal statistical model.\nThe atmospheric CO\\(_2\\) process varies in space and in time, but the extent of its spatio-temporal domain means that exhaustive measurement of it is not possible; and even if it were possible, it would not be a good use of resources (a conclusion you should find evident after reading our book). This is a generic problem in spatio-temporal statistics—our noisy data traverse different paths through the “space-time cube”, but we want to gain knowledge about unobserved (and even observed) parts of it. We shall address this problem in the chapters, the Labs, and the technical notes that follow, drawing on a number of data sets introduced in Chapter 2.\nHumans have a longing to understand their place (temporally and spatially) in the universe. In an Einsteinian universe, space and time interact in a special, “curved” way; however, in this book our methodology and applications are for a Newtonian world. Rick Delmonico, author of The Philosophy of Fractals (Delmonico, 2017), has been quoted elsewhere as saying that “light is time at maximum compression and matter is space at maximum compression.” Our Newtonian world is definitely more relaxed than this! Nevertheless, it is fascinating that images of electron motion at a scale of \\(10^{-11}\\) meters look very much like images of the cosmos at a scale of \\(10^{17}\\) meters (Morrison & Morrison, 1982).\nTrying to understand spatio-temporal data and how (and ultimately why) they vary in space and time is not new — just consider trying to describe the growth and decline of populations, the territorial expansion and contraction of empires, the spread of world religions, species (including human) migrations, the dynamics of epidemics, and so on. Indeed, history and geography are inseparable. From this “big picture” point of view, there is a complex system of interacting physical, biological, and social processes across a range of spatial/temporal scales.\nHow does one do spatio-temporal statistics? Well, it is not enough to consider just spatial snapshots of a process at a given time, nor just time-series profiles at a given spatial location—the behavior at spatial locations at one time point will almost certainly affect the behavior at nearby spatial locations at the next time point. Only by considering time and space together can we address how spatially coherent entities change over time or, in some cases, why they change. It turns out that a big part of the how and why of such change is due to interactions across space and time, and across multiple processes.\nFor example, consider an influenza epidemic, which is generally in the winter season. Individuals in the population at risk can be classified as susceptible (S), infected (I), or recovered (R), and a well-known class of multivariate temporal models, called SIR models, capture the transition of susceptibles to infecteds to recovereds and then possibly back to susceptibles. At a micro level, infection occurs in the household, in the workplace, and in public places due to the interaction (contact) between infected and susceptible individuals. At a macro level, infection and recovery rates can be tracked and fitted to an SIR model that might also account for the weather, demographics, and vaccination rates. Now suppose we can disaggregate the total-population SIR rates into health-district SIR rates. This creates a spatio-temporal data set, albeit at a coarse spatial scale, and the SIR rates can be visualized dynamically on a map of the health districts. Spatio-temporal interactions may then become apparent, and the first steps of spatio-temporal modeling can be taken.\nSpatio-temporal interactions are not limited to similar types of processes nor to spatial and temporal scales of variability that seem obvious. For example, El Niño and La Niña phenomena in the tropical Pacific Ocean correspond to periods of warmer-than-normal and colder-than-normal sea surface temperatures (SST), respectively. These SST “events” occur every two to seven years, although the exact timing of their appearance and their end is not regular. But it is well known that they have a tremendous impact on the weather across the globe, and weather affects a great number of things! For example, the El Niño and La Niña events can affect the temperature and rainfall over the Midwest USA, which can affect, say, the soil moisture in the state of Iowa, which would likely affect corn production and could lead to a stressed USA agro-economy during that period. Simultaneously, these El Niño and La Niña events can also affect the probability of tornado outbreaks in the famed “tornado alley” region of the central USA, and they can even affect the breeding populations of waterfowl in the USA.\nDoing some clever smoothing and sharp visualizations of the spatial, temporal, and spatio-temporal variability in the data is a great start. But the information we glean from these data analyses needs to be organized, and this is done through models. In the next section, we make the case for spatio-temporal models that are statistical.\nIn the physical world, phenomena evolve in space and time following deterministic, perhaps “chaotic,” physical rules (except at the quantum level), so why do we need to consider randomness and uncertainty? The primary reason comes from the uncertainty resulting from incomplete knowledge of the science and of the mechanisms driving a spatio-temporal phenomenon. In particular, statistical spatio-temporal models give us the ability to model components in a physical system that appear to be random and, even if they are not, the models are useful if they result in accurate and precise predictions. Such models introduce the notion of uncertainty, but they are able to do so without obscuring the salient trends or regularities of the underlying process (that are typically of primary interest).\nTake, for instance, the raindrops falling on a surface; to predict exactly where and when each drop will fall would require an inconceivably complex, deterministic, meteorological model, incorporating air pressure, wind speed, water-droplet formation, and so on. A model of this sort at a large spatial scale is not only infeasible but also unnecessary for many purposes. By studying the temporal intensity of drops on a regular spatial grid, one can test for spatio-temporal interaction or look for dynamic changes in spatial intensity (given in units of “per area”) for each cell of the grid. The way in which the intensity evolves over time may reveal something about the driving mechanisms (e.g., wind vectors) and be useful for prediction, even though the exact location and time of each incident raindrop is uncertain.\nSpatio-temporal statistical models are not at odds with deterministic ones. Indeed, the most powerful (in terms of predictive performance) spatio-temporal statistical models are those that are constructed based on an understanding of the biological or physical mechanisms that give rise to spatio-temporal variability and interactions. Hence, we sometimes refer to them as physical-statistical models (see the editorial by Kuhnert, 2014), or generally as mechanistically motivated statistical models. To this understanding, we add the reality that observations may have large gaps between them (in space and in time), they are observed with error, our understanding of the physical mechanisms is incomplete, we have limited knowledge about model parameters, and so on. Then it becomes clear that incorporating statistical distributions into the model is a very natural way to solve complex problems. Answers to the problems come as estimates or predictions along with a quantification of their uncertainties. These physical-statistical models, in the temporal domain, the spatial domain, and the spatio-temporal domain, have immense use in everything from anthropology to zoology and all the “ologies” in-between.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to Spatio-Temporal Statistics</span>"
    ]
  },
  {
    "objectID": "Chapter1.html#sec-goals",
    "href": "Chapter1.html#sec-goals",
    "title": "1  Introduction to Spatio-Temporal Statistics",
    "section": "1.2 Goals of Spatio-Temporal Statistics",
    "text": "1.2 Goals of Spatio-Temporal Statistics\nWhat are we trying to accomplish with spatio-temporal data analysis and statistical modeling? Sometimes we are just trying to gain more understanding of our data. We might be interested in looking for relationships between two spatio-temporally varying processes, such as temperature and rainfall. This can be as simple as visualizing the data or exploring them through various summaries (Chapter 2). Augmenting these data with scientific theories and statistical methodologies allows valid inferences to be made (Chapter 3). For example, successive reports from the United Nations Intergovernmental Panel on Climate Change have concluded from theory and data that a build-up of atmospheric CO\\({_2}\\) leads to a greenhouse effect that results in global warming. Models can then be built to answer more focused questions. For example, the CO\\({_2}\\) data shown in Figure 1.2 are a manifestation of Earth’s carbon cycle: can we find precisely the spatio-temporal “places” on Earth’s surface where carbon moves in and out of the atmosphere? Or, how might this warming affect our ability to predict whether an El Ni~no event will occur within 6 months?\nBroadly speaking, there are three main goals that one might pursue with a spatio-temporal statistical model: (1) prediction in space and time (filtering and smoothing); (2) inference on parameters; and (3) forecasting in time. More specific goals might include data assimilation, computer-model emulation, and design of spatio-temporal monitoring networks. These are all related through the presence of a spatio-temporal statistical model, but they have their own nuances and may require different methodologies (Chapters 4 and 5).\n\n1.2.1 The Two Ds of Spatio-Temporal Statistical Modeling\nThere have been two approaches to spatio-temporal statistical modeling that address the goals listed above. These are the “two Ds” referred to in the title of this subsection, namely the descriptive approach and the dynamic approach. Both are trying to capture statistical dependencies in spatio-temporal phenomena, but they go about it in quite different ways.\nProbably the simplest example of this is in time-series modeling. Suppose that the dependence between any two data at different time points is modeled with a stationary first-order autoregressive process (AR(1)). Dynamically, the model says that the value at the current time is equal to a “propagation factor” (or “transition factor”) times the value at the previous time, plus an independent “innovation error.” This is a mechanistic way of presenting the model that is easy to simulate and easy to interpret.\nDescriptively, the same probability structure can be obtained by defining the correlation between two values at any two given time points to be an exponentially decreasing function of the lag between the two time points. (The rate of decrease depends on the AR(1) propagation factor.) Viewing the model this way, it is not immediately obvious how to simulate from it nor what the behavior of the correlation function means physically.\nThe “take-home” message here is that, while there is a single underlying probability model common to the two specifications, the dynamic approach has some attractive interpretable features that the descriptive approach does not have. Nevertheless, in the absence of knowledge of the dynamics, it can be the descriptive approach that is more “fit for purpose.” With mean and covariance functions that are sufficiently flexible, a good fit to the data can be obtained and, consequently, the spatio-temporal variability can be well described.\n\n\n1.2.2 Descriptive Modeling\nThe descriptive approach typically seeks to characterize the spatio-temporal process in terms of its mean function and its covariance function. When these are sufficient to describe the process, we can use “optimal prediction” theory to obtain predictions and, crucially, their associated prediction uncertainties. This approach has a distinguished history in spatial statistics and is the foundation of the famed kriging methodology. (Cressie, 1990, presents the early history of kriging.) In a spatio-temporal setting, the descriptive approach is most useful when we do not have a strong understanding of the mechanisms that drive the spatio-temporal phenomenon being modeled. Or perhaps we are more interested in studying how covariates in a regression are influencing the phenomenon, but we also recognize that the errors that occur when fitting that relationship are statistically dependent in space and time. That is, the standard assumption given in Chapter 3, that errors are independent and identically distributed (\\(iid\\)), is not tenable. In this case, knowing spatio-temporal covariances between the data is enough for statistically efficient inferences (via generalized least squares) on regression coefficients (see Chapter 4). But, as you might suspect, it can be quite difficult to specify all possible covariances for complex spatio-temporal phenomena (and, for nonlinear processes, covariances are not sufficient to describe the spatio-temporal statistical dependence within the process).\nSometimes we can describe spatio-temporal dependence in a phenomenon by including in our model covariates that capture spatio-temporal “trends.” This large-scale spatio-temporal variability leaves behind smaller-scale variability that can be modeled statistically with spatio-temporal covariances. The descriptive approach often relies on an important statistical characteristic of dependent data, namely that nearby (in space and time) observations tend to be more alike than those far apart. In spatial modeling, this is often referred to as “Tobler’s first law of geography” Tobler (1970), and it is often a good guiding principle. It is fair to point out, though, that there are exceptions: there might be “competition” (e.g., only smaller trees are likely to grow close to or under bigger trees as they compete over time for light and nutrients), or things may be more alike on two distant mountain peaks at the same elevation than they are on the same mountain peak at different elevations.\nIt is important to take a look back at the writings of the pioneers in statistics and ask why spatio-temporal statistical dependencies were not present in early statistical models if they are so ubiquitous in real-world data. Well, we know that some people definitely were aware of these issues. For example, in his ground-breaking treatise on the design of experiments in agriculture, R. A. Fisher (Fisher (1935), p. 66) wrote:\n\n“After choosing the area we usually have no guidance beyond the widely verified fact that patches in close proximity are commonly more alike, as judged by the yield of crops, than those which are further apart.”\n\nIn this case, the spatial variability between plots is primarily due to the fact that the soil properties vary relatively smoothly across space at the field level. Unfortunately, Fisher could not implement complex error models that included spatial statistical dependence due to modeling and computational limitations at that time. So he came up with the brilliant solution of introducing randomization into the experimental design in order to avoid confounding plot effects and treatment effects (but note, only at the plot scale). This was one of the most important innovations in twentieth-century science, and it revolutionized experimentation, not only in agriculture but also in industrial and medical applications. Readers interested in more details behind the development of spatial and spatio-temporal statistics could consult Chapter 1 of Cressie (1993) and Chapter 1 of Cressie & Wikle (2011), respectively.\n\n\n1.2.3 Dynamic Modeling \nDynamic modeling in the context of spatio-temporal data is simply the notion that we build statistical models that posit (either probabilistically or mechanistically) how a spatial process changes through time. It is inherently a conditional approach, in that we condition on knowing the past, and then we model how the past statistically evolves into the present. If the spatio-temporal phenomenon is what we call “stationary,” we could take what we know about it in the present (and the past) and forecast what it will look like in the future.\nBuilding spatio-temporal models using the dynamic approach is closer to how scientists think about the etiology of processes they study – that is, most spatio-temporal data really do correspond to a mechanistic real-world process that can be thought of as a spatial process evolving through time. This connection to the mechanism of the process allows spatio-temporal dynamic models a better chance to establish answers to the “why” questions (causality) – is this not the ultimate goal of science? Yet, there is no free lunch – the power of these models comes from established knowledge about the process’s behavior, which may not be available for the problem at hand. In that case, one might specify more flexible classes of dynamic models that can adapt to various types of evolution, or turn to the descriptive approach and fit flexible mean and covariance functions to the data.\nFrom a statistical perspective, dynamic models are closer to the kinds of statistical models studied in time series than to those studied in spatial statistics. Yet, there are two fundamental differences between spatio-temporal statistical models that are dynamic, and the usual multivariate time-series models. The first is that dynamic spatio-temporal models have to represent realistically the kinds of spatio-temporal interactions that take place in the phenomenon being studied – not all relationships that one might put into a multivariate time-series model make physical (or biological or economic or \\(\\ldots\\)) sense. The second reason has to do with dimensionality. It is very often the case in spatio-temporal applications that the dimensionality of the spatial component of the model prohibits standard inferential methods. That is, there would be too much “multi” if one chose a multivariate time-series representation of the phenomenon. Special care has to be taken as to how the model is parameterized in order to obtain realistic yet parsimonious dynamics. As discussed in Chapter 5, this has been facilitated to a large extent by the development of basis function expansions within hierarchical statistical models.\nIrrespective of which “D” is used to model a spatio-temporal data set, its sheer size can overwhelm computations. Model formulations that use basis functions are a powerful way to leap-frog the computational bottleneck caused by inverting a very large covariance matrix of the data. The general idea is to represent a spatio-temporal process as a mixed linear model with known covariates whose coefficients are unknown and non-random, together with known basis functions whose coefficients are unknown and random (Chapters 4 and 5). Usually the basis functions are functions of space and their coefficients define a multivariate time series of dependent random vectors. Depending on the type of basis functions considered, this formulation gives computational advantages due to reduced dimensions and/or sparse covariance/precision matrices that facilitate or eliminate the need for matrix inversions.\nThere are many classes of basis functions to choose from (e.g., Fourier, wavelets, bisquares) and many are multi-resolutional, although physically based functions (e.g., elevation) can easily be added to the class. If the basis functions are spatial and their random coefficients depend only on time, then the temporal dependence of the coefficients can capture complex spatio-temporal interactions. These include phenomena for which fine spatial scales affect coarse spatial scales and, importantly, vice versa.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to Spatio-Temporal Statistics</span>"
    ]
  },
  {
    "objectID": "Chapter1.html#sec-HMs",
    "href": "Chapter1.html#sec-HMs",
    "title": "1  Introduction to Spatio-Temporal Statistics",
    "section": "1.3 Hierarchical Statistical Models",
    "text": "1.3 Hierarchical Statistical Models\nWe believe that we are seeing the end of the era of constructing marginal-probability-based models for complex data. Such models are typically based on the specification of likelihoods from which unknown parameters are estimated. However, these likelihoods can be extremely difficult (or impossible) to compute when there are complex dependencies, and they cannot easily deal with the reality that the data are noisy versions of an underlying real-world process that we care about.\nAn alternative way to introduce statistical uncertainty into a model is to think conditionally and build complexity through a series of conditional-probability models. For example, if most of the complex dependencies in the data are due to the underlying process of interest, then one should model the distribution of the data conditioned on that process (data model), followed by a model of the process’ behavior and its uncertainties (process model). There will typically be unknown parameters present, in both the statistical model for the data (conditioned on the process) and the statistical model for the process. %How to deal with those parameters will be discussed below.\nWhen a dynamic model of one or several variables is placed within a hierarchical model formulation (see below), one obtains what has been historically called a state-space model in the time-series literature. That is, one has data that are collected sequentially in time (i.e., a time series), and they are modeled as “noisy” observations of an underlying state process evolving (statistically) through time. These models are at the core of a number of engineering applications (e.g., space missions), and the challenge is to find efficient approaches to perform inference on the underlying state process of interest while accounting for the noise.\nIn general, there are three such situations of interest when considering state-space models: smoothing, filtering, and forecasting. Smoothing refers to inference on the hidden state process during a fixed time period in which we have observations throughout the time period. (The reader might note that this is the temporal analog of spatial prediction on a bounded spatial domain.) Now consider a time period that always includes the most current time, at which the latest observation is available. Filtering refers to inference on the hidden state value at the most current time based on the current and all past data. The most famous example of filtering in this setting is a methodology known widely as the Kalman filter Kalman (1960). Finally, forecasting refers to inference on the hidden state value at any time point beyond the current time, where data are either not available or not considered in the forecast. In this book, instead of modeling the evolution of a single variable or several variables, we model entire spatial processes evolving through time, which often adds an extra layer of modeling complexity and computational difficulty. Chapter 5 discusses how basis-function representations can deal with these difficulties.\nIn addition to uncertainty associated with the data and the underlying spatio-temporal process, there might be uncertainties in the parameters. These uncertainties could be accounted for statistically by putting a prior distribution on the parameters. To make sense of all this, we use hierarchical (statistical) models (HMs), and follow the terminology of Berliner (1996), who defined an HM to include a data model, a process model, and a parameter model. Note 1.1 gives the conditional-probability structure that ties these models together into a coherent joint probability model of all the uncertainties. The key to the Berliner HM framework is that, at any level of a spatio-temporal HM, it is a good strategy to put as much of the dependence structure as possible in the conditional-mean specification in order to simplify the conditional-covariance specification.\nWhen the parameters are given prior distributions (i.e., a parameter model is posited) at the bottom level of the hierarchy, then we say that the model is a Bayesian hierarchical model (BHM). A BHM is often necessary for complex-modeling situations, because the parameters themselves may exhibit quite complex (e.g., spatial or temporal) structure. Or they may depend on other covariates and hence could be considered as processes in their own right. In simpler models, an alternative approach is to estimate the parameters present in the top two levels in some way using the data or other sources of data; then we like to say that the hierarchical model is an empirical hierarchical model (EHM). When applicable, an EHM may be preferred if the modeler is reluctant to put prior distributions on parameters about which little is known, or if computational efficiencies can be gained.\nIt is clear that the BHM approach allows very complex processes to be modeled by going deeper and deeper in the hierarchy, but at each level the conditional-probability model can be quite simple. Machine learning uses a similar approach with its deep models. A cascade of levels, where the processing of output from the previous level is relatively simple, results in a class of machine-learning algorithms known as deep learning. A potential advantage of the BHM approach over deep learning is that it provides a unified probabilistic framework that allows one to account for uncertainty in data, model, and parameters.\nA very important advantage of the data–process–parameter modeling paradigm in an HM is that, while marginal-dependence structures are difficult to model directly, conditional-dependence structures usually come naturally. For example, it is often reasonable to assume that the data covariance matrix (given the corresponding values of the hidden process) is simply a diagonal matrix of measurement-error variances. This frees up the process covariance matrix to capture the “pure” spatio-temporal dependence, ideally (but, not necessarily) from physical or mechanistic knowledge. Armed with these two covariance matrices, the seemingly complex marginal covariance matrix of the data can be simply obtained. This same idea is used in mixed-effects modeling (e.g., in longitudinal data analysis), and it is apparent in the spatio-temporal statistical models described in Chapters 4 and 5.\nThe product of the conditional-probability components of the HM gives the joint probability model for all random quantities (i.e., all “unknowns”). The HM could be either a BHM or an EHM, depending on whether, respectively, a prior distribution is put on the parameters (i.e., a parameter model is posited) or the parameters are estimated. (A hybrid situation arises when some but not all parameters are estimated and the remaining have a prior distribution put on them.) In this book, we are primarily interested in obtaining the (finite-dimensional) distribution of the hidden (discretized) spatio-temporal process given the data, which we call the predictive distribution. The BHM also allows one to obtain the posterior distribution of the parameters given the data, whereas the EHM requires an estimate of the parameters. Predictive and posterior distributions are obtained using Bayes’ Rule (Note 1.1).\nSince predictive and posterior distributions must have total probability mass equal to 1, there is a critical normalizing constant to worry about. Generally, it cannot be calculated in closed form, in which case we rely on computational methods to deal with it. Important advances in the last 30 years have alleviated this problem by making use of Monte Carlo samplers from a Markov chain whose stationary distribution is the predictive (or the posterior) distribution of interest. These Markov chain Monte Carlo (MCMC) methods have revolutionized the use of HMs for complex modeling applications, such as those found in spatio-temporal statistics.\n\n\n\n\n\n\nNote 1.1: Berliner’s Bayesian Hierarchical Model (BHM) paradigm\n\n\n\nFirst, the fundamental notion of the law of total probability allows one to decompose a joint distribution into a series of conditional distributions: \\({[A,B,C] = [A\\mid B,C][B\\mid C][C]}\\), where the “bracket notation” is used to denote probability distributions; for example, \\([A,B,C]\\) is the joint distribution of random variables \\(A\\), \\(B\\), and \\(C\\), and \\([A \\mid B, C]\\) is the conditional distribution of \\(A\\) given \\(B\\) and \\(C\\).\nMark Berliner’s insight (Berliner, 1996) was that one should use this simple decom-posi-tion as a way to formulate models for complex dependent processes. That is, the joint distribution, [data, process, parameters], can be factored into three levels.\nAt the top level is the data model, which is a probability model that specifies the distribution of the data given an underlying “true” process (sometimes called the hidden or latent process) and given some parameters that are needed to specify this distribution. At the next level is the process model, which is a probability model that describes the hidden process (and, thus, its uncertainty) given some parameters. Note that at this level the model does not need to account for measurement uncertainty. The process model can then use science-based theoretical or empirical knowledge, which is often physical or mechanistic. At the bottom level is the parameter model, where uncertainty about the parameters is modeled. From top to bottom, the levels of a BHM are:\n\\[\n\\begin{aligned}\n1.\\quad & \\text{Data model:}      &\\quad [\\text{data} \\mid \\text{process, parameters}] \\\\\n2.\\quad & \\text{Process model:}   &\\quad [\\text{process} \\mid \\text{parameters}] \\\\\n3.\\quad & \\text{Parameter model:} &\\quad [\\text{parameters}]\n\\end{aligned}\n\\] Importantly, each of these levels could have sub-levels, for which conditional-probability models could be given.\nUltimately, we are interested in the posterior distribution, [process, parameters \\(\\mid\\) data] which, conveniently, is proportional to the product of the levels of the BHM given above:\n\\[\n\\begin{aligned}\n~[\\text{process, parameters} \\mid \\text{data}] \\;\\propto\\; & [\\text{data} \\mid \\text{process, parameters}] \\\\\n                                            & \\times\\; [\\text{process} \\mid \\text{parameters}] \\\\\n                                            & \\times\\; [\\text{parameters}],\n\\end{aligned}\n\\]\nwhere “\\(\\propto\\)” means “is proportional to.” (Dividing the right-hand side by the normalizing constant, [data], makes it equal to the left-hand side.) Note that this result comes from application of Bayes’ Rule, applied to the hierarchical model. Inference based on complex models typically requires numerical evaluation of the posterior (e.g., MCMC methods), because the normalizing constant cannot generally be calculated in closed form.\nAn empirical hierarchical model (EHM) uses just the first two levels, from which the predictive distribution is\n\\[\n\\begin{aligned}\n~[\\text{process} \\mid \\text{data, parameters}] \\;\\propto\\; & [\\text{data} \\mid \\text{process, parameters}] \\\\\n                                                          & \\times\\; [\\text{process} \\mid \\text{parameters}],\n\\end{aligned}\n\\]\nwhere parameter estimates are substituted in for “parameters.” Numerical evaluation of this (empirical) predictive distribution is also typically needed, since the EHM’s normalizing constant cannot generally be calculated in closed form.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to Spatio-Temporal Statistics</span>"
    ]
  },
  {
    "objectID": "Chapter1.html#structure-of-the-book",
    "href": "Chapter1.html#structure-of-the-book",
    "title": "1  Introduction to Spatio-Temporal Statistics",
    "section": "1.4 Structure of the Book",
    "text": "1.4 Structure of the Book\nThe remaining chapters in this book are arranged in the way that we often approach stat-ist-ical modeling in general and spatio-temporal modeling in particular. That is, we begin by exploring our data. So, Chapter 2 gives ways to do this through visualization and through various summaries of the data. We note that both of these types of exploration can be tricky with spatio-temporal data, because we have one or more dimensions in space and one in time. It can be difficult to visualize information in more than two dimensions, so it often helps to slice through or aggregate over a dimension, or use color, or build animations through time. Similarly, when looking at numerical summaries of the data, we have to come up with innovative ways to help reduce the inherent dimensionality and to examine dependence structures and potential relationships in time and space.\nAfter having explored our data, it is often the case that we would like to fit some fairly simple models – sometimes to help us do an initial filling-in of missing observations that will assist with further exploration, or sometimes just to see if we have enough covariates to adequately explain the important dependencies in the data. This is the spirit of Chapter 3, which presents some ways to do spatial prediction that are not based on a statistical model or are based on very basic statistical models that do not explicitly account for spatio-temporal structure (e.g., linear regression, generalized linear models, and generalized additive models).\nIf the standard models presented in Chapter 3 are not sufficient to accomplish the goals we gave in Section 1.2, what are we to do? This is when we start to consider the descriptive and dynamic approaches to spatio-temporal modeling discussed above. The descriptive approach has been the “workhorse” of spatio-temporal statistical modeling for most of the history of the discipline, and these methods (e.g., kriging) are described in Chapter 4. But, as mentioned above, when we have strong mechanistic knowledge about the underlying process and/or are interested in complex prediction or forecasting scenarios, we often bene-fit from the dynamic approach described in Chapter 5. Take note that Chapters 4 and 5 will require a bit more patience to go through, because process models that incorporate statistical dependence require more mathematical machinery. Hence, in these two chapters, the notation and motivation will be somewhat more technical than for the models presented in Chapter 3. It should be kept in mind, though, that the aim here is not to make you an expert, rather it is to introduce you (via the text, the Labs, and the technical notes) to the motivations, main concepts, and practicalities behind spatio-temporal statistical modeling.\nAfter building a model, we would like to know how good it is. There are probably as many ways to evaluate models as there are models! So, it is safe to say that there is no standard way to evaluate a spatio-temporal statistical model. However, there are some common approaches that have been used in the past to carry out model evaluation and model comparison, some of which apply to spatio-temporal models (see Chapter 6). We note that the aim there is not to show you how to obtain the “best” model (as there isn’t one!). Rather, it is to show you how a model or a set of models can be found that does a reasonable job with regard to the goals outlined in Section 1.2.\nLast, but certainly not least, each of Chapters 2–6 contain Lab vignettes that go through the implementation of many of the important methods presented in each chapter using the R~programming language. This book represents the first time such a comprehensive collection of R~examples for spatio-temporal data have been collected in one place. We believe that it is essential to “get your hands dirty” with data, but we recognize that quite a few of the methods and approaches used in spatio-temporal statistics can be complicated and that it can be daunting to program them yourself from scratch. Therefore, we have tried to identi-fy some useful (and stable) R~functions from existing R~packages (see the list following the appendices) that can be used to implement the methods discussed in Chapters 2–6. We have also put a few functions of our own, along with the data sets that we have used, in the R~package, STRbook, associated with this book (instructions for obtaining this package are available at https://spacetimewithr.org. We note that there are many other R~packages that implement various spatio-temporal methods, whose approaches could arrive at the same result with more or less effort, depending on familiarity. As is often the case with R,~ one gets used to doing things a certain way, and so most of our choices are representative of this.\n\n\n\n\nBerliner, L. M. (1996). Hierarchical Bayesian time series models. In K. M. Hanson & R. N. Silver (Eds.), Maximum entropy and bayesian methods (pp. 15–22). Kluwer.\n\n\nCressie, N. (1990). The origins of kriging. Mathematical Geology, 22, 239–252.\n\n\nCressie, N. (1993). Statistics for spatial data (revised). John Wiley & Sons.\n\n\nCressie, N., & Wikle, C. K. (2011). Statistics for spatio-temporal data. John Wiley & Sons.\n\n\nDelmonico, R. (2017). The philosophy of fractals.\n\n\nFisher, R. A. (1935). The design of experiments (8th ed.). Edinburgh: Oliver; Boyd.\n\n\nKalman, R. E. (1960). A new approach to linear filtering and prediction problems. Journal of Basic Engineering, 82, 35–45.\n\n\nKuhnert, P. (2014). Editorial: Physical-statistical modelling. Environmetrics, 25, 201–202.\n\n\nMorrison, P., & Morrison, P. (1982). Powers of ten: About the relative size of things in the universe. Scientific American Library, distributed by WH Freeman.\n\n\nTobler, W. R. (1970). A computer movie simulating urban growth in the Detroit region. Economic Geography, 46(sup1), 234–240.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to Spatio-Temporal Statistics</span>"
    ]
  },
  {
    "objectID": "Chapter2.html",
    "href": "Chapter2.html",
    "title": "2  Exploring Spatio-Temporal Data",
    "section": "",
    "text": "2.1 Spatio-Temporal Data\nExploration into territory unknown, or little known, requires both curiosity and survival skills. You need to know where you are, what you are looking at, and how it relates to what you have seen already. The aim of this chapter is to teach you those skills for exploring spatio-temporal data sets. The curiosity will come from you!\nSpatio-temporal data are everywhere in science, engineering, business, and industry. This is driven to a large extent by various automated data acquisition instruments and software. In this chapter, after a brief introduction to the data sets considered in this book, we describe some basic components of spatio-temporal data structures in R, followed by spatio-temporal visualization and exploratory tools. The chapter concludes with fairly extensive Labs that provide examples of R commands for data wrangling, visualization, and exploratory data analysis.\nWhen you discover the peaks and valleys, trends and seasonality, and changing landscapes in your data set, what then? Are they real or illusory? Are they important? Chapters 3–6 will give you the inferential and modeling skills required to answer these questions.\nTime-series analysts consider univariate or multivariate sequential data as a random process observed at regular or irregular intervals, where the process can be defined in continuous time, discrete time, or where the temporal event is itself the random event (i.e., a point process). Spatial statisticians consider spatial data as either temporal aggregations or tem-por-ally frozen states (“snapshots”) of a spatio-temporal process. Spatial data are traditionally thought of as random according to either geostatistical, areal or lattice, or point process (and sometimes random set) behavior. We think of geostatistical data as the kind where we could have observations of some variable or variables of interest (e.g., temperature and wind speed) at continuous locations over a given spatial domain, and where we seek to predict those variables at unknown locations in space (e.g., using interpolation methodology such as kriging). Lattice processes are defined on a finite or countable subset in space (e.g., grid nodes, pixels, polygons, small areas), such as the process defined by work-force indicators on a specific political geography (e.g., counties in the USA) over a specific period of time. A spatial point process is a stochastic process in which the locations of the points (sometimes called events) are random over the spatial domain, where these events can have attributes given in terms of marks (e.g., locations of trees in a forest are random events, with the diameter at breast height being the mark). Given the proliferation of various data sources and geographical information system (GIS) software, it is important to broaden the perspective of spatial data to include not only points and polygons, but also lines, trajectories, and objects. It is also important to note that there can be significant differences in the abundance of spatial information versus temporal information.\nIt should not be surprising that data from spatio-temporal processes can be considered from either a time-series perspective or a spatial-random-process perspective, as described in the previous paragraph. In this book, we shall primarily consider spatio-temporal data that can be described by processes that are discrete in time and either geostatistical or on a lattice in space. For a discussion of a broader collection of spatio-temporal processes, see Cressie & Wikle (2011), particularly Chapters 5–9.\nThroughout this book, we consider the following data sets:",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Exploring Spatio-Temporal Data</span>"
    ]
  },
  {
    "objectID": "Chapter2.html#sec-STdata",
    "href": "Chapter2.html#sec-STdata",
    "title": "2  Exploring Spatio-Temporal Data",
    "section": "",
    "text": "Tip\n\n\n\nSpace-time data are usually provided in comma-separated value (CSV) files, which can be read into R using read.csv or read.table; shapefiles, which can be read into R using functions from rgdal and maptools; NetCDF files, which can be read into R using a variety of packages, such as ncdf4 and RNetCDF; and HDF5 files, which can be read into R using the package h5.\n\n\n\n\n\nNOAA daily weather data. These daily data originated from the US National Oceanic and Atmospheric Administration (NOAA) National Climatic Data Center and can be obtained from the IRI/LDEO Climate Data Library at Columbia University (available at http://iridl.ldeo.columbia.edu/SOURCES/.NOAA/.NCDC/.DAILY/.FSOD/). The data set we consider consists of four variables: daily maximum temperature (Tmax) in degrees Fahrenheit (\\(^\\circ\\)F), minimum temperature (Tmin) in \\(^\\circ\\)F, dew point temperature (TDP) in \\(^\\circ\\)F, and precipitation (Precip) in inches at 138 weather stations in the central USA (between 32\\(^\\circ\\)N–46\\(^\\circ\\)N and 80\\(^\\circ\\)W–100\\(^\\circ\\)W), recorded between the years 1990 and 1993 (inclusive). These data are considered to be discrete and regular in time (daily) and geostatistical and irregular in space. However, the data are not complete, in that there are missing measurements at various stations and at various time points, and the stations themselves are obviously not located everywhere in the central USA. We will refer to these data as the “NOAA data set.” Three days of Tmax measurements from the NOAA data set are shown in Figure 2.1.\n\n\n\n\n\n\n\nFigure 2.1: Maximum temperature (Tmax) in \\(^\\circ\\)F from the NOAA data set on 01, 15, and 30 May 1993.\n\n\n\n\nSea-surface temperature anomalies. These sea-surface temperature (SST) anomaly data are from the NOAA Climate Prediction Center as obtained from the IRI/LDEO Climate Data Library at Columbia University (available at http://iridl.ldeo.columbia.edu/SOURCES/.CAC/). The data are gridded at a 2\\(^\\circ\\) by 2\\(^\\circ\\) resolution from 124\\(^\\circ\\)E–70\\(^\\circ\\)W and 30\\(^\\circ\\)S–30\\(^\\circ\\)N, and they represent monthly anomalies from a January 1970–December 2003 climatology (averaged over time). We refer to this data set as the “SST data set.” Three individual months from the SST data set are shown in Figure 2.2.\n\n\n\n\n\n\n\nFigure 2.2: Sea-surface temperature anomalies in \\(^\\circ\\)C for the month of January in the years 1989, 1993, and 1998. The year 1989 experienced a La Niña event (colder than normal temperatures) while the year 1998 experienced an El Niño event (warmer than normal temperatures).\n\n\n\n\nBreeding Bird Survey (BBS) counts. These data are from the North American Breeding Bird Survey (available at https://www.pwrc.usgs.gov/bbs/RawData/). Note that we used the archived 2016.0 version of the data set, doi: 10.5066/F7W0944J, which is accessible through the data archive link on the BBS website ftp://ftpext.usgs.gov/pub/er/md/laurel/BBS/Archivefiles/Version2016v0/). In particular, we consider yearly counts of the house finch (Carpodacus mexicanus) at BBS routes for the period 1966–2000 and the Carolina wren (Thryothorus ludovicianus) for the period 1967–2014. The BBS sampling unit is a roadside route of length approximately 39.2 km. In each sampling unit, volunteer observers make 50 stops and count birds for a period of 3 minutes when they run their routes (typically in June). There are over 4000 routes in the North American survey, but not all routes are available every year. For the purposes of the analyses in this book, we consider the total route counts to occur yearly (during the breeding season) and define the spatial location of each route to be the route’s centroid. Thus, we consider the data to be discrete in time, geostatistical and irregular in space, and non-Gaussian in the sense that they are counts. We refer to this data set as the “BBS data set.” Counts of house finches for the period 1980–1999 are shown in Figure 2.3.\n\n\n\n\n\n\n\nFigure 2.3: Counts of house finches between 1980 and 1999. The size of the points is proportional to the number of observed birds, while transparency is used to draw attention to regions of high sampling density or high observed counts.\n\n\n\n\nPer capita personal income. We consider yearly per capita personal income (in dollars) data from the US Bureau of Economic Analysis (BEA) (available at http://www.bea.gov/regional/downloadzip.cfm). These data have areal spatial support corresponding to USA counties in the state of Missouri, and they cover the period 1969–2014. We refer to this data set as the “BEA income data set.” Figure 2.4 shows these data, on a log scale, for the individual years 1970, 1980, and 1990; note that these data have been adjusted for inflation.\n\n\n\n\n\n\n\nFigure 2.4: Per capita personal income (in dollars) by county for residents in Missouri in the years 1970, 1980, and 1990, plotted on a log scale. The data have been adjusted for inflation. Note how both the overall level of income as well as the spatial variation change with time.\n\n\n\n\nSydney radar reflectivity. These data are a subset of consecutive weather radar reflectivity images considered in the World Weather Research Programme (WWRP) Sydney 2000 Forecast Demonstration Project. There are 12 images at 10-minute intervals starting at 08:25 UTC on 03 November, 2000 (i.e., 08:25–10:15 UTC). The data were originally mapped to a \\(45 \\times 45\\) grid of 2.5 km pixels centered on the radar location. The data used in this book are for a region of dimension \\(28 \\times 40\\), corresponding to a 70 km by 100 km domain. All reflectivities are given in “decibels relative to Z” (dBZ, a dimensionless logarithmic unit used for weather radar reflectivities). We refer to this data set as the “Sydney radar data set.” For more details on these data, shown in Figure 2.5, see Xu et al. (2005).\n\n\n\n\n\n\n\nFigure 2.5: Weather radar reflectivities in dBZ for Sydney, Australia, on 03 November 2000. The images correspond to consecutive 10-minute time intervals from 08:25 UTC to 10:15 UTC.\n\n\n\n\nMediterranean winds. These data are east–west (\\(u\\)) and north–south (\\(v\\)) wind-component observations over the Mediterranean region (from 6.5\\(^\\circ\\)W–16.5\\(^\\circ\\)E and 33.5\\(^\\circ\\)N–45.5\\(^\\circ\\)N) for 28 time periods (every 6 hours) from 00:00 UTC on 29 January 2005 to 18:00 UTC on 04 February 2005. There are two data sources: satellite wind observations from the QuikSCAT scatterometer, and surface winds and pressures from an analysis by the European Center for Medium Range Weather Forecasting (ECMWF). The ECMWF-analysis winds and pressures are given on a \\(0.5^\\circ \\times 0.5^\\circ\\) spatial grid (corresponding to 47 longitude locations and 25 latitude locations), and they are available at each time period for all locations. The QuikSCAT observations are only available intermittently in space, due to the polar orbit of the satellite, but at much higher spatial resolution (25 km) than the ECMWF data when they are available. The QuikSCAT observations given for each time period correspond to all observations available in the spatial domain within 3 hours of time periods stated above. There are no QuikSCAT observations available at 00:00 UTC and 12:00 UTC in the spatial domain and time periods considered here. We refer to this data set as the “Mediterranean winds data set.” Figure 2.6 shows the wind vectors (“quivers”) for the ECMWF data at 06:00 UTC on 01 February 2005. These data are a subset of the data described in Cressie & Wikle (2011) and Milliff et al. (2011).\n\n\n\n\n\n\n\nFigure 2.6: ECMWF wind vector observations over the Mediterranean region for 06:00 UTC on 01 February 2005.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Exploring Spatio-Temporal Data</span>"
    ]
  },
  {
    "objectID": "Chapter2.html#sec-STdatainR",
    "href": "Chapter2.html#sec-STdatainR",
    "title": "2  Exploring Spatio-Temporal Data",
    "section": "2.2 Representation of Spatio-Temporal Data in R",
    "text": "2.2 Representation of Spatio-Temporal Data in R\nAlthough there are many ways to represent spatial data and time-series data in R, there are relatively few ways to represent spatio-temporal data. In this book we use the class definitions defined in the R package spacetime. These classes extend those used for spatial data in sp and time-series data in xts. For details, we refer the interested reader to the package documentation and vignettes in Pebesma (2012). Here, we just provide a brief introduction to some of the concepts that facilitate thinking about spatio-temporal data structures.\nAlthough spatio-temporal data can come in quite sophisticated relational forms, they most often come in the form of fairly simple “tables.” Pebesma (2012) classifies these simple tables into three classes:\n\ntime-wide, where columns correspond to different time points;\nspace-wide, where columns correspond to different spatial features (e.g., locations, regions, grid points, pixels);\nlong formats, where each record corresponds to a specific time and space coordinate.\n\n\n\n\n\n\n\nTip\n\n\n\nData in long format are space inefficient, as spatial coordinates and time attributes are required for each data point, whether or not data are on a lattice. However, it is easy to subset and manipulate data in long format. Powerful “data wrangling” tools in packages such as dplyr and tidyr, and visualization tools in ggplot2, are designed for data in long format.\n\n\nTables are very useful elementary data objects. However, an object from the spacetime package contains additional information, such as the map projection and the time zone. Polygon objects may further contain the individual areas of the polygons as well as the individual bounding boxes. These objects have elaborate, but consistent, class definitions that greatly aid the geographical (e.g., spatial) component of the analysis.\nPebesma (2012) considers four classes of space-time data: \n\nfull grid (STF), a combination of any sp object and any xts object to represent all possible locations on the implied space-time lattice;\nsparse grid (STS), as STF, but contains only the non-missing space-time combinations on a space-time lattice;\nirregular (STI), an irregular space-time data structure, where each point is allocated a spatial coordinate and a time stamp;\nsimple trajectories (STT), a sequence of space-time points that form trajectories.\n\nNote that the “grid” in the first two classes corresponds to a space-time lattice — but the spatial locations may or may not be on a lattice! The sparse grid is most effective when there are missing observations, or when there are a relatively few spatial locations that have different time stamps, or when there are a relatively small number of times that have differing spatial locations.\nIt is important to note that the class objects that make up the spacetime package are not used to store data; this is accomplished through the use of the R data frame. As illustrated in Lab 2.1 at the end of this chapter and in Pebesma (2012), there are several important methods in sp and spacetime that help with the construction and manipulation of these spatio-temporal data sets. In particular, there are methods to construct an object, replace/select data or various spatial or temporal subsets, coerce spatio-temporal objects to other classes, overlay spatio-temporal observations, and aggregate over space, time, or space-time.\n\n\n\n\n\n\nTip\n\n\n\nWhen spatio-temporal data have non-trivial support (i.e., a spatio-temporal region over which a datum is defined), and if the geometry allows it, use SpatialPixels and not SpatialPolygons as the underlying sp object. This results in faster geometric manipulations such as when finding the overlap between points and polygons using the function over.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Exploring Spatio-Temporal Data</span>"
    ]
  },
  {
    "objectID": "Chapter2.html#visualization-of-spatio-temporal-data",
    "href": "Chapter2.html#visualization-of-spatio-temporal-data",
    "title": "2  Exploring Spatio-Temporal Data",
    "section": "2.3 Visualization of Spatio-Temporal Data",
    "text": "2.3 Visualization of Spatio-Temporal Data\nA picture – or a video – can be worth a thousand tables. Use of maps, color, and animation is a very powerful way to provide insight that suggests exploratory data analysis that then leads to spatio-temporal models (Chapters 3–5). Although there are distinct challenges in visualizing spatio-temporal data due to the fact that several dimensions often have to be considered simultaneously (e.g., two or three spatial dimensions and time), there are some fairly common tools that can help explore such data visually. For the most part, we are somewhat selective in what we present here as we want to convey fairly simple methods that have consistently proven useful in our own work and in the broader literature. These can be as simple as static spatial maps and time-series plots, or they can be interactive explorations of the data (Lamigueiro, 2018). In addition, because of the special dynamic component of many spatio-temporal processes, where spatial processes evolve through time, it is often quite useful to try to visualize this evolution. This can be done in the context of one-dimensional space through a space-time (Hovmöller) plot, or more generally through animations. We conclude by discussing an increasingly popular approach to help with visualization of very high-dimensional data.\n\n\n\n\n\n\nTip\n\n\n\nSpatio-temporal visualization in R generally proceeds using one of two methods: the trellis graph or the grammar of graphics. The command plot invokes the trellis graph when sp or spacetime objects are supplied as arguments. The commands associated with the package ggplot2 invoke the grammar of graphics. The data objects frequently need to be converted into a data frame in long format for use with ggplot2, which we often use throughout this book.\n\n\n\n2.3.1 Spatial Plots\nSnapshots of spatial processes for a given time period can be plotted in numerous ways. If the observations are irregular in space, then it is often useful to plot a symbol at the data location and give it a different color and/or size to reflect the value of the observation. For example, consider Tmax for 01 May 1993 from the NOAA data set plotted in the left panel of Figure 2.1. In this case, the circle center corresponds to the measurement location and the color of the filled-in circle corresponds to the value of the maximum temperature. Notice the clear visual trend of decreasing temperatures from the southeast to the northwest over this region of the USA.\nSpatial plots of gridded data are often presented as contour plots, so-called “image” plots, or surface plots. For example, Figure 2.2 shows image representations for three individual months of the Pacific SST data set. Note the La Niña signal (cooler than normal SSTs) in 1989 and the El Niño signal (warmer than normal SSTs) in 1998 in the tropical Pacific Ocean. Figure 2.7 shows contour and surface representations of the SST anomalies in January 1998, corresponding to the right panel (i.e., the El Niño event) in Figure 2.2.\nIt is often useful to plot a sequence of spatial maps for consecutive times to gain greater insight into the changes in spatial patterns through time. Figure 2.8 shows a sequence of SST spatial maps for the months January–June 1989. Note how the initially strong La Niña event dissipates by June 1989.\n\n\n\n\n\n\nTip\n\n\n\nMultiple time-indexed spatial maps can be plotted from one long-format table using the functions facet_grid or facet_wrap in ggplot2 with time as a grouping variable.\n\n\n\n\n\n\n\n\nFigure 2.7: Sea-surface temperature anomalies (in \\(^\\circ\\)C) for January 1998 as a contour plot (top) and as a surface plot (bottom).\n\n\n\n\n\n\n\n\n\nFigure 2.8: Sea-surface temperature anomalies (in \\(^\\circ\\)C) for January–June 1989.\n\n\n\n\n\n2.3.2 Time-Series Plots\nIt can be instructive to plot time series corresponding to an observation location, an aggregation of observations, or multiple locations simultaneously. For example, Figure 2.9 shows time-series plots of daily Tmax for 10 of the NOAA stations (chosen randomly from the 139 stations) for the time period 01 May 1993–30 September 1993. The time-series plots are quite noisy, as is to be expected from the variability inherent in mid-latitude weather systems. However, there is an overall temporal trend corresponding to the annual seasonal cycle. That is, all of the time series appear to peak somewhat towards the center of the time horizon, which corresponds to the month of July. In this case, since we are using only five months of data, this trend appears to be roughly quadratic in time. Periodic functions are often used when considering a whole year or multiple years of data, especially with weather and economic data. Although all of these temperature series contain a seasonal component, some appear shifted on the vertical axis (Tmax) relative to one another (e.g., station 13881 has higher temperatures than station 14897). This is due to the latitudinal trend apparent in Figure 2.1.\n\n\n\n\n\n\nFigure 2.9: Maximum temperature (\\(^\\circ\\)F) for ten stations chosen from the NOAA data set at random, as a function of the day number, with the first day denoting 01 May 1993 and the last day denoting 30 September 1993. The number in the grey heading of each plot denotes the station ID.\n\n\n\n\n\n2.3.3 Hovmöller Plots\nA Hovmöller plot Hovmöller (1949) is a two-dimensional space-time visualization in which space is collapsed (projected or averaged) onto one dimension and where the second dimension denotes time. These plots have traditionally been considered in the atmospheric-science and ocean-science communities to visualize propagating features. For example, the left panel of Figure 2.10 shows monthly SST anomalies averaged from \\(1^\\circ\\)S–\\(1^\\circ\\)N and plotted such that longitude (over the Pacific Ocean) is on the \\(x\\)-axis and time (from 1996 to 2003) is on the \\(y\\)-axis (increasing from top to bottom). The darker red colors correspond to warmer than normal temperatures (i.e., El Niño events) and the darker blue colors correspond to colder than normal temperatures (i.e., La Niña events). Propagation through time is evident if a coherent color feature is “slanted.” In this plot, one can see several cases of propagating features along the longitudinal axis (e.g., both of the major La Niña events show propagation from the eastern longitudes towards the western longitudes).\nHovmöller plots are straightforward to generate with regular spatio-temporal data, but they can also be generated for irregular spatio-temporal data after suitable interpolation to a regular space-time grid. For example, in Figure 2.11, we show Hovmöller plots for the Tmax variable in the NOAA data set between 01 May 1993 and 30 September 1993. We see that the temporal trend is fairly constant with longitude (left panel), but it decreases considerably with increasing latitude (right panel) as expected, since overall maximum temperature decreases with increasing latitude in the conterminous USA. Such displays may affect modeling decisions of the trend (e.g., a time–latitude interaction might become evident in such plots).\n\n\n\n\n\n\nFigure 2.10: Hovmöller plots for both the longitude (left) and latitude (right) coordinates for the SST data set. The color denotes the temperature anomaly in \\(^\\circ\\)C.\n\n\n\n\n\n\n\n\n\nFigure 2.11: Hovmöller plots for both the longitude (left) and latitude (right) coordinates for the Tmax variable in the NOAA data set between 01 May 1993 and 30 September 1993, where the data are interpolated as described in Lab 2.2. The color denotes the maximum temperature in \\(^\\circ\\)F. The dashed lines correspond to the longitude and latitude coordinates of station 13966 (compare to Figure 2.9).\n\n\n\n\n\n2.3.4 Interactive Plots\nProgramming tools for interactive visualization are becoming increasingly accessible. These tools typically allow for a more data-immersive experience, and they allow one to explore the data without having to resort to scripting. In the simplest of cases, one can “hover” a cursor over a figure, and some information related to the data corresponding to the current location of the cursor is conveyed to the user. For example, in Figure 2.12 we show the interaction of the user with a spatial plot of SST using the package plotly. This package works in combination with a web portal for more advanced exploration methods (e.g., the exploration of three-dimensional data).\nThere are several interactive plots that may aid with the visualization of spatio-temporal data. One of the most useful plots builds on linked brushing, with the link acting between time and space. Here, one hovers a cursor over a spatial observation or highlights a spatial area, and then the time series corresponding to that point or area is visualized; see Figure 2.12. This allows one to explore the time series corresponding to known geographic areas with minimal effort. Code for generating a linked brush is available from the book’s website (https://spacetimewithr.org).\n\n\n\n\n\n\nFigure 2.12: Interactively exploring maximum temperatures on 01 May 1993 using the NOAA data set. The “hover” feature can be added to ggplot2 objects by using ggplotly from the package plotly (left). A linked brush can be used to explore the time series (right) corresponding to a user-chosen set of spatial locations (middle) with the package ggvis.\n\n\n\n\n\n2.3.5 Animations\nEveryone loves a movie. Animation captures our attention and can suggest structure in a way that a sequence of still frames cannot. Good movies should be watched again and again for understanding why the spatio-temporal data behave the way they do.\nAn animation is typically constructed by plotting spatial data frame-by-frame, and then stringing them together in sequence. When doing so, it is important to ensure that all spatial axes and color scales remain constant across all frames. In situations with missing or unequally spaced observations, one may sometimes improve the utility of an animation by performing a simple interpolation (in space and/or time) before constructing the sequence. Animations in R can be conveniently produced using the package animation. We provide an example using this package in Lab 2.2.\n\n\n2.3.6 Trelliscope: Visualizing Large Spatio-Temporal Data Sets\nMost spatio-temporal statistical analyses to date have been carried out on manageable data sets that can fit into a computer’s memory which, at the time of writing, was in the order of a few tens or a couple of hundreds of gigabytes in size. Being able to visualize these data is important and useful in many respects. Proceeding with modeling and prediction where not all the data can be processed in a single place (known as parallel-data algorithms) is an active area of research and will not be discussed here.\nThe Trelliscope system, available with the package trelliscope, helps users visualize massive data sets. The first advantage of trelliscope is that it facilitates exploration when, due to their size, the data may only be visualized using hundreds or thousands of plots (or panels). The Trelliscope system can calculate subset summaries (known as cognostics) that are then used for filtering and sorting the panels. For example, consider the SST data set. If a grouping is made by month, then there are over 300 spatial maps that can be visualized between, say, 1970 and 2003. Alternatively, one may decide to visualize only those months in which the SST exceeded a certain maximum or minimum threshold. One can formulate a cognostic using the monthly spatial mean values of SST averaged over their spatial domain and visualize them in a quantile plot (see Figure 2.13). The analyst can use this approach to quickly view the strongest El Niño and La Niña events in this time period.\nThe second advantage is that the trelliscope package is designed to visualize data that are on a distributed file system that may be residing on more than one node. The data are processed in a divide and recombine fashion; that is, the data are divided and processed by group in parallel fashion and then recombined. In trelliscope, this can be useful for generating both the cognostics and the viewing panels efficiently. Therefore, the Trelliscope system provides a way to visualize terabytes of space-time data but, as quoted in its package manual, it “can also be very useful for small data sets.”\n\n\n\n\n\n\nFigure 2.13: Exploring a large spatio-temporal data set with Trelliscope. Quantile plot of monthly averages of sea-surface temperature from the SST data set; the insets are what would be displayed if the user highlighted the circle points, corresponding to El Niño and La Niña events.\n\n\n\n\n\n\n\n\n\nTip\n\n\n\nProcessing and visualizing large data sets residing on a distributed file system using divide and recombine may seem like a daunting task. The R package datadr, which can be used together with trelliscope, provides an easy-to-use front-end for data residing on distributed file systems. More importantly, it reduces the barrier to entry by allowing the same, or very similar, code to be used for data residing in memory and data residing on a distributed file system such as Hadoop.\n\n\n\n\n2.3.7 Visualizing Uncertainty\nOne of the main things that separates statistics from other areas of data science is the focus on uncertainty quantification. Uncertainties could be associated with data (e.g., measurement error in satellite observations or sampling error in a survey), estimates (e.g., uncertainty in regression parameter estimates), or predictions (e.g., uncertainties in a forecast of SST anomalies). Taking a Bayesian point of view, uncertainties could also be associated with the parameters themselves. In the case where these uncertainties are indexed in time, space, or space-time, one can use any of the methods discussed in this section to produce visualizations of these uncertainties. It is increasingly the case that one seeks methods to visualize both the values of interest and their uncertainty simultaneously. This is challenging given the difficulties in visualizing information in multiple dimensions, and it is an active area of research both in geography and statistics (see, for example, the discussion of “visuanimation” in Genton et al. (2015)). For a recent overview in the case of areal data, and an accompanying R vignette, see Lucchesi & Wikle (2017) and the R package Vizumap (https://doi.org/10.5281/zenodo.1479951).",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Exploring Spatio-Temporal Data</span>"
    ]
  },
  {
    "objectID": "Chapter2.html#exploratory-analysis-of-spatio-temporal-data",
    "href": "Chapter2.html#exploratory-analysis-of-spatio-temporal-data",
    "title": "2  Exploring Spatio-Temporal Data",
    "section": "2.4 Exploratory Analysis of Spatio-Temporal Data",
    "text": "2.4 Exploratory Analysis of Spatio-Temporal Data\nVisualization of data is certainly an important and necessary component of exploratory data analysis. In addition, we often wish to explore spatio-temporal data in terms of summaries of first-order and second-order characteristics. Here we consider visualizations of empirical means and empirical covariances, spatio-temporal covariograms and semivariograms, the use of empirical orthogonal functions and their associated principal-component time series, and spatio-temporal canonical correlation analysis. To do this, we have to start using some mathematical symbols and formulas. Mathematics is the language of science (and of statistical science), and we introduce this language along the way to help readers who are a bit less fluent. For reference, we present some fundamental definitions of vectors and matrices and their manipulation in Appendix A. Readers who are not familiar with the symbols and basic manipulation of vectors and matrices would benefit from looking at this material before proceeding.\n\n2.4.1 Empirical Spatial Means and Covariances\nIt can be useful to explore spatio-temporal data by examining the empirical means and empirical covariances. Assume for the moment that we have observations \\(\\{Z(\\mathbf{s}_i;t_j)\\}\\) for spatial locations \\(\\{\\mathbf{s}_i: i=1,\\ldots,m\\}\\) and times \\(\\{t_j: j=1,\\ldots,T\\}\\).\nThe empirical spatial mean for location \\(\\mathbf{s}_i\\), \\(\\widehat{\\mu}_{z,s}(\\mathbf{s}_i)\\), is then found by averaging over time:\n\\[\n\\widehat{\\mu}_{z,s}(\\mathbf{s}_i) \\equiv \\frac{1}{T} \\sum_{j=1}^T Z(\\mathbf{s}_i;t_j).\n\\]\nIf we consider the means for all spatial data locations and assume that we have \\(T\\) observations at each location, then we can write down the spatial mean as an \\(m\\)-dimensional vector, \\(\\widehat{\\boldsymbol{\\mu}}_{z,s}\\), where\n\\[\n\\widehat{\\boldsymbol{\\mu}}_{z,s} \\equiv  \n\\left[\\begin{array}{c}\n\\widehat{\\mu}_{z,s}(\\mathbf{s}_1) \\\\\n\\vdots \\\\\n\\widehat{\\mu}_{z,s}(\\mathbf{s}_m)\n\\end{array} \\right] =\n\\begin{bmatrix}\n\\displaystyle\\frac{1}{T}\\sum_{j=1}^T Z(\\mathbf{s}_1;t_j) \\\\\n\\vdots \\\\\n\\displaystyle\\frac{1}{T}\\sum_{j=1}^T Z(\\mathbf{s}_m;t_j)\n\\end{bmatrix} =\n\\frac{1}{T}\\sum_{j=1}^T\\mathbf{Z}_{t_j},\n\\tag{2.1}\\]\nand \\(\\mathbf{Z}_{t_j} \\equiv (Z(\\mathbf{s}_1;t_j),\\ldots, Z(\\mathbf{s}_m;t_j))'\\).\nThis mean vector is a spatial quantity whose elements are indexed by their location. Therefore, it can be plotted on a map, as in the case of the maximum temperature in the NOAA data set (see Figure 2.1), or as a function of the spatial coordinates (e.g., longitude or latitude) as in Figure 2.14. From these plots one can see that there is a clear trend in the empirical spatial mean of maximum temperature with latitude, but not so much with longitude. Note that one may not have the same number of observations at each location to calculate the average, in which case each location in space must be calculated separately (e.g., \\(\\widehat{\\mu}_{z,s}(\\mathbf{s}_i) = (1/T_i) \\sum_{j=1}^{T_i} Z(\\mathbf{s}_i;t_j)\\), where \\(T_i\\) is the number of time points at which there are data at location \\(\\mathbf{s}_i\\)).\nAdditionally, one can average across space and plot the associated time series. The empirical temporal mean for time \\(t_j\\), \\(\\widehat{\\mu}_{z,t}(t_j)\\), is given by\n\\[\n\\widehat{\\mu}_{z,t}(t_j) \\equiv \\frac{1}{m} \\sum_{i=1}^m Z(\\mathbf{s}_i;t_j).\n\\tag{2.2}\\]\nFor example, Figure 2.15 shows the time series of Tmax for the NOAA temperature data set averaged across all of the spatial locations. This plot of the empirical temporal means shows the seasonal nature of the mid-latitude temperature over the central USA, but it also shows variations in that seasonal pattern due to specific large-scale weather systems.\n\n\n\n\n\n\nTip\n\n\n\nComputing empirical means is quick and easy using functions in the package dplyr. For example, to find a temporal average, the data in a long-format data frame can first be grouped by spatial location using the function group_by. A mean can then be computed for every spatial location using the function summarise. See Lab 2.1 for more details on these functions.\n\n\n\n\n\n\n\n\nFigure 2.14: Empirical spatial mean, \\(\\widehat{\\mu}_{z,s}(\\cdot)\\), of Tmax (in \\(^\\circ\\)F) as a function of station longitude (left) and station latitude (right).\n\n\n\n\n\n\n\n\n\nFigure 2.15: Tmax data (in \\(^\\circ\\)F), from the NOAA data set (blue lines, where each blue line corresponds to a station) and the empirical temporal mean \\(\\widehat\\mu_{z,t}(\\cdot)\\) (black line), with \\(t\\) in units of days, ranging from 01 May 1993 to 30 September 1993.\n\n\n\nIt is often useful to consider the empirical spatial covariability in the spatio-temporal data set. This covariability can be used to determine to what extent data points in the data set covary (behave similarly) as a function of space and/or time.\nIn the context of the data described above, the empirical lag-\\(\\tau\\) covariance between spatial locations \\(\\mathbf{s}_i\\) and \\(\\mathbf{s}_k\\) is given by\n\\[\n\\widehat{C}_z^{(\\tau)}(\\mathbf{s}_i,\\mathbf{s}_k) \\equiv \\frac{1}{T- \\tau} \\sum_{j=\\tau + 1}^T (Z(\\mathbf{s}_i;t_j) - \\widehat{\\mu}_{z,s}(\\mathbf{s}_i)) (Z(\\mathbf{s}_k;t_j - \\tau)  - \\widehat{\\mu}_{z,s}(\\mathbf{s}_k)),\n\\tag{2.3}\\]\nfor \\(\\tau = 0,1,\\ldots,T-1\\), which is called the empirical lag-\\(\\tau\\) spatial covariance.\nNote that this is the average (over time) of the cross products of the centered observations at the two locations (\\(\\mathbf{s}_i\\) and \\(\\mathbf{s}_k\\)); that is, it is a summary of the covariation of these data. It is often useful to consider the \\(m \\times m\\) lag-\\(\\tau\\) empirical spatial covariance matrix, \\(\\widehat{\\mathbf{C}}_z^{(\\tau)}\\), where the \\((i,k)\\)th element is given by the formula above. Alternatively, this can be calculated directly by\n\\[\n\\widehat{\\mathbf{C}}_z^{(\\tau)} \\equiv \\frac{1}{T - \\tau}\\sum_{j= \\tau + 1}^T(\\mathbf{Z}_{t_j} - \\widehat{\\boldsymbol{\\mu}}_{z,s})(\\mathbf{Z}_{t_j-\\tau} - \\widehat{\\boldsymbol{\\mu}}_{z,s})';\\quad \\tau = 0,1,\\dots,T-1.\n\\tag{2.4}\\]\nThus, in order to find the lag-\\(\\tau\\) covariance matrices, we consider the cross products of the residual vectors for each spatial location and each time point relative to its corresponding time-averaged empirical spatial mean.\nIn general, it can be difficult to obtain any intuition from these matrices, since locations in a two-dimensional space do not have a natural ordering. However, one can sometimes gain insight by splitting the domain into “strips” corresponding to one of the spatial dimensions (e.g., longitudinal strips) and then plotting the associated covariance matrices for those strips. For example, Figure 2.16 shows empirical covariance matrices for the maximum temperature in the NOAA data set (after, as shown in Lab 2.3, a quadratic trend in time has been removed), split into four longitudinal strips. Not surprisingly, these empirical spatial covariance matrices reveal the presence of spatial dependence in the residuals. The lag-0 plots seem to be qualitatively similar, suggesting that there is no strong correlational dependence on longitude but that there is a correlational dependence on latitude, with the spatial covariance decreasing with decreasing latitude.\nWe can also calculate the empirical lag-\\(\\tau\\) cross-covariance matrix between two spatio-temporal data sets, \\(\\{\\mathbf{Z}_{t_j}\\}\\) and \\(\\{\\mathbf{X}_{t_j}\\}\\), where \\(\\{\\mathbf{X}_{t_j}\\}\\) corresponds to data vectors at \\(n\\) different locations (assumed to correspond to the same time points). We define this \\(m \\times n\\) matrix by\n\\[\n\\widehat{\\mathbf{C}}_{z,x}^{(\\tau)} \\equiv \\frac{1}{T-\\tau} \\sum_{j=\\tau+1}^T ({\\mathbf{Z}}_{t_j} - \\widehat{\\boldsymbol{\\mu}}_{z,s})({\\mathbf{X}}_{t_j - \\tau}  - \\widehat{\\boldsymbol{\\mu}}_{x,s})',\n\\tag{2.5}\\]\nfor \\(\\tau=0,1,\\ldots,T-1\\), where \\(\\widehat{\\boldsymbol{\\mu}}_{x,s}\\) is the empirical spatial mean vector for \\(\\{\\mathbf{X}_{t_j}\\}\\). Cross-covariances may be useful in characterizing the spatio-temporal dependence relationship between two different variables, for example maximum temperature and minimum temperature.\nAlthough not as common in spatio-temporal applications, one can also calculate empirical temporal covariance matrices averaging across space (after removing temporal means averaged across space). In this case, the time index is unidimensional and ordered, so one does not have to work as hard on the interpretation as we did with empirical spatial covariance matrices.\n\n\n\n\n\n\nFigure 2.16: Maximum temperature lag-0 (top) and lag-1 (bottom) empirical spatial covariance plots for four longitudinal strips (from left to right, \\([-100, -95)\\), \\([-95, -90)\\), \\([-90, -85)\\), \\([-85, -80)\\) degrees) in which the domain of interest is subdivided.\n\n\n\n\n\n2.4.2 Spatio-Temporal Covariograms and Semivariograms\nIn Chapter 4 we shall see that it is necessary to characterize the joint spatio-temporal dependence structure of a spatio-temporal process in order to perform optimal prediction (i.e., kriging). Thus, for measures of the joint spatio-temporal dependence, we consider empirical spatio-temporal covariograms (and their close cousins, semivariograms). The biggest difference between what we are doing here and the covariance estimates in the previous section is that we are interested in characterizing the covariability in the spatio-temporal data as a function of specific lags in time and in space. Note that the lag in time is a scalar, but the lag in space is a vector (corresponding to the displacement between locations in \\(d\\)-dimensional space).\nConsider the empirical spatio-temporal covariance function for various space and time lags. Here, we make an assumption that the first moment (mean) depends on space but not on time and that the second moment (covariance) depends only on the lag differences in space and time. Then the empirical spatio-temporal covariogram for spatial lag \\(\\mathbf{h}\\) and time lag \\(\\tau\\) is given by\n\\[\n\\widehat{C}_z(\\mathbf{h};\\tau) = \\frac{1}{|N_{\\mathbf{s}}(\\mathbf{h})|}\\frac{1}{|N_t(\\tau)|}\n\\sum_{\\mathbf{s}_i,\\mathbf{s}_k \\in N_{\\mathbf{s}}(\\mathbf{h})}  \\sum_{t_j,t_{\\ell} \\in N_t(\\tau)} (Z(\\mathbf{s}_i;t_j)-\\widehat{\\mu}_{z,s}(\\mathbf{s}_i))(Z(\\mathbf{s}_k;t_\\ell)-\\widehat{\\mu}_{z,s}(\\mathbf{s}_k)),\n\\tag{2.6}\\]\nwhere you will recall that \\(\\widehat{\\mu}_{z,s}(\\mathbf{s}_i) = (1/T) \\sum_{j=1}^T Z(\\mathbf{s}_i;t_j)\\), \\(N_{\\mathbf{s}}(\\mathbf{h})\\) refers to the pairs of spatial locations with spatial lag within some tolerance of \\(\\mathbf{h}\\), \\(N_t(\\tau)\\) refers to the pairs of time points with time lag within some tolerance of \\(\\tau\\), and \\(|N(\\cdot)|\\) refers to the number of elements in \\(N(\\cdot)\\). Under isotropy, one often considers the lag only as a function of distance, \\(h = ||\\mathbf{h}||\\), where \\(|| \\cdot ||\\) is the Euclidean norm (see Appendix A).\n\n\n\n\n\n\nNote 2.1: Semivariogram\n\n\n\nThe semivariogram is defined as \\[\n\\gamma_z(\\mathbf{s}_i, \\mathbf{s}_k; t_j, t_{\\ell}) \\equiv \\frac{1}{2} \\textrm{var}(Z(\\mathbf{s}_i;t_j) - Z(\\mathbf{s}_k;t_{\\ell})).\n\\] In the case where the covariance depends only on displacements in space and differences in time, this can be written as \\[\n\\begin{aligned}\n\\gamma_z(\\mathbf{h};\\tau) &= \\frac{1}{2} \\textrm{var}(Z(\\mathbf{s}+ \\mathbf{h}; t+ \\tau) - Z(\\mathbf{s}; t)) \\\\\n&= C_z(\\mathbf{0};0) - \\textrm{cov}(Z(\\mathbf{s}+ \\mathbf{h}; t + \\tau), Z(\\mathbf{s};t)) \\\\\n&= C_z(\\mathbf{0};0) - C_z(\\mathbf{h};\\tau)\n\\end{aligned}\n\\tag{2.7}\\] where \\(\\mathbf{h}= \\mathbf{s}_k - \\mathbf{s}_i\\) is a spatial lag and \\(\\tau = t_{\\ell} - t_j\\) is a temporal lag.\nNow, Equation 2.7 does not always hold. It is possible that \\(\\gamma_z\\) is a function of spatial lag \\(\\mathbf{h}\\) and temporal lag \\(\\tau\\), but there is no stationary covariance function \\(C_z(\\mathbf{h};\\tau)\\). We generally try to avoid these models of covariability by fitting trend terms that are linear and/or quadratic in spatio-temporal coordinates.\nIf the covariance function of the process is well defined, then the semivariogram is generally characterized by the nugget effect, the sill, and the partial sill. The nugget effect is given by \\(\\gamma_z(\\mathbf{h};\\tau)\\) when \\(\\mathbf{h}\\rightarrow {\\bf 0}\\) and \\(\\tau \\rightarrow 0\\), while the sill is \\(\\gamma_z(\\mathbf{h};\\tau)\\) when \\(\\mathbf{h}\\rightarrow \\infty\\) and \\(\\tau \\rightarrow \\infty\\). The partial sill is the difference between the sill and the nugget effect. The diagram below shows these components of a semivariogram as a function of spatial distance \\(\\|\\mathbf{h}\\|\\).\n\n\n\nIn some kriging applications, one might be interested in looking at the empirical spatio-temporal semivariogram (see Note 2.1). The empirical semivariogram, for the case where the covariance only depends on the displacements in space and the time lags, is obtained from Equation 2.6 as \\(\\widehat{\\gamma}_z(\\mathbf{h};\\tau) = \\widehat{C}_z({\\mathbf{0}};0) - \\widehat{C}_z(\\mathbf{h};\\tau)\\), and so it is easy to go back and forth between the empirical semivariogram and the covariogram in this case (see the caveat in Note 2.1). Assuming a constant spatial mean \\(\\mu_{z,s}\\), then Equation 2.7 can be equivalently written as \\[\n\\gamma_z(\\mathbf{h};\\tau) = \\frac{1}{2} E\\left(Z(\\mathbf{s}+ \\mathbf{h}; t+ \\tau) - Z(\\mathbf{s};t)\\right)^2,\n\\] and hence an alternative estimate is \\[\n\\widehat{\\gamma}_z(\\mathbf{h};\\tau) =  \\frac{1}{2}  \\frac{1}{|N_{\\mathbf{s}}(\\mathbf{h})|}\\frac{1}{|N_t(\\tau)|}\n\\sum_{\\mathbf{s}_i,\\mathbf{s}_k \\in N_{\\mathbf{s}}(\\mathbf{h})}  \\sum_{t_j,t_{\\ell} \\in N_t(\\tau)} (Z(\\mathbf{s}_i;t_j)- Z(\\mathbf{s}_k;t_\\ell))^2,\n\\tag{2.8}\\] where the notation in Equation 2.8 is the same as used above in Equation 2.6. Note that this calculation does not need any information about the spatial means.\n\n\n\n\n\n\nFigure 2.17: Empirical spatio-temporal semivariogram of daily Tmax from the NOAA data set during July 2003, computed using the function variogram in gstat.\n\n\n\n\n\n2.4.3 Empirical Orthogonal Functions (EOFs)\nEmpirical orthogonal functions (EOFs) can reveal spatial structure in spatio-temporal data and can also be used for subsequent dimensionality reduction. EOFs came out of the meteorology/climatology literature, and in the context of discrete space and time, EOF analysis is the spatio-temporal manifestation of principal component analysis (PCA) in statistics (Cressie & Wikle, 2011). In the terminology of this chapter, one should probably modify “EOFs” to empirical spatial orthogonal functions, since they are obtained from an empirical spatial covariance matrix, but for legacy reasons we stick with “EOFs.” Before we discuss EOFs, we give a brief review of PCA.\n\nBrief Review of Principal Component Analysis\nAssume we have two measured traits on a subject of interest (e.g., measurements of \\(x_1 = \\mbox{height}\\) (in cm) and \\(x_2 = \\mbox{weight}\\) (in kg) in a sample of women in the USA). Figure 2.18 (left panel) shows a (simulated) plot of what such data might look like for \\(m=500\\) individuals. We note that these data are quite correlated, as expected. Now, we wish to construct new variables that are linear combinations of the measured traits, say \\(a_1 = w_{11} x_1 + w_{12} x_2\\) and \\(a_2 = w_{21} x_1 + w_{22} x_2\\). One way to think of this is that we are “projecting” the original data onto new axes given by the variables \\(a_1\\) and \\(a_2\\). Figure 2.18 (center and right panels) shows two possible projections, which differ according to the values we choose for the weights, \\(\\{w_{11}, w_{12}, w_{21}, w_{22}\\}\\). Note that in the case of the right-hand panel in Figure 2.18, the new axis \\(a_1\\) aligns with the axis of largest variation, and the new axis \\(a_2\\) corresponds to the axis of largest variation perpendicular (orthogonal) to the axis \\(a_1\\). Maximizing these axes of variation subject to orthogonality helps us think about decomposing the data into lower-dimensional representations in an optimal way. That is, the new variable on the axis \\(a_1\\) represents the optimal linear combination of the data that accounts for the most variation in the original data. If the variation along the other axis (\\(a_2\\)) is fairly small relative to \\(a_1\\), then it might be sufficient just to consider \\(a_1\\) to represent the data.\n\n\n\n\n\n\nFigure 2.18: Simulated height (in cm) versus weight (in kg) for \\(m=500\\) females in the USA (left) with two orthogonal projections (center and right). The right panel shows the optimal PCA projection.\n\n\n\nHow does one go about choosing the weights \\(\\{w_{ij}\\}\\)? Let \\({\\mathbf{x}}_i = (x_{1i},\\ldots, x_{pi})'\\) be a random vector with variance–covariance matrix \\({\\mathbf{C}}_x\\). Note from Appendix A that by spectral decomposition, a \\(p \\times p\\) non-negative-definite, symmetric, real matrix, \\({\\mathbf{\\Sigma}}\\), can be diagonalized such that \\({\\mathbf{W}}' {\\mathbf{\\Sigma}} {\\mathbf{W}} = {\\mathbf{\\Lambda}}\\) (i.e., \\(\\boldsymbol{\\Sigma}= \\mathbf{W}{\\mathbf{\\Lambda}} \\mathbf{W}'\\)), where \\({\\mathbf{\\Lambda}}\\) is a diagonal matrix containing the eigenvalues \\(\\{\\lambda_i\\}\\) of \\({\\mathbf{\\Sigma}}\\) (where \\(\\lambda_1 \\ge \\lambda_2 \\ge \\ldots \\ge \\lambda_p \\ge 0\\)) and \\({\\mathbf{W}} = [{\\mathbf{w}}_1 \\; {\\mathbf{w}}_2 \\; \\ldots \\; {\\mathbf{w}}_p]\\) is the associated matrix of orthogonal eigenvectors, \\(\\{{\\mathbf{w}}_i\\}\\) (i.e., \\(\\mathbf{W}\\mathbf{W}' = \\mathbf{W}' \\mathbf{W}= \\mathbf{I}\\)); thus, \\({\\mathbf{C}}_x = {\\mathbf{W}} {\\mathbf{\\Lambda}}_x {\\mathbf{W}}'\\). It can be shown that these eigenvectors give the optimal weights, so that \\({\\mathbf{w}}_1\\) are the weights for \\(a_1\\) and \\({\\mathbf{w}}_2\\) are the weights for \\(a_2\\), and so on.\nAs an example, consider the variance–covariance matrix associated with the simulated height and weight traits, where \\(p=2\\): \\[\n{\\mathbf{C}}_x = \\left( \\begin{array}{rr}\n81  &  50 \\\\\n50   & 49\n\\end{array} \\right).\n\\] Then \\({\\mathbf{W}}\\) and \\({\\mathbf{\\Lambda}}_x\\) are given (using the function eigen in R) by \\[\n{\\mathbf{W}} = \\left( \\begin{array}{rr}\n-0.8077 & 0.5896 \\\\\n-0.5896 & -0.8077\n\\end{array} \\right), \\quad\n{\\mathbf{\\Lambda}}_x = \\left( \\begin{array}{rr}\n117.5 & 0 \\\\\n0 & 12.5\n\\end{array} \\right).\n\\] So, for each of the observation vectors, \\(\\{{\\mathbf{x}}_i, i=1,\\ldots,500\\}\\), we make new variables \\[\na_{1i} = -0.8077 x_{1i} - 0.5896 x_{2i}\n\\] \\[\na_{2i} = 0.5896 x_{1i} - 0.8077 x_{2i}.\n\\] These coefficients (which are the data projected onto axes \\((a_1,a_2)\\)) are plotted in Figure 2.19. Note that these new variables are uncorrelated (no slant to the points in the plot) and the first axis (\\(a_1\\)) corresponds to the one that has the most variability. In PCA, one sometimes attempts to interpret the “loadings” given by \\(\\{\\mathbf{w}_i: i =1,\\ldots, p\\}\\) (or some scaled version of them). That is, one contrasts the signs and magnitudes of the loadings within a given eigenvector (e.g., the first eigenvector, \\({\\mathbf{w}}_1 = (-0.8077, -0.5896)'\\), suggests that both height and weight are important and vary in the same way, so that the first principal component might represent an overall “size” attribute).\n\n\n\n\n\n\nFigure 2.19: Principal components corresponding to the simulated data in Figure 2.18.\n\n\n\nThe notions presented in the example above extend to more than just two traits and, in general, the principal-component decomposition has some nice properties. For example, the \\(k\\)th eigenvalue is the variance of the associated linear combination of the elements of \\(\\mathbf{x}\\); that is, \\(\\textrm{var}(a_k) = \\textrm{var}({\\mathbf{w}}_k' {\\mathbf{x}})= \\lambda_k\\). In addition, \\[\n\\textrm{var}(x_1) + \\ldots + \\textrm{var}(x_p) = \\textrm{trace}({\\mathbf{C}}_x) = \\lambda_1 + \\ldots + \\lambda_p = \\textrm{var}(a_1) + \\ldots + \\textrm{var}(a_p).\n\\] Thus, one can consider the proportion of the total variance accounted for by the \\(k\\)th principal component, which is \\(\\lambda_k/\\sum_{j=1}^p \\lambda_j\\). In the example above, the first principal component accounts for about 90% of the variance in the original data (i.e., \\(\\lambda_1/(\\lambda_1 + \\lambda_2)= 117.5/130 = 0.90\\)).\nOf course, in practice we would not know the covariance matrix, \\(\\mathbf{C}_x\\), but we can calculate an empirical covariance matrix using Equation 2.4 with \\(\\tau = 0\\), \\(\\{\\mathbf{Z}_{t_j}\\}\\) replaced by \\(\\{\\mathbf{x}_i\\}\\), and \\(\\widehat{\\boldsymbol{\\mu}}_{z,s}\\) replaced by \\((1/500) \\sum_{i=1}^{500} \\mathbf{x}_i\\). In that case, the spectral decomposition of \\(\\widehat{\\mathbf{C}}_x\\) gives empirical estimates of the eigenvectors \\(\\widehat{\\mathbf{W}}\\) and eigenvalues \\(\\widehat{\\boldsymbol{\\Lambda}}_x\\). The analysis then proceeds with these empirical estimates.\n\n\n\n\n\n\nTip\n\n\n\nThe PCA routine prcomp is included with base R. When the plot function is used on an object returned by prcomp, the variances of the principal components are displayed. The function biplot returns a plot showing how the observations relate to the principal components.\n\n\n\n\n2.4.3.1 Empirical Orthogonal Functions\nThe study of EOFs is related to PCA in the sense that the “traits” of the multivariate data vector now are spatially indexed, and the samples are usually taken over time. It is shown in Cressie & Wikle (2011) (Chapter 5) that the EOFs can be obtained from the data through either a spectral decomposition of an empirical (spatial or temporal) covariance matrix or a singular value decomposition (SVD) of a centered data matrix (see Note 2.2).\nLet \\(\\mathbf{Z}_{t_j} \\equiv (Z(\\mathbf{s}_1;t_j),\\ldots,Z(\\mathbf{s}_m;t_j))'\\) for \\(j=1,\\ldots,T\\). Using Equation 2.4 to estimate the lag-0 spatial covariance matrix, \\(\\widehat{\\mathbf{C}}^{(0)}_z\\) (which is symmetric and non-negative-definite), the PCA decomposition is given by the spectral decomposition\n\\[\n\\widehat{\\mathbf{C}}^{(0)}_z = \\boldsymbol{\\Psi}\\boldsymbol{\\Lambda}\\boldsymbol{\\Psi}'\n\\tag{2.9}\\]\nwhere \\(\\boldsymbol{\\Psi}\\equiv (\\boldsymbol{\\psi}_1,\\ldots,\\boldsymbol{\\psi}_m)\\) is a matrix of spatially indexed eigenvectors given by the vectors \\(\\boldsymbol{\\psi}_k \\equiv (\\psi_k(\\mathbf{s}_1),\\ldots,\\psi_k(\\mathbf{s}_m))'\\) for \\(k=1,\\ldots,m\\), and \\(\\boldsymbol{\\Lambda}\\equiv \\textrm{diag}(\\lambda_1,\\ldots,\\lambda_m)\\) is a diagonal matrix of corresponding non-negative eigenvalues (decreasing down the diagonal). The eigenvectors are called “EOFs” and are often plotted as spatial maps (since they are spatially indexed, which is also why \\({\\mathbf{\\Psi}}\\) is used to distinguish them from the more general PCA weights, \\({\\mathbf{W}}\\), above). For \\(k=1,\\ldots,m\\), the so-called \\(k\\)th principal component (PC) time series are given by \\(a_k(t_j) \\equiv \\boldsymbol{\\psi}'_k \\mathbf{Z}_{t_j}\\), where \\(j=1,\\ldots,T\\). From PCA considerations, the EOFs have the nice property that \\(\\boldsymbol{\\psi}_1\\) provides the linear coefficients such that \\(\\textrm{var}(a_1) = \\lambda_1\\) is maximized, \\(\\boldsymbol{\\psi}_2\\) provides the linear coefficients such that \\(\\textrm{var}(a_2) = \\lambda_2\\) accounts for the next largest variance such that \\(\\textrm{cov}(a_1,a_2) = 0\\), and so on. As with the principal components in PCA, the EOFs form a discrete orthonormal basis (i.e., \\(\\boldsymbol{\\Psi}' \\boldsymbol{\\Psi}= \\boldsymbol{\\Psi}\\boldsymbol{\\Psi}' = \\mathbf{I}\\)).\nThere are two primary uses for EOFs. First, it is sometimes the case that one can gain some understanding about important spatial patterns of variability in a sequence of spatio-temporal data by examining the EOF coefficient maps (loadings). But care must be taken not to interpret the EOF spatial structures in terms of dynamical or kinematic properties of the underlying process Monahan et al. (2009). Second, these bases can be quite useful for dimension reduction in a random-effects spatial or spatio-temporal representation (see Section 4.4), although again, in general, they are not “optimal” bases in terms of reduced-order dynamical systems.\n\n\n\n\n\n\nNote 2.2: Calculating EOFs\n\n\n\nAs stated above, EOFs can be calculated directly from the spectral decomposition of the empirical lag-0 spatial covariance matrix (Equation 2.9). However, they are more often obtained directly through a singular value decomposition (SVD, see Appendix A), which provides computational benefits in some situations. To see the equivalence, first we show how to calculate the empirical covariance-based EOFs. Let \\(\\mathbf{Z}\\equiv [\\mathbf{Z}_1,\\ldots,\\mathbf{Z}_T]^\\prime\\) be the \\(T \\times m\\) space-wide data matrix and then let \\(\\widetilde{\\mathbf{Z}}\\) be the “detrended” and scaled data matrix,\n\\[\n\\widetilde{\\mathbf{Z}} \\equiv \\frac{1}{\\sqrt{T-1}}(\\mathbf{Z}- \\mathbf{1}_T \\widehat{\\boldsymbol{\\mu}}_{z,s}^\\prime)\n\\tag{2.10}\\]\nwhere \\(\\mathbf{1}_T\\) is a \\(T\\)-dimensional vector of ones and \\(\\widehat{\\boldsymbol{\\mu}}_{z,s}\\) is the spatial mean vector given by Equation 2.1. Then it is easy to show that\n\\[\n\\mathbf{C}_z^{(0)} = \\widetilde{\\mathbf{Z}}' \\widetilde{\\mathbf{Z}} = {\\boldsymbol{\\Psi}} \\boldsymbol{\\Lambda}\\boldsymbol{\\Psi}'\n\\tag{2.11}\\]\nand the principal component (PC) time series are given by the columns of \\(\\mathbf{A}=  (\\sqrt{T-1}) \\widetilde{\\mathbf{Z}} \\boldsymbol{\\Psi}\\); that is, they are projections of the detrended data matrix onto the EOF basis functions, \\({\\boldsymbol{\\Psi}}\\). The normalized PC time series are then given by \\(\\mathbf{A}_{\\mathrm{norm}} \\equiv \\mathbf{A}\\boldsymbol{\\Lambda}^{-1/2}\\); these are just the PC time series divided by their standard deviation (i.e., the square root of the associated eigenvalue), so that the temporal variance of the normalized time series is equal to one. This normalization allows the \\(m\\) time series to be plotted on the same scale, leaving their relative importance to be captured by their corresponding eigenvalues.\nNow, consider the SVD of the detrended and scaled data matrix,\n\\[\n\\widetilde{\\mathbf{Z}} = \\mathbf{U}\\mathbf{D}\\mathbf{V}'\n\\tag{2.12}\\]\nwhere \\(\\mathbf{U}\\) is the \\(T \\times T\\) matrix of left singular vectors, \\(\\mathbf{D}\\) is a \\(T \\times m\\) matrix containing singular values on the main diagonal, and \\(\\mathbf{V}\\) is an \\(m \\times m\\) matrix containing the right singular vectors, where both \\(\\mathbf{U}\\) and \\(\\mathbf{V}\\) are orthonormal matrices. Upon substituting Equation 2.12 into Equation 2.11, it is easy to see that the EOFs are given by \\(\\boldsymbol{\\Psi}= \\mathbf{V}\\), and \\(\\boldsymbol{\\Lambda}= \\mathbf{D}' \\mathbf{D}\\). In addition, it is straightforward to show that \\(\\mathbf{A}= (\\sqrt{T-1}) \\mathbf{U}\\mathbf{D}\\) and that the first \\(m\\) columns of \\((\\sqrt{T-1}) \\mathbf{U}\\) correspond to the normalized PC time series, \\(\\mathbf{A}_{\\mathrm{norm}}\\). Thus, the advantages of the SVD calculation approach are: (1) we do not need to calculate the empirical spatial covariance matrix; (2) we get the normalized PC time series and EOFs simultaneously; and (3) the procedure still works when \\(T &lt; m\\). The case of \\(T &lt; m\\) can be problematic in the covariance context since then \\(\\mathbf{C}^{(0)}_z\\) is not positive-definite, although, as shown in Cressie & Wikle (2011) (Section 5.3.4), in this case one can still calculate the EOFs and PC time series.\n\n\nFigure 2.20 and Figure 2.21 show the first four EOFs and PC time series for the SST data set. In this case, the number of spatial locations \\(m = 2261\\), and the number of time points \\(T = 399\\). The first four EOFs account for slightly more than 60% of the variation in the data. The EOF spatial patterns show strong variability in the eastern and central tropical Pacific, and they are known to be related to the El Niño and La Niña climate patterns that dominate the tropical Pacific SST variability. The corresponding PC time series (particularly for the first EOF) show time periods at which the data project very strongly on this spatial pattern (both in terms of large positive and large negative values), and it can be shown that these times correspond to strong El Niño and La Niña events, respectively.\n\n\n\n\n\n\nFigure 2.20: The first two empirical orthogonal functions and normalized principal-component time series for the SST data set obtained using an SVD of a space-wide matrix.\n\n\n\n\n\n\n\n\n\nFigure 2.21: The third and fourth empirical orthogonal functions and normalized principal-component time series for the SST data set obtained using an SVD of a space-wide matrix.\n\n\n\nHow many EOFs should one consider? This is a long-standing question in PCA, and there are numerous suggestions. Perhaps the simplest is just to consider the number of EOFs that account for some desired proportion of overall variance. Alternatively, one can produce a scree plot, which is a plot of the relative variance associated with each eigenvalue of the EOF as a function of the index of that EOF (see Figure 2.22), and where the sum of all relative variances is 1. One typically sees a fairly quick drop in relative variance with increasing order of the eigenvalue, and then the variance reduction flattens out. It is sometimes recommended that one only focus on those EOFs before the index that begins the flat part of the curve; this choice of index can be a bit subjective. One can also get a sense as to the “significance” of each component by comparing the relative variances to those in an EOF analysis in which the values for each spatial location are randomly permuted at each time (Hastie et al., 2009, Chapter 14). Then, one plots the scree plot with the actual data superimposed on the permuted data. We recommend that the EOFs retained are around the index at which the two “curves” intersect. For example, the black symbols in Figure 2.22 correspond to the relative variance associated with the first 50 EOFs for the SST data, and the red symbols are the very tight boxplots of relative variances obtained from EOF analyses of 100 random permutations of the data. One can see that by about index 12, the scree plot of the actual data and the boxplots are starting to intersect, suggesting that there is very little “real” variability being accounted for by the EOFs with indices greater than about 12.\n\n\n\n\n\n\nFigure 2.22: Scree plot for the EOF analysis of the SST data. The black symbols correspond to the relative variance associated with the ordered eigenvalues. The red symbols correspond to (very tight) boxplots of the relative variance associated with the eigenvalues from 100 EOF analyses in which the SST values at the spatial locations were randomly permuted for each time point.\n\n\n\n\n\n2.4.3.2 Some Technical Comments on Empirical Orthogonal Functions\nThe EOF decomposition is sometimes derived in a continuous-space context through a Karhunen–Loève expansion, with eigenvalues and eigenfunctions obtained through a solution of a Fredholm integral equation (Cressie & Wikle (2011), Section 5.3). This is relevant, as it shows why one should account for the area/support associated with each spatial observation when working in a discrete-space EOF environment. In particular, one should multiply the elements of the eigenvectors by the square root of the length, area, or volume of the spatial support associated with that spatial observation (e.g., Cohen & Jones (1969)). For example, consider spatial location \\(\\mathbf{s}_i\\); for each of the \\(k\\) eigenvectors, one should multiply \\(\\psi_k(\\mathbf{s}_i)\\) by \\(\\sqrt{e}_i\\), where \\(e_i\\) is the length, area, or volume associated with location \\(\\mathbf{s}_i\\) (and we assume that not all of the \\(\\{e_i\\}\\) are identical). This modification to the eigenvectors \\(\\boldsymbol{\\psi}_1,\\ldots,\\boldsymbol{\\psi}_k\\) must be done before calculating the PC time series.\nAlthough most EOF analyses in the spatio-temporal context consider spatial EOFs and PC time series, one can certainly consider the analogous decomposition in which the EOFs are time-series bases and the projection of the data onto these bases is given by PC spatial fields. Implementation is straightforward – one either works with the temporal covariance matrix (averaging over spatial location) or considers the SVD of an \\(m \\times T\\) (temporally detrended) data matrix. EOF time series are used as temporal basis functions in a spatio-temporal model in Lab 4.3.\nIt is also important to note that in cases where EOF analysis is used for dimension reduction (see Section 4.3), it is often necessary to either interpolate the EOFs in a sensible manner (e.g., Obled & Creutin (1986)) or “pre-interpolate” the data onto a finely gridded spatial domain.\nFinally, there are many extensions to the basic EOF analysis presented here, including so-called complex EOFs, cyclostationary EOFs, multivariate EOFs, and extended EOFs. These all have particular utility depending on the type of data and the goal of the analysis. For example, complex EOFs are used for trying to identify propagating features that account for a significant amount of variation in the data. Cyclostationary EOFs are appropriate when there are strong periodicities in the data and spatial variation is expected to shift dramatically within this periodicity. Multivariate EOFs are considered when multivariate spatial data are observed at the same time points. Extended EOFs are useful for understanding spatial patterns associated with temporal lags. These methods are described in more detail in Cressie & Wikle (2011) (Section 5.3) and the references therein. In Lab 2.3 we will demonstrate the “classic” EOF analysis in R.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Exploring Spatio-Temporal Data</span>"
    ]
  },
  {
    "objectID": "Chapter2.html#spatio-temporal-canonical-correlation-analysis",
    "href": "Chapter2.html#spatio-temporal-canonical-correlation-analysis",
    "title": "2  Exploring Spatio-Temporal Data",
    "section": "2.5 Spatio-Temporal Canonical Correlation Analysis",
    "text": "2.5 Spatio-Temporal Canonical Correlation Analysis\nIn multivariate statistics, canonical correlation analysis (CCA) seeks to create new variables that are linear combinations of two multivariate data sets (separately) such that the correlations between these new variables are maximized (e.g., Hotelling (1936)). Such methods can be extended to the case where the two data sets are indexed in space and time, typically where a spatial location corresponds to a “trait” in a multivariate set of “traits” (this terminology is borrowed from psychometrics). Time corresponds to the samples. (Note that just as with EOFs, one can reverse the roles of space and time in this setting as well.) A spatio-temporal CCA (ST-CCA) is given below where spatial location corresponds to the multivariate trait.\nAssume that we have two data sets that have the same temporal domain of interest but potentially different spatial domains. In particular, consider the data sets given by the collection of spatial vectors \\(\\{\\mathbf{Z}_{t_j} \\equiv (Z(\\mathbf{s}_1;t_j),\\ldots, Z(\\mathbf{s}_m;t_j))' : j=1,\\ldots,T\\}\\), and \\(\\{\\mathbf{X}_{t_j} \\equiv (X(\\mathbf{r}_1;t_j),\\ldots,X(\\mathbf{r}_n;t_j))' : j=1,\\ldots, T\\}\\). Now, consider the two new variables that are linear combinations of \\(\\mathbf{Z}_{t_j}\\) and \\(\\mathbf{X}_{t_j}\\), respectively:\n\\[\na_k(t_j) = \\sum_{i=1}^m \\xi_{ik} \\; Z(\\mathbf{s}_i;t_j) = \\boldsymbol{\\xi}_k^\\prime \\mathbf{Z}_{t_j},\n\\tag{2.13}\\]\n\\[\nb_k(t_j) = \\sum_{\\ell=1}^n  \\psi_{\\ell k} \\; X(\\mathbf{r}_\\ell;t_j) = \\boldsymbol{\\psi}_k^\\prime \\mathbf{X}_{t_j},\n\\tag{2.14}\\]\nFor suitable choices of weights (see below), the \\(k\\)th canonical correlation, for \\(k=1,2,\\ldots,\\min\\{n,m\\}\\), is then simply the correlation between \\(a_k\\) and \\(b_k\\),\n\\[\nr_k \\equiv \\mathrm{corr}(a_k,b_k) =  \\frac{\\mathrm{cov}(a_k,b_k)}{\\sqrt{\\mathrm{var}(a_k)}\\sqrt{\\mathrm{var}(b_k)}},\n\\]\nwhich can also be written as\n\\[\nr_k = \\frac{\\boldsymbol{\\xi}_k^\\prime \\mathbf{C}^{(0)}_{z,x}\\boldsymbol{\\psi}_k}{(\\boldsymbol{\\xi}_k^\\prime \\mathbf{C}_z^{(0)} \\boldsymbol{\\xi}_k)^{1/2}(\\boldsymbol{\\psi}_k^\\prime \\mathbf{C}_x^{(0)} \\boldsymbol{\\psi}_k)^{1/2}},\n\\tag{2.15}\\]\nwhere the variance–covariance matrices \\(\\mathbf{C}^{(0)}_z\\) and \\(\\mathbf{C}^{(0)}_x\\) are of dimension \\(m \\times m\\) and \\(n \\times n\\), respectively, and the cross-covariance matrix \\(\\mathbf{C}^{(0)}_{z,x} \\equiv\\mathrm{cov}(\\mathbf{Z},\\mathbf{X})\\) has dimension \\(m \\times n\\). So the first pair of canonical variables corresponds to the weights \\({\\boldsymbol{\\xi}}_1\\) and \\({\\boldsymbol{\\psi}}_1\\) that maximize \\(r_1\\) in Equation 2.15. In addition, we standardize these weights such that the new canonical variables have unit variance. Given this first pair of canonical variables, we can then find a second pair, \\({\\boldsymbol{\\xi}}_2\\) and \\(\\boldsymbol{\\psi}_2\\), associated with \\(\\{a_2, b_2\\}\\) that are uncorrelated with \\(\\{a_1,b_1\\}\\), have unit variance, and maximize \\(r_2\\) in Equation 2.15. This procedure continues so that the \\(k\\)th set of canonical variables are the linear combinations, \\(\\{a_k,b_k\\}\\), that have unit variance, are uncorrelated with the previous \\(k-1\\) canonical variable pairs, and maximize \\(r_k\\) in Equation 2.15. A specific procedure for calculating ST-CCA is given in Note 2.3.\nBecause the weights given by \\({\\boldsymbol{\\xi}}_k\\) and \\({\\boldsymbol{\\psi}}_k\\) are indexed in space, they can be plotted as spatial maps, and the associated canonical variables can be plotted as time series. From an interpretation perspective, the time series of the first few canonical variables typically match up fairly closely (given they are optimized to maximize correlation), and the spatial patterns in the weights show the areas in space that are most responsible for the high correlations. Like EOFs, principal components, and other such approaches, one has to be careful with the interpretation of canonical variables beyond the first pair, given the restriction that CCA time series are uncorrelated. In addition, given that high canonical correlations within a canonical pair naturally result from this procedure, one has to be careful in evaluating the importance of that correlation. One way to do this is to randomly permute the spatial locations in the \\({\\mathbf{Z}}_{t_j}\\) and \\({\\mathbf{X}}_{t_j}\\) data vectors (separately) and recalculate the ST-CCA many times, thereby giving a permutation-based range of canonical correlations when there is no real structural relationship between the variables.\nIn addition to the consideration of two separate data sets, one can perform an ST-CCA between \\({\\mathbf{Z}}_{t_j}\\) and, say, \\({\\mathbf{X}}_{t_j} \\equiv {\\mathbf{Z}}_{t_j-\\tau}\\), a \\(\\tau\\)-lagged version of the \\({\\mathbf{Z}}_{t_j}\\) data. This “one-field ST-CCA” is often useful for exploratory data analysis or for generating a forecast of a spatial field. Some binning of the spatio-temporal data into temporal bins lagged by \\(\\tau\\) may be needed in practice.\nFinally, in practice, because the covariance matrices required to implement ST-CCA are often fairly noisy (and even singular), depending on the sample size, we typically first project the data into a lower dimension using EOFs for computational stability (see Cressie & Wikle (2011), Section 5.6.1). This is the approach we take in Lab 2.3.\nAs an example of ST-CCA, we consider a one-field ST-CCA on the SST data set. In particular, we are interested in forecasting SST seven months in the future, so we let the data \\(\\mathbf{X}\\) be the lag \\(\\tau = 7\\) month SST data and the data \\(\\mathbf{Z}\\) be the same SSTs with no lag. However, because \\(T &lt; \\max\\{m,n\\}\\) for these data, we first project the data onto the first 10 EOFs (which account for about 74% of the variance in the data). For the projected data, Figure 2.23 shows the first canonical variables (i.e., \\(\\{a_1(t_j),b_1(t_j): j=1,\\ldots,T\\}\\)), plotted as individual time series and which correspond to a canonical correlation of \\(r_1 = 0.843\\). Figure 2.24 shows the corresponding spatial-weights maps for \\({\\boldsymbol{\\xi}}_1\\) and \\({\\boldsymbol{\\psi}}_1\\), respectively. In this example, it can be seen from the time-series plots that the series are quite highly correlated, and it can be shown that the large peaks correspond to known El Niño Southern Oscillation (ENSO) events. Similarly, the left panel of Figure 2.24 suggests a precursor pattern to the SST field in the right panel.\n\n\n\n\n\n\nNote 2.3: Calculating ST-CCA\n\n\n\nFirst, let \\(k=1\\) and, because \\(\\mathbf{C}_z^{(0)}\\) and \\(\\mathbf{C}_x^{(0)}\\) are positive-definite, note that we can write \\(\\mathbf{C}_z^{(0)} = (\\mathbf{C}_z^{(0)})^{1/2}(\\mathbf{C}_z^{(0)})^{1/2}\\) and \\(\\mathbf{C}_x^{(0)} = (\\mathbf{C}_x^{(0)})^{1/2}(\\mathbf{C}_x^{(0)})^{1/2}\\) (see Appendix A). Thus, from Equation 2.15, the square of the canonical correlation can be written as\n\\[\nr_1^2 = \\frac{[\\widetilde{\\boldsymbol{\\xi}}_1^\\prime (\\mathbf{C}_z^{(0)})^{-1/2} \\mathbf{C}_{z,x}^{(0)} (\\mathbf{C}_x^{(0)})^{-1/2} \\widetilde{\\boldsymbol{\\psi}}_1]^2}{(\\widetilde{\\boldsymbol{\\xi}}_1^\\prime \\widetilde{\\boldsymbol{\\xi}}_1)(\\widetilde{\\boldsymbol{\\psi}}_1^\\prime \\widetilde{\\boldsymbol{\\psi}}_1)},\n\\tag{2.16}\\]\nwith \\(\\widetilde{\\boldsymbol{\\xi}}_1 \\equiv (\\mathbf{C}_z^{(0)})^{1/2} \\boldsymbol{\\xi}_1\\) and \\(\\widetilde{\\boldsymbol{\\psi}}_1 \\equiv (\\mathbf{C}_x^{(0)})^{1/2} \\boldsymbol{\\psi}_1\\). In the multivariate statistics literature (e.g., Johnson & Wichern (1992), p. 463), it is well known that \\(r_1^2\\) corresponds to the largest singular value of the singular value decomposition (SVD; see Appendix A) of\n\\[\n(\\mathbf{C}_z^{(0)})^{-1/2} \\mathbf{C}_{z,x}^{(0)} (\\mathbf{C}_x^{(0)})^{-1/2},\n\\tag{2.17}\\]\nwhere the normalized weight vectors \\(\\widetilde{\\boldsymbol{\\xi}}_1\\) and \\(\\widetilde{\\boldsymbol{\\psi}}_1\\) are the left and right singular vectors, respectively. Then we can obtain the unnormalized weights through \\(\\boldsymbol{\\xi}_1 \\equiv (\\mathbf{C}_z^{(0)})^{-1/2} \\widetilde{\\boldsymbol{\\xi}}_1\\) and \\(\\boldsymbol{\\psi}_1 \\equiv (\\mathbf{C}_x^{(0)})^{-1/2} \\widetilde{\\boldsymbol{\\psi}}_1\\), respectively. As mentioned above, these are the first ST-CCA pattern maps. The corresponding time series of ST-CCA canonical variables are then calculated directly from \\(a_1(t_j) = \\boldsymbol{\\xi}_1^\\prime \\mathbf{Z}_{t_j}\\) and \\(b_1(t_j)\n        = \\boldsymbol{\\psi}_1^\\prime \\mathbf{X}_{t_j}\\), for \\(j=1,\\ldots,T\\). More generally, \\(\\widetilde{\\boldsymbol{\\xi}}_k\\) and \\(\\widetilde{\\boldsymbol{\\psi}}_k\\) correspond to the left and right singular vectors associated with the \\(k\\)th singular value (\\(r^2_k\\)) in the SVD of Equation 2.17. Then the unnormalized spatial-weights maps and the canonical time series are obtained analogously to the \\(k=1\\) case.\nIn practice, to evaluate the SVD in Equation 2.17, we must first calculate the empirical covariance matrices \\(\\widehat{\\mathbf{C}}_z^{(0)}\\), \\(\\widehat{\\mathbf{C}}_x^{(0)}\\) using Equation 2.4, as well as the empirical cross-covariance matrix \\(\\widehat{\\mathbf{C}}_{z,x}^{(0)}\\) given by Equation 2.5. Finally, we consider the SVD of \\((\\widehat{\\mathbf{C}}_z^{(0)})^{-1/2} \\widehat{\\mathbf{C}}_{z,x}^{(0)} (\\widehat{\\mathbf{C}}_x^{(0)})^{-1/2}\\). As mentioned in the text, the empirical covariance matrices can be unstable (or singular) unless \\(T \\gg \\max(n,m)\\), and so it is customary to work in EOF space; that is, project the data for one or both variables onto a lower-dimensional space given by a relatively few EOFs before carrying out ST-CCA.\n\n\n\n\n\n\n\n\nFigure 2.23: Time series of the first canonical variables, {a_1,b_1}, for \\(\\tau=7\\) month lagged monthly SST anomalies at time \\(t_j-\\tau\\) (blue) and at time \\(t_j\\) (red).\n\n\n\n\n\n\n\n\n\nFigure 2.24: Spatial-weights maps corresponding to the linear combination of EOFs used to construct the canonical variables for SST data lagged \\(\\tau=7\\) months (left) and unlagged SST data (right).",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Exploring Spatio-Temporal Data</span>"
    ]
  },
  {
    "objectID": "Chapter2.html#chapter-2-wrap-up",
    "href": "Chapter2.html#chapter-2-wrap-up",
    "title": "2  Exploring Spatio-Temporal Data",
    "section": "2.6 Chapter 2 Wrap-Up",
    "text": "2.6 Chapter 2 Wrap-Up\nThere were three main goals in this chapter. First, we wanted to expose the reader to some basic ideas about data structures in R that are useful for working with spatio-temporal data. Next, we wanted to illustrate some useful ways to visualize spatio-temporal data, noting that it can be particularly challenging to visualize dynamical evolution of spatial fields either without collapsing the spatial component onto one spatial dimension (e.g., as with the Hovmöller plots) or through animation. Finally, we wanted to describe some standard ways to explore spatio-temporal data in preparation for developing models in Chapter 3. In particular, we discussed the exploration of the first moments (means) in space or time, and the second-order structures (covariances) either jointly in space and time, or averaged over one of the dimensions (usually the time dimension) to give covariance and cross-covariance matrices. Stepping up the technical level, we considered eigenvector approaches to explore the structure and potentially reduce the dimensionality of the spatio-temporal data. Specifically, we considered EOFs and ST-CCA. Of these, the EOFs are the most ubiquitous in the literature. Even if the technical details were a bit elaborate, the end result is a powerful and interpretable visualization and exploration of spatio-temporal variability.\nYou now have the survival skills to start building statistical models for spatio-temporal data, with the goal of spatial prediction, parameter inference, or temporal forecasting. In subsequent chapters, spatio-temporal statistical models will be discussed from an introductory perspective in Chapter 3, from a descriptive perspective in Chapter 4, and from a dynamic perspective in Chapter 5.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Exploring Spatio-Temporal Data</span>"
    ]
  },
  {
    "objectID": "Chapter2.html#lab-2.1-data-wrangling",
    "href": "Chapter2.html#lab-2.1-data-wrangling",
    "title": "2  Exploring Spatio-Temporal Data",
    "section": "Lab 2.1: Data Wrangling",
    "text": "Lab 2.1: Data Wrangling\n\n\nSpatio-temporal modeling and prediction generally involve substantial amounts of data that are available to the user in a variety of forms, but more often than not as tables in CSV files or text files. A considerable amount of time is usually spent in loading the data and pre-processing them in order to put them into a form that is suitable for analysis. Fortunately, there are several packages in R that help the user achieve these goals quickly; here we focus on the packages dplyr and tidyr, which contain functions particularly suited for the data manipulation techniques that are required. We first load the required packages, as well as STRbook (visit https://spacetimewithr.org for instructions on how to install STRbook).\n\n\nlibrary(\"dplyr\")\nlibrary(\"tidyr\")\nlibrary(\"STRbook\")\n\n\nAs a running example, we shall consider a data set from the National Oceanic and Atmospheric Administration (NOAA) that was provided to us as text in data tables available with the package STRbook. There are six data tables:\n\nStationinfo.dat. This table contains 328 rows (one for each station) and three columns (station ID, latitude coordinate, and longitude coordinate) containing information on the stations’ locations.\nTimes_1990.dat. This table contains 1461 rows (one for each day between 01 January 1990 and 30 December 1993) and four columns (Julian date, year, month, day) containing the data time stamps.\nTmax_1990.dat. This table contains 1461 rows (one for each time point) and 328 columns (one for each station location) containing all maximum temperature data with missing values coded as \\(-9999\\).\nTmin_1990.dat. Same as Tmax_1990.dat but containing minimum temperature data.\nTDP_1990.dat. Same as Tmax_1990.dat but containing temperature dew point data with missing values coded as \\(-999.90001\\).\nPrecip_1990.dat. Same as Tmax_1990.dat but containing precipitation data with missing values coded as \\(-99.989998\\).\n\nThe first task is to reconcile all these data into one object. Before seeing how to use the spatio-temporal data classes to do this, we first consider the rather simpler task of reconciling them into a standard R data frame in long format.\n\nWorking with Spatio-Temporal Data in Long Format\nThe station locations, time stamps and maximum temperature data can be loaded into R from STRbook as follows.\n\n\nlocs &lt;- read.table(system.file(\"extdata\", \"Stationinfo.dat\",\n                               package = \"STRbook\"),\n                  col.names = c(\"id\", \"lat\", \"lon\"))\nTimes &lt;- read.table(system.file(\"extdata\", \"Times_1990.dat\",\n                                package = \"STRbook\"),\n                  col.names = c(\"julian\", \"year\", \"month\", \"day\"))\nTmax &lt;- read.table(system.file(\"extdata\", \"Tmax_1990.dat\",\n                               package = \"STRbook\"))\n\n\nIn this case, system.file and its arguments are used to locate the data within the package STRbook, while read.table is the most important function used in R for reading data input from text files. By default, read.table assumes that data items are separated by a blank space, but this can be changed using the argument sep. Other important data input functions worth looking up include read.csv for comma-separated value files, and read.delim.\nAbove we have added the column names to the data locs and Times since these were not available with the original text tables. Since we did not assign column names to Tmax, the column names are the default ones assigned by read.table, that is, V1, V2, \\(\\dots\\), V328. As these do not relate to the station ID in any way, we rename these columns as appropriate using the data in locs.\n\n\nnames(Tmax) &lt;- locs$id\n\n\nThe other data can be loaded in a similar way to Tmax; we denote the resulting variables as Tmin, TDP, and Precip, respectively. One can, and should, use the functions head and tail to check that the loaded data are sensible.\n\n\nConsider now the maximum-temperature data in the NOAA data set. Since each row in Tmax is associated with a time point, we can attach it columnwise to the data frame Times using cbind.\n\n\nTmax &lt;- cbind(Times, Tmax)\nhead(names(Tmax), 10)\n\n [1] \"julian\" \"year\"   \"month\"  \"day\"    \"3804\"   \"3809\"   \"3810\"   \"3811\"  \n [9] \"3812\"   \"3813\"  \n\n\n\nNow Tmax contains the time information in the first four columns and temperature data in the other columns. To put Tmax into long format we need to identify a pair. In our case, the data are in space-wide format where the are the station IDs and the are the maximum temperatures (which we store in a field named z). The function we use to put the data frame into long format is gather. This function takes the data as first argument, the key–value pair, and then the next arguments are the names of any columns to exclude as values (in this case those relating to the time stamp).\n\n\nTmax_long &lt;- gather(Tmax, id, z, -julian, -year, -month, -day)\nhead(Tmax_long)\n\n  julian year month day   id  z\n1 726834 1990     1   1 3804 35\n2 726835 1990     1   2 3804 42\n3 726836 1990     1   3 3804 49\n4 726837 1990     1   4 3804 59\n5 726838 1990     1   5 3804 41\n6 726839 1990     1   6 3804 45\n\n\n\nNote how gather has helped us achieve our goal: we now have a single row per measurement and multiple rows may be associated with the same time point. As is, the column id is of class character since it was extracted from the column names. Since the station ID is an integer it is more natural to ensure the field is of class integer.\n\n\nTmax_long$id &lt;- as.integer(Tmax_long$id)\n\n\nThere is little use to keep missing data (coded as \\(-9999\\) in our case) when the data are in long format. To filter out these data we can use the function filter. Frequently it is better to use an criterion (e.g., less than) when filtering in this way rather than an criterion (is equal to) due to truncation error when storing data. This is what we do below, and filter out data with values less than \\(-9998\\) rather than data with values equal to \\(-9999\\). This is particularly important when processing the other variables, such as precipitation, where the missing value is \\(-99.989998\\).\n\n\nnrow(Tmax_long)\n\n[1] 479208\n\nTmax_long &lt;- filter(Tmax_long, !(z &lt;= -9998))\nnrow(Tmax_long)\n\n[1] 196253\n\n\n\nNote how the number of rows in our data set (returned from the function nrow) has now decreased by more than half. One may also use the R function subset; however, filter tends to be faster for large data sets. Both subset and filter take a logical expression as instruction on how to filter out unwanted rows. As with gather, the column names in the logical expression do not appear as strings. In R this method of providing arguments is known as , and we shall see several instances of it in the course of the Labs.\nNow assume we wish to include minimum temperature and the other variables inside this data frame too. The first thing we need to do is first make sure every measurement z is attributed to a process. In our case, we need to add a column, say proc, indicating what process the measurement relates to. There are a few ways in which to add a column to a data frame; here we shall introduce the function mutate, which will facilitate operations in the following Labs.\n\n\nTmax_long &lt;- mutate(Tmax_long, proc = \"Tmax\")\nhead(Tmax_long)\n\n  julian year month day   id  z proc\n1 726834 1990     1   1 3804 35 Tmax\n2 726835 1990     1   2 3804 42 Tmax\n3 726836 1990     1   3 3804 49 Tmax\n4 726837 1990     1   4 3804 59 Tmax\n5 726838 1990     1   5 3804 41 Tmax\n6 726839 1990     1   6 3804 45 Tmax\n\n\n\n\n\nNow repeat the same procedure with the other variables to obtain data frames Tmin_long, TDP_long, and Precip_long (remember the different codings for the missing values!). To save time, the resulting data frames can also be loaded directly from STRbook as follows.\n\n\ndata(Tmin_long, package = \"STRbook\")\ndata(TDP_long, package = \"STRbook\")\ndata(Precip_long, package = \"STRbook\")\n\n\nWe can now construct our final data frame in long format by simply concatenating all these (rowwise) together using the function rbind.\n\n\nNOAA_df_1990 &lt;- rbind(Tmax_long, Tmin_long, TDP_long, Precip_long)\n\n\nThere are many advantages of having data in long form. For example, it makes grouping and summarizing particularly easy. Let us say we want to find the mean value for each variable in each year. We do this using the functions group_by and summarise. The function group_by creates a , while summarise does an operation .\n\n\nsumm &lt;- group_by(NOAA_df_1990, year, proc) %&gt;%  # groupings\n        summarise(mean_proc = mean(z))          # operation\n\n`summarise()` has grouped output by 'year'. You can override using the\n`.groups` argument.\n\n\n\nAlternatively, we may wish to find out the number of days on which it did not rain at each station in June of every year. We can first filter out the other variables and then use summarise.\n\n\nNOAA_precip &lt;- filter(NOAA_df_1990, proc == \"Precip\" & month == 6)\nsumm &lt;- group_by(NOAA_precip, year, id) %&gt;%\n        summarise(days_no_precip = sum(z == 0))\n\n`summarise()` has grouped output by 'year'. You can override using the\n`.groups` argument.\n\nhead(summ)\n\n# A tibble: 6 × 3\n# Groups:   year [1]\n   year    id days_no_precip\n  &lt;int&gt; &lt;int&gt;          &lt;int&gt;\n1  1990  3804             19\n2  1990  3810             26\n3  1990  3811             21\n4  1990  3812             24\n5  1990  3813             25\n6  1990  3816             23\n\n\n\nThe median number of days with no recorded precipitation was\n\n\nmedian(summ$days_no_precip)\n\n[1] 20\n\n\n\nIn the R~code above, we have used the operator $&gt;$, known as the operator. This operator has its own nuances and should be used with care, but we find it provides a clear desciption of the processing pipeline a data set is passed through. We shall always use this operator as x $&gt;$ f(y), which is shorthand for f(x,y). For example, the June summaries above can be found equivalently using the commands\n\n\ngrps &lt;- group_by(NOAA_precip, year, id)\nsumm &lt;- summarise(grps, days_no_precip = sum(z == 0))\n\n\nThere are other useful commands in dplyr that we use in other Labs. First, the function arrange sorts by a column. For example, NOAA_df_1990 is sorted first by station ID, and then by time (Julian date). The following code sorts the data first by time and then by station ID.\n\n\nNOAA_df_sorted &lt;- arrange(NOAA_df_1990, julian, id)\n\n\nCalling head``(NOAA_df_sorted) reveals that no measurements on temperature dew point are available for the first few days of the data set.\nAnother useful function is select, which can be used to select or discard columns. For example, in the following, df1 selects only the Julian date and the measurement while df2 contains all columns except the Julian date.\n\n\ndf1 &lt;- select(NOAA_df_1990, julian, z)\ndf2 &lt;- select(NOAA_df_1990, -julian)\n\n\nAt present, our long data frame contains no spatial information attached to it. However, for each station ID we have an associated coordinate in the data frame locs. We can merge locs to NOAA_df_1990 using the function left_join; this is considerably faster than the function merge. With left_join we need to supply the column field name by which we are merging. In our case, the field common to both data sets is \"id\".\n\n\nNOAA_df_1990 &lt;- left_join(NOAA_df_1990, locs, by = \"id\")\n\n\nFinally, it may be the case that one wishes to revert from long format to either space-wide or time-wide format. The reverse function of gather is spread. This also works by identifying the key–value pair in the data frame; the values are then “widened” into a table while the keys are used to label the columns. For example, the code below constructs a space-wide data frame of maximum temperatures, with each row denoting a different date and each column containing data z from a specific station id.\n\n\nTmax_long_sel &lt;- select(Tmax_long, julian, id, z)\nTmax_wide &lt;- spread(Tmax_long_sel, id, z)\ndim(Tmax_wide)\n\n[1] 1461  138\n\n\n\nThe first column is the Julian date. Should one wish to construct a standard matrix containing these data, then one can simply drop this column and convert as follows.\n\n\nM &lt;- select(Tmax_wide, -julian) %&gt;% as.matrix()\n\n\n\n\nWorking with Spatio-Temporal Data Classes\nNext, we convert the data into objects of class STIDF and STFDF; in these class names “DF” is short for “data frame,” which indicates that in addition to the spatio-temporal locations (which only need STI or STF objects), the objects will also contain data. These classes are defined in the package spacetime. Since sometimes we construct spatio-temporal objects using spatial objects we also need to load the package sp. For details on these classes see Pebesma (2012).\n\n\nlibrary(\"sp\")\nlibrary(\"spacetime\")\n\n\n\nConstructing an STIDF Object\nThe spatio-temporal object for irregular data, STIDF, can be constructed using two functions: stConstruct and STIDF. Let us focus on the maximum temperature in Tmax_long. The only thing we need to do before we call stConstruct is to define a formal time stamp from the year,month,day fields. First, we construct a field with the date in year–month–day format using the function paste, which concatenates strings together. Instead of typing NOAA_df_1990$year, NOAA_df_1990$month and NOAA_df_1990$day we embed the paste function within the function with to reduce code length.\n\n\nNOAA_df_1990$date &lt;- with(NOAA_df_1990,\n                       paste(year, month, day, sep = \"-\"))\nhead(NOAA_df_1990$date, 4)   # show first four elements\n\n[1] \"1990-1-1\" \"1990-1-2\" \"1990-1-3\" \"1990-1-4\"\n\n\n\nThe field date is of type character. This field can now be converted into a Date object using as.Date.\n\n\nNOAA_df_1990$date &lt;- as.Date(NOAA_df_1990$date)\nclass(NOAA_df_1990$date)\n\n[1] \"Date\"\n\n\n\nNow we have everything in place to construct the spatio-temporal object of class STIDF for maximum temperature. The easiest way to do this is using stConstruct, in which we provide the data frame in long format and indicate which are the spatial and temporal coordinates. This is the bare minimum required for constructing a spatio-temporal data set.\n\n\nTmax_long2 &lt;- filter(NOAA_df_1990, proc == \"Tmax\")\nSTObj &lt;- stConstruct(x = Tmax_long2,           # data set\n                     space = c(\"lon\", \"lat\"),  # spatial fields\n                     time = \"date\")            # time field\nclass(STObj)\n\n[1] \"STIDF\"\nattr(,\"package\")\n[1] \"spacetime\"\n\n\n The function class can be used to confirm we have successfully generated an object of class STIDF. There are several other options that can be used with stConstruct. For example, one can set the coordinate reference system or specify whether the time field indicates an instance or an interval. Type help``(stConstruct) into the R console for more details.\nThe function STIDF is slightly different from stConstruct as it requires one to also specify the spatial part as an object of class Spatial from the package sp. In our case, the spatial component is simply an object containing irregularly spaced data, which in the package sp is a SpatialPoints object. A SpatialPoints object may be constructed using the function SpatialPoints and by supplying the coordinates as arguments. As with stConstruct, several other arguments can also be supplied to SpatialPoints; see the help file of SpatialPoints for more details.\n\n\nspat_part &lt;- SpatialPoints(coords = Tmax_long2[, c(\"lon\", \"lat\")])\ntemp_part &lt;- Tmax_long2$date\nSTObj2 &lt;- STIDF(sp = spat_part,\n                time = temp_part,\n                data = select(Tmax_long2, -date, -lon, -lat))\nclass(STObj2)\n\n[1] \"STIDF\"\nattr(,\"package\")\n[1] \"spacetime\"\n\n\n\n\n\nConstructing an STFDF Object\nA similar approach can be used to construct an STFDF object instead of an STIDF object. When the spatial points are fixed in time, we only need to provide as many spatial co-ord-in-ates as there are spatial points, in this case those of the station locations. We also need to provide the regular time stamps, that is, one for each day between 01 January 1990 and 30 December 1993. Finally, the data can be provided both in space-wide or time-wide format with stConstruct, and in long format with STFDF. Here we show how to use STFDF.\nThe spatial and temporal parts can be obtained from the original data as follows.\n\n\nspat_part &lt;- SpatialPoints(coords = locs[, c(\"lon\", \"lat\")])\ntemp_part &lt;- with(Times,\n                   paste(year, month, day, sep = \"-\"))\ntemp_part &lt;- as.Date(temp_part)\n\n\nThe data need to be provided in long format, but now they must contain all the missing values too since a data point must be provided for every spatial and temporal combination. To get the data into long format we use gather.\n\n\nTmax_long3 &lt;- gather(Tmax, id, z, -julian, -year, -month, -day)\n\n\nIt is very important that the data frame in long format supplied to STFDF has the spatial index moving faster than the temporal index, and that the order of the spatial index is the same as that of the spatial component supplied.\n\n\nTmax_long3$id &lt;- as.integer(Tmax_long3$id)\nTmax_long3 &lt;- arrange(Tmax_long3,julian,id)\n\n\nConfirming that the spatial ordering in Tmax_long3 is the correct one can be done as follows.\n\n\nall(unique(Tmax_long3$id) == locs$id)\n\n[1] TRUE\n\n\n\nWe are now ready to construct the STFDF.\n\n\nSTObj3 &lt;- STFDF(sp = spat_part,\n                time = temp_part,\n                data = Tmax_long3)\nclass(STObj3)\n\n[1] \"STFDF\"\nattr(,\"package\")\n[1] \"spacetime\"\n\n\n\nSince we will be using STObj3 often in the Labs we further equip it with a coordinate reference system (see Bivand et al. (2013) for details on these reference systems),\n\n\nproj4string(STObj3) &lt;- CRS(\"+proj=longlat +ellps=WGS84\")\n\n\nand replace the missing values (currently coded as \\(-9999\\)) with NAs.\n\n\nSTObj3$z[STObj3$z == -9999] &lt;- NA\n\n\nFor ease of access, this object is saved as a data file in STRbook and can be loaded using the command data(\"STObj3\", package = \"STRbook\").",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Exploring Spatio-Temporal Data</span>"
    ]
  },
  {
    "objectID": "Chapter2.html#lab-2.2-visualization",
    "href": "Chapter2.html#lab-2.2-visualization",
    "title": "2  Exploring Spatio-Temporal Data",
    "section": "Lab 2.2: Visualization",
    "text": "Lab 2.2: Visualization\nIn this Lab we shall visualize maximum temperature data in the NOAA data set. Specifically, we consider the maximum recorded temperature between May 1993 and September 1993 (inclusive). The packages we need are animation, dplyr, ggplot2, gstat, maps, and STRbook.\n\n\nlibrary(\"animation\")\nlibrary(\"dplyr\")\nlibrary(\"ggplot2\")\nlibrary(\"gstat\")\nlibrary(\"maps\")\nlibrary(\"STRbook\")\n\n\n\n\nIn order to ensure consistency of results and visualizations we fix the seed to 1.\n\n\nset.seed(1)\n\n\nWe now load the data set and take a subset of it using the function filter.\n\n\ndata(\"NOAA_df_1990\", package = \"STRbook\")\nTmax &lt;- filter(NOAA_df_1990,     # subset the data\n              proc == \"Tmax\" &   # only max temperature\n              month %in% 5:9 &   # May to September\n              year == 1993)      # year of 1993\n\n\nThe data frame we shall work with is hence denoted by Tmax. The first six records in Tmax are:\n\n\nTmax %&gt;% select(lon, lat, date, julian, z) %&gt;% head()\n\n        lon   lat       date julian  z\n1 -81.43333 39.35 1993-05-01 728050 82\n2 -81.43333 39.35 1993-05-02 728051 84\n3 -81.43333 39.35 1993-05-03 728052 79\n4 -81.43333 39.35 1993-05-04 728053 72\n5 -81.43333 39.35 1993-05-05 728054 73\n6 -81.43333 39.35 1993-05-06 728055 78\n\n\n\nThe first record has a Julian date of 728050, corresponding to 01 May 1993. To ease the following operations, we create a new variable t that is equal to 1 when julian == 728050 and increases by 1 for each day in the record.\n\n\nTmax$t &lt;- Tmax$julian - 728049     # create a new time variable\n\n\nThe first task faced by the spatio-temporal modeler is data visualization. This is an important preliminary task that needs to be carried out prior to the exploratory-data-analysis stage and the modeling stages. Throughout, we shall make extensive use of the grammar of graphics package ggplot2, which is a convenient way to plot and visualize data and results in R. The book by Wickham (2016) provides a comprehensive introduction to ggplot2.\n\nSpatial Plots\nVisualization techniques vary with the data being analyzed. The NOAA data are collected at stations that are fixed in space; therefore, initial plots should give the modeler an idea of the overall spatial variation of the observed data. If there are many time points, usually only a selection of time points are chosen for visualization. In this case we choose three time points.\n\n\nTmax_1 &lt;- subset(Tmax, t %in% c(1, 15, 30))  # extract data\n\n\nThe variable Tmax_1 contains the data associated with the first, fifteenth, and thirtieth day in Tmax. We now plot this data subset using ggplot2. Note that the function col_scale, below, is simply a wrapper for the ggplot2 function scale_colour_distiller, and is provided with STRbook.\n\n\nNOAA_plot &lt;- ggplot(Tmax_1) +             # plot points\n    geom_point(aes(x = lon,y = lat,       # lon and lat\n                   colour = z),           # attribute color\n               size = 2) +                # make all points larger\n    col_scale(name = \"degF\") +            # attach color scale\n    xlab(\"Longitude (deg)\") +             # x-axis label\n    ylab(\"Latitude (deg)\") +              # y-axis label\n    geom_path(data = map_data(\"state\"),   # add US states map\n          aes(x = long, y = lat, group = group)) +\n    facet_grid(~date) +                   # facet by time\n    coord_fixed(xlim = c(-105, -75),\n                ylim = c(25, 50))  +      # zoom in\n    theme_bw()                            # B&W theme\n\n\n\n\nNOAA_plot is a plot of the spatial locations of the stations. The function aes (short for aesthetics) for geom_point identifies which field in the data frame Tmax_1 is the \\(x\\)-coordinate and which is the \\(y\\)-coordinate. ggplot2 also allows one to attribute color (and size, if desired) to other fields in a similar fashion. Use the command print(NOAA_plot) to display the plot. The command print(NOAA_plot) generates the figure shown in Figure 2.1. As can be seen, the stations are approximately regularly spaced within the domain.\nWhen working with geographic data, it is also good practice to put the spatial locations of the data into perspective, by plotting country or state boundaries together with the data locations. Above, the US state boundaries are obtained from the maps package through the command map_data(\"state\"). The boundaries are then overlayed on the plot using geom_path, which simply joins the points and draws the resulting path with x against y. Projections can be applied by adding another layer to the ggplot2 object using coord_map. For example adding + coord_map(projection = \"sinusoidal\") will plot using a sinusoidal projection. One can also plot in three dimensions by using projection = \"ortho\".\nIn this example we have used ggplot2 to plot point-referenced data. Plots of regular lattice data, such as those shown in Figure 2.2, are generated similarly by using geom_tile instead. Plots of irregular lattice data are generated using geom_polygon. As an example of the latter, consider the BEA income data set. These data can be loaded from STRbook as follows.\n\n\ndata(\"BEA\", package = \"STRbook\")\nhead(BEA %&gt;% select(-Description), 3)\n\n         NAME10 X1970 X1980 X1990\n6     Adair, MO  2723  7399 12755\n9    Andrew, MO  3577  7937 15059\n12 Atchison, MO  3770  5743 14748\n\n\n\nFrom the first three records, we can see that the data set contains the personal income, in dollars, by county and by year for the years 1970, 1980 and 1990. These data need to be merged with Missouri county data which contain geospatial information. These county data, which are also available in STRbook, were originally processed from a shapefile that was freely available online at http://msdis-archive.missouri.edu/archive/metadata_gos/MO_2010_TIGER_Census_County_Boundaries.xml.\n\n\ndata(\"MOcounties\", package = \"STRbook\")\nhead(MOcounties %&gt;% select(long, lat, NAME10), 3)\n\n      long     lat    NAME10\n1 627911.9 4473554 Clark, MO\n2 627921.4 4473559 Clark, MO\n3 627923.0 4473560 Clark, MO\n\n\n\nThe data set contains the boundary points for the counties, amongst several other variables which we do not explore here. For example, to plot the boundary of the first county one can simply type:\n\n\nCounty1 &lt;- filter(MOcounties, NAME10 == \"Clark, MO\")\nplot(County1$long, County1$lat)\n\n\nTo add the BEA income data to the county data containing geospatial information we use left_join.\n\n\nMOcounties &lt;- left_join(MOcounties, BEA, by = \"NAME10\")\n\n\nNow it is just a matter of calling ggplot with geom_polygon to display the BEA income data as spatial polygons. We also use geom_path to draw the county boundaries. Below we show the code for 1970; similar code would be needed for 1980 and 1990. Note the use of the group argument to identify which points correspond to which county. The resulting plots are shown in Figure 2.4.\n\n\ng1 &lt;- ggplot(MOcounties) +\n    geom_polygon(aes(x = long, y = lat,     # county boundary\n                     group = NAME10,        # county group\n                     fill = log(X1970))) +  # log of income\n    geom_path(aes(x = long, y = lat,        # county boundary\n                  group = NAME10)) +        # county group\n    fill_scale(limits = c(7.5,10.2),\n               name = \"log($)\")  +\n    coord_fixed() + ggtitle(\"1970\") +       # annotations\n    xlab(\"x (m)\") + ylab(\"y (m)\") + theme_bw()\n\n\nType print(g1) in the R console to display the plot.\n\n\n\n\n\n\nTime-Series Plots\nNext, we look at the time series associated with the maximum temperature data in the NOAA data set. One can plot the time series at all 139 weather stations (and this is recommended); here we look at the time series at a set of stations selected at random. We first obtain the set of unique station identifiers, choose 10 at random from these, and extract the data associated with these 10 stations from the data set.\n\n\nUIDs &lt;- unique(Tmax$id)                     # extract IDs\nUIDs_sub &lt;- sample(UIDs, 10)                # sample 10 IDs\nTmax_sub &lt;- filter(Tmax, id %in% UIDs_sub)  # subset data\n\n\nTo visualize the time series at these stations, we use facets. When given a long data frame, one can first subdivide the data frame into groups and generate a plot for each group. The following code displays the time series at each station. The command we use is facet_wrap, which automatically adjusts the number of rows and columns in which to display the facets. The command facet_grid instead uses columns for one grouping variable and rows for a second grouping variable, if specified.\n\n\nTmaxTS &lt;- ggplot(Tmax_sub) +\n    geom_line(aes(x = t, y = z)) + # line plot of z against t\n    facet_wrap(~id, ncol = 5) +    # facet by station\n    xlab(\"Day number (days)\") +    # x label\n    ylab(\"Tmax (degF)\") +          # y label\n    theme_bw() +                   # BW theme\n    theme(panel.spacing = unit(1, \"lines\")) # facet spacing\n\n\n\n\nThe argument ~id supplied to facet_wrap is a formula in R. In this case, the formula is used to denote the groups by which we are faceting. The syntax x~y can be used to facet by two variables. The command print(TmaxTS) produces the required figure.\n\n\nHovmöller Plots\nA Hovmöller plot is a two-dimensional space-time visualization, where space is collapsed (projected or averaged) onto one dimension; the second dimension then denotes time. A Hovmöller plot can be generated relatively easily if the data are on a space-time grid, but unfortunately this is rarely the case! This is where data-wrangling techniques such as those explored in Lab 2.1 come in handy.\nConsider the latitudinal Hovmöller plot. The first step is to generate a regular grid of, say, 25 spatial points and 100 temporal points using the function expand.grid, with limits set to the latitudinal and temporal limits available in the data set.\n\n\nlim_lat &lt;- range(Tmax$lat)        # latitude range\nlim_t &lt;- range(Tmax$t)            # time range\nlat_axis &lt;- seq(lim_lat[1],       # latitude axis\n                lim_lat[2],\n                length=25)\nt_axis &lt;- seq(lim_t[1],           # time axis\n              lim_t[2],\n              length=100)\nlat_t_grid &lt;- expand.grid(lat = lat_axis,\n                          t = t_axis)\n\n\nWe next need to associate each station’s latitudinal coordinate with the closest one on the grid. This can be done by finding the distance from the station’s latitudinal coordinate to each point of the grid, finding which gridpoint is the closest, and allocating that to it. We store the gridded data in Tmax_grid.\n\n\nTmax_grid &lt;- Tmax\ndists &lt;- abs(outer(Tmax$lat, lat_axis, \"-\"))\nTmax_grid$lat &lt;- lat_axis[apply(dists, 1, which.min)]\n\n\nNow that we have associated each station with a latitudinal coordinate, all that is left is to group by latitude and time, and then we average all station values falling in the latitude–time bands.\n\n\nTmax_lat_Hov &lt;- group_by(Tmax_grid, lat, t) %&gt;%\n                summarise(z = mean(z))\n\n`summarise()` has grouped output by 'lat'. You can override using the `.groups`\nargument.\n\n\n\nIn this case, every latitude–time band contains at least one data point, so that the Hovmöller plot contains no missing points on the established grid. This may not always be the case, and simple interpolation methods, such as interp from the akima package, can be used to fill out grid cells with no data.\n\n\nPlotting gridded data is facilitated using the ggplot2 function geom_tile. The function geom_tile is similar to geom_point, except that it assumes regularly spaced data and automatically uses rectangular patches in the plot. Since rectangular patches are “filled,” we use the STRbook function fill_scale instead of col_scale, which takes the legend title in the argument name.\n\n\nHovmoller_lat &lt;- ggplot(Tmax_lat_Hov) +            # take data\n        geom_tile(aes(x = lat, y = t, fill = z)) + # plot\n        fill_scale(name = \"degF\") +     # add color scale\n        scale_y_reverse() +             # rev y scale\n        ylab(\"Day number (days)\") +     # add y label\n        xlab(\"Latitude (degrees)\") +    # add x label\n        theme_bw()                      # change theme\n\n\n\n\nThe function scale_y_reverse ensures that time increases from top to bottom, as is typical in Hovmöller plots. We can generate a longitude-based Hovmöller plot in the same way. Type print(Hovmoller_lat) in the R console to display the plot. The resulting Hovmöller plots are shown in Figure 2.11.\n\n\n\n\n\n\nAnimations\nTo generate an animation in R, one can use the package animation. First, we define a function that plots a spatial map of the maximum temperature as a function of time:\n\n\nTmax_t &lt;- function(tau) {\n    Tmax_sub &lt;- filter(Tmax, t == tau)        # subset data\n    ggplot(Tmax_sub) +\n        geom_point(aes(x = lon, y = lat, colour = z),   # plot\n                   size = 4) +                         # pt. size\n        col_scale(name = \"z\", limits = c(40, 110)) +\n        theme_bw() # B&W theme\n}\n\n\nThe function above takes a day number tau, filters the data frame according to the day number, and then plots the maximum temperature at the stations as a spatial map.\nNext, we construct a function that plots the data for every day in the data set. The function that generates the animation within an HTML webpage is saveHTML. This takes the function that plots the sequence of images and embeds them in a webpage (by default named index.html) using JavaScript. The function saveHTML takes many arguments; type the command\n\n\nhelp(saveHTML)\n\n\nin the R console for more details.\n\n\ngen_anim &lt;- function() {\n    for(t in lim_t[1]:lim_t[2]){  # for each time point\n       plot(Tmax_t(t))            # plot data at this time point\n    }\n}\n\nani.options(interval = 0.2)     # 0.2s interval between frames\nsaveHTML(gen_anim(),            # run the main function\n         autoplay = FALSE,      # do not play on load\n         loop = FALSE,          # do not loop\n         verbose = FALSE,       # no verbose\n         outdir = \".\",          # save to current dir\n         single.opts = \"'controls': ['first', 'previous',\n                                     'play', 'next', 'last',\n                                      'loop', 'speed'],\n                                      'delayMin': 0\",\n         htmlfile = \"NOAA_anim.html\")  # save filename\n\n\nTo view the animation, load NOAA_anim.html from your working directory. The animation reveals dynamics within the spatio-temporal data that are not apparent using other visualization methods. For example, the maximum temperature clearly drifts from west to east at several points during the animation. This suggests that a dynamic spatio-temporal model that can capture this drift could provide a good fit to these data.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Exploring Spatio-Temporal Data</span>"
    ]
  },
  {
    "objectID": "Chapter2.html#lab-2.3-exploratory-data-analysis",
    "href": "Chapter2.html#lab-2.3-exploratory-data-analysis",
    "title": "2  Exploring Spatio-Temporal Data",
    "section": "Lab 2.3: Exploratory Data Analysis",
    "text": "Lab 2.3: Exploratory Data Analysis\nIn this Lab we carry out exploratory data analysis (EDA), which typically requires visualization techniques similar to those utilized in Lab 2.2. There are several ways in which to carry out EDA with spatio-temporal data; in this Lab we consider the construction and visualization of the empirical means and covariances, the use of empirical orthogonal functions and their associated principal component time series, semivariogram analysis, and spatio-temporal canonical correlation analysis.\nFor the first part of the Lab, as in Lab 2.2, we shall consider the daily maximum temperatures in the NOAA data set between May 1993 and September 1993 (inclusive). The packages we need are CCA, dplyr, ggplot2, gstat, sp, spacetime, STRbook and tidyr.\n\n\nlibrary(\"CCA\")\nlibrary(\"dplyr\")\nlibrary(\"ggplot2\")\nlibrary(\"gstat\")\nlibrary(\"sp\")\nlibrary(\"spacetime\")\nlibrary(\"STRbook\")\nlibrary(\"tidyr\")\n\n\n\n\nIn order to ensure consistency of results and visualizations, we fix the seed to 1.\n\n\nset.seed(1)\n\n\nWe now load the NOAA data set using the data command. To keep the data size manageable, we take a subset of it corresponding to the maximum daily temperatures in the months May–September 1993. As in Lab 2.2 we also add a new variable t which starts at 1 at the beginning of the data set and increases by 1 each day.\n\n\ndata(\"NOAA_df_1990\", package = \"STRbook\")\nTmax &lt;- filter(NOAA_df_1990,     # subset the data\n              proc == \"Tmax\" &   # only max temperature\n              month %in% 5:9 &   # May to September\n              year == 1993)      # year of 1993\nTmax$t &lt;- Tmax$julian - 728049   # create a new time variable\n\n\n\nEmpirical Spatial Means\nThe empirical spatial mean of our data is given by Equation 2.1. The empirical spatial mean is a spatial quantity that can be stored in a new data frame that contains the spatial locations and the respective average maximum temperature at each location. These, and other data manipulations to follow, can be carried out easily using the tools we learned in Lab 2.1. We group by longitude and latitude, and then we compute the average maximum temperature at each of the separate longitude–latitude coordinates.\n\n\nspat_av &lt;- group_by(Tmax, lat, lon) %&gt;%    # group by lon-lat\n           summarise(mu_emp = mean(z))     # mean for each lon-lat\n\n\nWe can now plot the average maximum temperature per station and see how this varies according to longitude and latitude. The following plots are shown in Figure 2.14.\n\n\nlat_means &lt;- ggplot(spat_av) +\n             geom_point(aes(lat, mu_emp)) +\n             xlab(\"Latitude (deg)\") +\n             ylab(\"Maximum temperature (degF)\") + theme_bw()\n\n\nlon_means &lt;- ggplot(spat_av) +\n             geom_point(aes(lon, mu_emp)) +\n             xlab(\"Longitude (deg)\") +\n             ylab(\"Maximum temperature (degF)\") + theme_bw()\n\n\n\n\n\n\nEmpirical Temporal Means\nWe now generate the plot of Figure 2.15. The empirical temporal mean can be computed easily using the tools we learned in Lab 2.1: first, group the data by time; and second, summarize using the summarise function.\n\n\nTmax_av &lt;- group_by(Tmax, date) %&gt;%\n           summarise(meanTmax = mean(z))\n\n\nThe variable Tmax_av is a data frame containing the average maximum temperature on each day (averaged across all the stations). This can be visualized easily, together with the original raw data, using ggplot2.\n\n\ngTmaxav &lt;-\n    ggplot() +\n    geom_line(data = Tmax, aes(x = date, y = z, group = id),\n              colour = \"blue\", alpha = 0.04) +\n    geom_line(data = Tmax_av, aes(x = date, y = meanTmax)) +\n    xlab(\"Month\") + ylab(\"Maximum temperature (degF)\") +\n    theme_bw()\n\n\n\n\n\n\nEmpirical Covariances\nBefore obtaining the empirical covariances, it is important that all trends are removed (not just the intercept). One simple way to do this is to first fit a linear model (that has spatial and/or temporal covariates) to the data. Then plot the empirical covariances of the detrended data (i.e., the residuals). Linear-model fitting proceeds with use of the lm function in R. The residuals from lm can then be incorporated into the original data frame Tmax.\nIn the plots of Figure 2.9 we observed a quadratic tendency of temperature over the chosen time span. Therefore, in what follows, we consider time and time squared as covariates. Note the use of the function I. This is required for R to interpret the power sign ^ as an arithmetic operator instead of a formula operator.\n\n\nlm1 &lt;- lm(z ~ lat + t + I(t^2), data = Tmax) # fit a linear model\nTmax$residuals &lt;- residuals(lm1)             # store the residuals\n\n\nWe also need to consider the spatial locations of the stations, which we extract from Tmax used above.\n\n\nspat_df &lt;- filter(Tmax, t == 1) %&gt;% # lon/lat coords of stations\n            dplyr::select(lon, lat)  %&gt;%   # select lon/lat only\n            arrange(lon, lat)       # sort ascending by lon/lat\nm &lt;- nrow(spat_av)                  # number of stations\n\n\nThe most straightforward way to compute the empirical covariance matrix shown in Equation 2.4 is using the cov function in R. When there are missing data, the usual way forward is to drop all records that are not complete (provided there are not too many of these). Specifically, if any of the elements in \\(\\mathbf{Z}_{t_j}\\) or \\(\\mathbf{Z}_{t_j - \\tau}\\) are missing, the associated term in the summation of Equation 2.4 is ignored altogether. The function cov implements this when the argument use = 'complete.obs' is supplied. If there are too many records that are incomplete, imputation, or the consideration of only subsets of stations, might be required.\nIn order to compute the empirical covariance matrices, we first need to put the data into space-wide format using spread.\n\n\nX &lt;- dplyr::select(Tmax, lon, lat, residuals, t) %&gt;% # select columns\n     spread(t, residuals) %&gt;%                 # make time-wide\n     dplyr::select(-lon, -lat) %&gt;%                   # drop coord info\n     t()                                      # make space-wide\n\n\nNow it is simply a matter of calling cov(X, use = 'complete.obs') for computing the lag-0 empirical covariance matrix. For the lag-1 empirical covariance matrix, we compute the covariance between the residuals from X excluding the first time point and X excluding the last time point.\n\n\nLag0_cov &lt;- cov(X, use = 'complete.obs')\nLag1_cov &lt;- cov(X[-1, ], X[-nrow(X),], use = 'complete.obs')\n\n\nIn practice, it is very hard to gain any intuition from these matrices, since points in a two-dimensional space do not have any specific ordering. One can, for example, order the stations by longitude and then plot the permuted spatial covariance matrix, but this works best when the domain of interest is rectangular with a longitude span that is much larger than the latitude span. In our case, with a roughly square domain, a workaround is to split the domain into either latitudinal or longitudinal strips, and then plot the spatial covariance matrix associated with each strip. In the following, we split the domain into four longitudinal strips (similar code can be used to generate latitudinal strips).\n\n\nspat_df$n &lt;- 1:nrow(spat_df)    # assign an index to each station\nlim_lon &lt;- range(spat_df$lon)   # range of lon coordinates\nlon_strips &lt;- seq(lim_lon[1],   # create 4 long. strip boundaries\n                  lim_lon[2],\n                  length = 5)\nspat_df$lon_strip &lt;- cut(spat_df$lon,     # bin the lon into\n                         lon_strips,      # their respective bins\n                         labels = FALSE,  # don't assign labels\n                         include.lowest = TRUE) # include edges\n\n\nThe first six records of spat_df are:\n\n\nhead(spat_df)   # print the first 6 records of spat_df\n\n        lon      lat n lon_strip\n1 -99.96667 37.76667 1         1\n2 -99.76667 36.30000 2         1\n3 -99.68333 32.43333 3         1\n4 -99.05000 35.00000 4         1\n5 -98.81667 38.86666 5         1\n6 -98.51667 33.98333 6         1\n\n\n\nNow that we know in which strip each station falls, we can subset the station data frame by strip and then sort the subsetted data frame by latitude. In STRbook we provide a function plot_cov_strips that takes an empirical covariance matrix C and a data frame in the same format as spat_df, and then plots the covariance matrix associated with each longitudinal strip. Plotting requires the package fields. We can plot the resulting lag-0 and lag-1 covariance matrices using the following code.\n\n\nplot_cov_strips(Lag0_cov, spat_df)  # plot the lag-0 matrices\nplot_cov_strips(Lag1_cov, spat_df)  # plot the lag-1 matrices\n\n\n\n\nAs expected (see Figure 2.16), the empirical spatial covariance matrices reveal the presence of spatial correlation in the residuals. The four lag-0 plots seem to be qualitatively similar, suggesting that there is no strong dependence on longitude. However, there is a dependence on latitude, and the spatial covariance appears to decrease with decreasing latitude. This dependence is a type of spatial non-stationarity, and such plots can be used to assess whether non-stationary spatio-temporal models are required or not.\nSimilar code can be used to generate spatial correlation (instead of covariance) image plots.\n\nSemivariogram Analysis\nFrom now on, in order to simplify computations, we will use a subset of the data containing only observations in July. Computing the empirical semivariogram is much faster when using objects of class STFDF rather than STIDF since the regular space-time structure can be exploited. We hence take STObj3 computed in Lab 2.1 (load using data(STObj3)) and subset the month of July 1993 as follows.\n\n\ndata(\"STObj3\", package = \"STRbook\")\nSTObj4 &lt;- STObj3[, \"1993-07-01::1993-07-31\"]\n\n\nFor computing the sample semivariogram we use the function variogram. Although the function is named “variogram,” it is in fact the sample semivariogram that is computed. We bin the distances between measurement locations into bins of size 80 km, and consider at most six time lags.\n\n\nvv &lt;- variogram(object = z ~ 1 + lat, # fixed effect component\n                data = STObj4,        # July data\n                width = 80,           # spatial bin (80 km)\n                cutoff = 1000,        # consider pts &lt; 1000 km apart\n                tlags = 0:6)    # 0 days to 6 days\n\n\n\n\n\n\nThe command plot(vv) displays the empirical semivariogram. The plot suggests that there are considerable spatio-temporal correlations in the data; spatio-temporal modeling of the residuals is thus warranted.\n\n\n\nEmpirical Orthogonal Functions\nEmpirical orthogonal functions (EOFs) can reveal spatial structure in the data and can also be used for subsequent dimensionality reduction. EOFs can be obtained from the data through either a spectral decomposition of the covariance matrix or a singular value decomposition (SVD) of the detrended space-time data matrix. The data matrix has to be in space-wide format (i.e., where space varies along the columns and time varies along the rows).\nFor this part of the Lab we use the SST data set. The SST data set does not contain any missing values, which renders our task slightly easier than when data are missing. When data are missing, one typically needs to consider interpolation, median polishing, or other imputation methods to fill in the missing values prior to computing the EOFs.\nFirst we load the sea-land mask, the lon-lat coordinates of the SST grid, and the SST data set itself which is in time-wide format.\n\n\ndata(\"SSTlandmask\", package = \"STRbook\")\ndata(\"SSTlonlat\", package = \"STRbook\")\ndata(\"SSTdata\", package = \"STRbook\")\n\n\nSince SSTdata contains readings over land, we delete these using SSTlandmask. Further, in order to consider whole years only, we take the first 396 months (33 years) of the data, containing SST values spanning 1970–2002.\n\n\ndelete_rows &lt;- which(SSTlandmask == 1)\nSSTdata &lt;- SSTdata[-delete_rows, 1:396]\n\n\nFrom Equation 2.10 recall that prior to carrying out an SVD, we need to put the data set into space-wide format, mean-correct it, and then standardize it. Since SSTdata is in time-wide format, we first transpose it to make it space-wide.\n\n\n## Put data into space-wide form\nZ &lt;- t(SSTdata)\ndim(Z)\n\n[1]  396 2261\n\n\n\nNote that Z is of size 396 \\(\\times\\) 2261, and it is hence in space-wide format as required. Equation 2.10 is implemented as follows.\n\n\n## First find the matrix we need to subtract:\nspat_mean &lt;- apply(SSTdata, 1, mean)\nnT &lt;- ncol(SSTdata)\n\n## Then subtract and standardize:\nZspat_detrend &lt;- Z - outer(rep(1, nT), spat_mean)\nZt &lt;- 1/sqrt(nT - 1)*Zspat_detrend\n\n\nFinally, to carry out the SVD we run\n\n\nE &lt;- svd(Zt)\n\n\nThe SVD returns a list E containing the matrices \\(\\mathbf{V}\\), \\(\\mathbf{U}\\), and the singular values \\(\\textrm{diag}(\\mathbf{D})\\). The matrix \\(\\mathbf{V}\\) contains the EOFs in space-wide format. We change the column names of this matrix, and append the lon-lat coordinates to it as follows.\n\n\nV &lt;- E$v\ncolnames(E$v) &lt;- paste0(\"EOF\", 1:ncol(SSTdata)) # label columns\nEOFs &lt;- cbind(SSTlonlat[-delete_rows, ], E$v)\nhead(EOFs[, 1:6])\n\n   lon lat         EOF1         EOF2        EOF3         EOF4\n16 154 -29 -0.004915064 -0.012129566 -0.02882162 8.540892e-05\n17 156 -29 -0.001412275 -0.002276177 -0.02552841 6.726077e-03\n18 158 -29  0.000245909  0.002298082 -0.01933020 8.591251e-03\n19 160 -29  0.001454972  0.002303585 -0.01905901 1.025538e-02\n20 162 -29  0.002265778  0.001643138 -0.02251571 1.125295e-02\n21 164 -29  0.003598762  0.003910823 -0.02311128 1.002285e-02\n\n\n\nThe matrix \\(\\mathbf{U}\\) returned from svd contains the principal component time series in wide-table format (i.e., each column corresponds to a time series associated with an EOF). Here we use the function gather in the package tidyr that reverses the operation spread. That is, the function takes a spatio-temporal data set in wide-table format and puts it into long-table format. We instruct the function to gather every column except the column denoting time, and we assign the key-value pair EOF-PC:\n\n\nTS &lt;- data.frame(E$u) %&gt;%            # convert U to data frame\n      mutate(t = 1:nrow(E$u)) %&gt;%    # add a time field\n      gather(EOF, PC, -t)            # put columns (except time)\n                                     # into long-table format with\n                                     # EOF-PC as key-value pair\n\n\nFinally, the normalized time series are given by:\n\n\nTS$nPC &lt;- TS$PC * sqrt(nT - 1)\n\n\nWe now can use the visualization tools discussed earlier to visualize the EOFs and the (normalized) principal component time series during July 2003. In Figures Figure 2.20 and Figure 2.21, we show the first three EOFs and the first three principal component time series. We can use the following code to illustrate the first EOF:\n\n\nggplot(EOFs) + geom_tile(aes(x = lon, y = lat, fill = EOF1)) +\n    fill_scale(name = \"degC\") + theme_bw() +\n    xlab(\"Longitude (deg)\") + ylab(\"Latitude (deg)\")\n\n\n\n\nPlotting of other EOFs and principal component time series is left as an exercise to the reader. The EOFs reveal interesting spatial structure in the residuals. The second EOF is a west-east gradient, while the third EOF again reveals a temporally dependent north-south gradient. This north-south gradient has a lower effect in the initial part of the time series, and a higher effect towards the end.\n\n\n\n\nEOFs can also be constructed by using eof in the package spacetime. With the latter, one must cast the data into an STFDF object using the function stConstruct before calling the function eof. The last example in the help file of stConstruct shows how one can do this from a space-wide matrix. The function eof uses prcomp (short for principal component analysis) to find the EOFs, which in turn uses svd.\n\n\n\n\nSpatio-Temporal Canonical Correlation Analysis\nWe can carry out a canonical correlation analysis (CCA) using the package CCA in R. One cannot implement CCA on the raw data since \\(T &lt; n\\). Instead, we carry out CCA on the SST projected onto EOF space, specifically the first 10 EOFs which explain just over 74% of the variance of the signal (you can show this from the singular values in the object E). In this example, we consider the problem of long-lead prediction, and we check whether SST is a useful predictor for SST in 7 months’ time. To this end, we split the data set into two parts, one containing SST and another containing SST lagged by 7 months.\n\n\nnEOF &lt;- 10\nEOFset1 &lt;- E$u[1:(nT-7), 1:nEOF] * sqrt(nT - 1)\nEOFset2 &lt;- E$u[8:nT, 1:nEOF] * sqrt(nT - 1)\n\n\n\n\nThe CCA is carried out by running the function cancor.\n\n\ncc &lt;- cancor(EOFset1, EOFset2)  # compute CCA\noptions(digits = 3)             # print to three d.p.\nprint(cc$cor[1:5])              # print\n\n[1] 0.843 0.758 0.649 0.584 0.463\n\nprint(cc$cor[6:10])\n\n[1] 0.4137 0.3067 0.2058 0.0700 0.0273\n\n\n\nThe returned quantity cc$cor provides the correlations between the canonical variates of the unshifted and shifted SSTs in EOF space. The correlations decrease, as expected, but the first two canonical variates are highly correlated. The time series of the first canonical variables can be found by multiplying the EOF weights with the computed coefficients as follows (see Equation 2.13 and Equation 2.14).\n\n\nCCA_df &lt;- data.frame(t = 1:(nT - 7),\n                    CCAvar1 = (EOFset1 %*% cc$xcoef[,1])[,1],\n                    CCAvar2 = (EOFset2 %*% cc$ycoef[,1])[,1])\n\n\nA plot can be made using standard ggplot2 commands.\n\n\nt_breaks &lt;- seq(1, nT, by = 60)     # breaks for x-labels\nyear_breaks &lt;- seq(1970, 2002, by=5)  # labels for x-axis\ng &lt;- ggplot(CCA_df) +\n     geom_line(aes(t, CCAvar1), col = \"dark blue\") +\n     geom_line(aes(t, CCAvar2), col = \"dark red\") +\n     scale_x_continuous(breaks = t_breaks, labels = year_breaks) +\n     ylab(\"CCA variables\") + xlab(\"Year\") + theme_bw()\n\n\n\n\nThe plot of the time series of the first canonical variables is shown in Figure 2.23. The plot shows a high correlation between the first pair of canonical variables. What are these canonical variables? They are simply a linear combination of the EOFs, where the linear weights are given in cc$xcoef[,1] and cc$ycoef[,1], respectively.\n\n\nEOFs_CCA &lt;- EOFs[,1:4] # first two columns are lon-lat\nEOFs_CCA[,3] &lt;- c(as.matrix(EOFs[,3:12]) %*% cc$xcoef[,1])\nEOFs_CCA[,4] &lt;- c(as.matrix(EOFs[,3:12]) %*% cc$ycoef[,1])\n\n\nPlotting of the weights as spatial maps is straightforward and left as an exercise. We plot weights (recall these are just linear combination of EOFs) for the lagged SSTs and the unlagged SSTs in Figure 2.24.\n\n\n\n\n\n\n\n\nBivand, R. S., Pebesma, E., & Gomez-Rubio, V. (2013). Applied spatial data analysis with R (second). Springer. http://www.asdar-book.org/\n\n\nCohen, A., & Jones, R. H. (1969). Regression on a random field. Journal of the American Statistical Association, 64(328), 1172–1182.\n\n\nCressie, N., & Wikle, C. K. (2011). Statistics for spatio-temporal data. John Wiley & Sons.\n\n\nGenton, M. G., Castruccio, S., Crippa, P., Dutta, S., Huser, R., Sun, Y., & Vettori, S. (2015). Visuanimation in statistics. Stat, 4(1), 81–96.\n\n\nHastie, T., Tibshirani, R., & Friedman, J. (2009). The elements of statistical learning (2nd ed.). Springer.\n\n\nHotelling, H. (1936). Relations between two sets of variates. Biometrika, 28(3/4), 321–377.\n\n\nHovmöller, E. (1949). The trough-and-ridge diagram. Tellus, 1(2), 62–66.\n\n\nJohnson, R. A., & Wichern, D. W. (1992). Applied multivariate statistical analysis. In Prentice Hall (3rd ed.). Prentice Hall.\n\n\nLamigueiro, O. P. (2018). Displaying time series, spatial, and space-time data with r (2nd ed.). Chapman; Hall/CRC.\n\n\nLucchesi, L. R., & Wikle, C. K. (2017). Visualizing uncertainty in areal data with bivariate choropleth maps, map pixelation, and glyph rotation. Stat, 6, 292–302.\n\n\nMilliff, R. F., Bonazzi, A., Wikle, C. K., Pinardi, N., & Berliner, L. M. (2011). Ocean ensemble forecasting. Part i: Ensemble mediterranean winds from a bayesian hierarchical model. Quarterly Journal of the Royal Meteorological Society, 137(657), 858–878.\n\n\nMonahan, A. H., Fyfe, J. C., Ambaum, M. H., Stephenson, D. B., & North, G. R. (2009). Empirical orthogonal functions: The medium is the message. Journal of Climate, 22(24), 6501–6514.\n\n\nObled, Ch., & Creutin, J. D. (1986). Some developments in the use of empirical orthogonal functions for mapping meteorological fields. Journal of Climate and Applied Meteorology, 25(9), 1189–1204.\n\n\nPebesma, E. (2012). spacetime: Spatio-temporal data in R. Journal of Statistical Software, 51(7), 1–30. http://www.jstatsoft.org/v51/i07/\n\n\nWickham, H. (2016). ggplot2: Elegant graphics for data analysis (2nd ed.). Springer.\n\n\nXu, K., Wikle, C. K., & Fox, N. I. (2005). A kernel-based spatio-temporal dynamical model for nowcasting weather radar reflectivities. Journal of the American Statistical Association, 100(472), 1133–1144.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Exploring Spatio-Temporal Data</span>"
    ]
  },
  {
    "objectID": "Chapter3.html",
    "href": "Chapter3.html",
    "title": "3  Spatio-Temporal Statistical Models",
    "section": "",
    "text": "3.1 Spatio-Temporal Prediction\nAs you read this chapter and the next two, remind yourself that what you see in data may be different than what you might expect to see. Your view might be obstructed and/or not in sharp focus. Spatial predictive models can fill in the gaps and clear up your vision, but what you see in the data is still a “guess” at what is really there. We use statistical prediction methods that quantify these guesses with their associated prediction variances. Now, up the ante - include time as well and try to forecast the future… even in places where there are no current or past data! We show how this is possible in the pages that follow.\nSpatio-temporal prediction based on spatio-temporal statistical modeling is a central theme of this book. Importantly, our type of prediction comes with prediction variances that quantify the uncertainty in the prediction. Predicting the future is notoriously hard, but at least the spatio-temporal prediction variances can quantify how hard it is - if you use the “right” model! In this spatio-temporal setting, what if your goal is not to predict new values but to study the impact of covariates on a response? As we shall see, the same statistical models that are useful for prediction also allow us to infer important relationships between covariates and responses.\nWe see three principal goals for spatio-temporal statistical modeling:\nIt is important to note that our observations associated with each of these goals will always include measurement error and will often be incomplete, in the sense that there are some locations in space and time that have missing observations. When modeling to accomplish any of the goals above, we have to be able to take into account these data issues, and also that our model is almost surely “wrong.” As the famous aphorism by George Box goes, “all models are wrong but some are useful” Box (1976), Box (1979). Our task is to maximize the “usefulness” and to minimize the “wrongness.”\nThe primary purpose of this chapter is to present an example illustrating each of the three goals given above, along with a potential modeling solution that initially does not account for a spatio-temporal error process. This will allow us to illustrate some of the benefits and shortcomings of standard approaches and show why it is often better to consider statistical models that do account for spatio-temporal dependent errors (see Chapters 4 and 5). This will also give you a chance to use some of the visualization and exploratory techniques you learned in Chapter 2, and the R Labs at the end of this chapter will further develop your R programming and analysis skills for spatio-temporal data, in preparation for later chapters.\nTo start with, consider the prediction (i.e., “interpolation”) of maximum daily temperatures on 15 July 1993 at the location denoted by the triangle in the top panel of Figure 3.1, given observations on the same variable on the same date at 138 measurement locations in the central USA (NOAA data set). We seek a predictor, and it is easy to imagine visually how we might construct one – we somehow just combine the nearest observations. Indeed, as mentioned in Section 1.2.2, Tobler’s “law” suggests that we should give more weight to nearby observations when we interpolate. But, why stop with just space? We also have other observations at different time points, so we should consider nearby observations in both space and time, as shown in the bottom panel of Figure 3.1. We have already shown in Chapter 2 (e.g., Figure 2.9) that there is strong spatio-temporal dependence in these data. Since we have observations at times before and after 15 July 1993, this application is an example of smoothing – that is, we seek a smoothing predictor. If we only had observations up to 15 July, then we would seek a filtering predictor for the entire temperature field on 15 July 1993, and a forecasting predictor for the entire field at any time after 15 July 1993. Discussion of the distinction between the three types of spatio-temporal predictor is given in Section 1.3.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Spatio-Temporal Statistical Models</span>"
    ]
  },
  {
    "objectID": "Chapter3.html#sec-determ_pred",
    "href": "Chapter3.html#sec-determ_pred",
    "title": "3  Spatio-Temporal Statistical Models",
    "section": "",
    "text": "Figure 3.1: Top: NOAA maximum daily temperature observations for 15 July 1993 (degrees Fahrenheit).\nBottom: NOAA maximum daily temperature observations for 01, 15, and 30 July 1993 (degrees Fahrenheit).\nThe triangle corresponds to a spatial location and time point \\(\\{\\mathbf{s}_0;t_0\\}\\) for which we would like to obtain a prediction of the maximum daily temperatures.\n\n\n\n\nDeterministic Prediction\nPerhaps the simplest way to perform spatio-temporal prediction would be to follow Tobler’s law and simply average the data in such a way as to give more weight to the nearest observations in space and time. The most obvious way to do this is through inverse distance weighting (IDW). Suppose we have spatio-temporal data given by\n\\[\n\\{Z(\\mathbf{s}_{11};t_1), Z(\\mathbf{s}_{21};t_1),\\ldots, Z(\\mathbf{s}_{m_1 1};t_1),\\ldots, Z(\\mathbf{s}_{1T};t_T), Z(\\mathbf{s}_{2T};t_T),\\ldots,Z(\\mathbf{s}_{m_T T};t_T)\\},\n\\]\nwhere for each time \\(t_j\\) we have \\(m_{j}\\) observations. Then the IDW predictor at some location \\(\\mathbf{s}_{0}\\) and time \\(t_0\\) (where, in this smoothing-predictor case, we assume that \\(t_1 \\leq t_0 \\leq t_T\\)) is given by\n\\[\n\\widehat{Z}(\\mathbf{s}_0;t_0) =  \\sum_{j=1}^T \\sum_{i=1}^{m_j} w_{ij}(\\mathbf{s}_0;t_0) Z(\\mathbf{s}_{i j};t_j),\n\\tag{3.1}\\]\nwhere\n\\[\nw_{ij}(\\mathbf{s}_0;t_0) \\equiv \\frac{\\widetilde{w}_{ij}(\\mathbf{s}_0;t_0)}{\\sum_{k=1}^T \\sum_{\\ell=1}^{m_k} \\widetilde{w}_{\\ell k}(\\mathbf{s}_0;t_0)},\n\\tag{3.2}\\]\n\\[\n\\widetilde{w}_{ij}(\\mathbf{s}_0;t_0) \\equiv \\frac{1}{d((\\mathbf{s}_{i j};t_j),(\\mathbf{s}_0;t_0))^\\alpha},\n\\tag{3.3}\\]\n\\(d((\\mathbf{s}_{i j};t_i),(\\mathbf{s}_0;t_0))\\) is the “distance” between the spatio-temporal location \\((\\mathbf{s}_{i j};t_j)\\) and the prediction location \\((\\mathbf{s}_0;t_0)\\), and the power coefficient \\(\\alpha\\) is a positive real number that controls the amount of smoothing (e.g., often \\(\\alpha = 2\\), but it does not have to be). The notation makes this look more complicated than it actually is: IDW is simply a weighted average of the data points, giving the closest locations more weight (while requiring that the weights sum to 1). You are free to choose your preferred distance \\(d(\\cdot,\\cdot)\\); a simple one is the Euclidean distance (although this implicitly treats space and time in the same way, which may not be appropriate; see Section 4.2.3). Note that if we were interested in predicting at a different spatio-temporal location, we would necessarily get different weights, but in a way that respects Tobler’s first law of geography. Also note that some practitioners require an “exact interpolator” in the sense that if the prediction location \\((\\mathbf{s}_0;t_0)\\) corresponds to a data location, they want the prediction to be exactly the same as the data value (so, not a smoothed estimate there). The formula in Equation 3.1 gives an exact interpolator. Thus, \\(\\widehat{Z}(\\mathbf{s}_0;t_0) = Z(\\mathbf{s}_{k \\ell};t_\\ell)\\) if a data location \\((\\mathbf{s}_{k \\ell};t_\\ell)\\) corresponds to the prediction location \\((\\mathbf{s}_0;t_0)\\) (since \\(\\alpha &gt; 0\\), \\((\\mathbf{s}_0;t_0)\\) being a data location implies that the right-hand side of Equation 3.3 is infinite, so it gets a weight of 1 in Equation 3.2}. As discussed in Cressie (1993, p. 379), exact interpolators can be problematic when one has measurement uncertainty, and one way to obtain a smoothing predictor is to use weights in Equation 3.3 proportional to \\(1/(d(\\cdot,\\cdot) + c)^\\alpha\\), where \\(c &gt; 0\\). (Setting \\(c=0\\) reverts to the exact interpolator.)\n\n\n\n\n\n\nTip\n\n\n\nComputing distances between a single set of coordinates can be done in base R using the function dist. To compute distances between two sets of coordinates, it is more convenient to use the function rdist in the package fields, or the function spDists in the package sp, both of which take two sets of coordinates as arguments. The latter also works with Spatial objects defined in the package sp.\n\n\nThe left panel in Figure 3.2 shows predictions of maximum temperature for six days within the month of July 1993 using 30 days of July 1993 data, where data from 14 July 1993 was omitted. These predictions were obtained using IDW with \\(\\alpha = 5\\). In this example, setting \\(\\alpha\\) to a smaller value (such as \\(2\\)) gives a smoother surface since more weight is given to observations that are “far” from the prediction locations. In deterministic interpolators, smoothing parameters such as \\(\\alpha\\) are usually chosen using a procedure known as cross-validation (see Note 3.1 and the left panel in Figure 3.3). From the IDW prediction in Figure 3.2, we observe that our predictions on the day with no data look smoother than those on days for which we have data. We shall see in Chapter 4 that this is typical of most predictors, including stochastic ones that are optimal in the sense of minimizing the mean squared prediction error (MSPE).\n\n\n\n\n\n\nFigure 3.2: Predictions of Tmax in degrees Fahrenheit for the maximum temperature in the NOAA data set within a square box enclosing the domain of interest for six days (each five days apart) spanning the temporal window of the data, 01 July 1993 to 30 July 1993, using (left) inverse distance weighting functionality from the R package gstat with inverse distance power \\(\\alpha = 5\\) and (right) a Gaussian radial basis kernel with bandwidth \\(\\theta =0.5\\). Data for 14 July 1993 were omitted from the original data set.\n\n\n\nIn general, IDW is a type of spatio-temporal kernel predictor. That is, in Equation 3.3 we can let\n\\[\n\\widetilde{w}_{ij}(\\mathbf{s}_0;t_0) = k((\\mathbf{s}_{ij};t_j),(\\mathbf{s}_0;t_0);\\theta),\n\\]\nwhere \\(k((\\mathbf{s}_{ij};t_j),(\\mathbf{s}_0;t_0);\\theta)\\) is a kernel function (i.e., a function that quantifies the similarity between two locations) that depends on the distance between \\((\\mathbf{s}_{ij};t_j)\\) and \\((\\mathbf{s}_0;t_0)\\) and some bandwidth parameter, \\(\\theta\\). Specifically, the bandwidth controls the “width” of the kernel, so a larger bandwidth averages more observations (and produces smoother prediction fields) than a narrow bandwidth. A classic example of a kernel function is the Gaussian radial basis kernel\n\\[\nk((\\mathbf{s}_{ij};t_j),(\\mathbf{s}_0;t_0);\\theta) \\equiv \\exp\\left( - \\frac{1}{\\theta} d((\\mathbf{s}_{ij};t_j),(\\mathbf{s}_0;t_0))^2\\right),\n\\tag{3.4}\\]\nwhere the bandwidth parameter \\(\\theta\\) is proportional to the variance parameter in a normal (Gaussian) distribution. Many other kernels exist (e.g., tricube, bisquare, Epanechnikov), some of which have compact support (i.e., provide zero weight beyond a certain distance threshold). If we write \\(d(\\cdot,\\cdot)^\\alpha = \\exp( \\alpha \\log d(\\cdot,\\cdot))\\), it is clear that \\(\\alpha\\) in IDW plays the role of the bandwidth parameter and IDW has non-compact support. The right panel of Figure 3.2 shows an interpolation of the NOAA temperature data using a Gaussian radial basis kernel with \\(\\theta = 0.5\\). As in IDW, \\(\\theta\\) is usually chosen by cross-validation (see the right panel in Figure 3.3).\nTraditional implementations of deterministic methods do not explicitly account for measurement uncertainty in the data nor do they provide model-based estimates of the prediction uncertainty. One might argue that, for non-exact interpolators, one is implicitly removing (filtering or smoothing) the observation error with the averaging that takes place as part of the interpolation. However, there is no mechanism to incorporate explicit knowledge of the magnitude of the measurement error. Regarding prediction uncertainty of deterministic predictors, we can get estimates of the overall quality of predictions by doing cross-validation (see Note 3.1). Recall that we have also suggested using cross-validation to select the degree of smoothing (e.g., the \\(\\alpha\\) parameter in IDW and, more generally, the \\(\\theta\\) parameter in the kernel-based prediction). As an example, in Figure 3.3 we show the leave-one-out cross-validation (LOOCV) MSPE score for different values of \\(\\alpha\\) and \\(\\theta\\) (lower cross-validation scores are better) when doing IDW and Gaussian kernel smoothing for the NOAA maximum temperature data set in July 1993. These cross-validation analyses suggest that \\(\\alpha = 5\\) and \\(\\theta = 0.6\\) are likely to give the best out-of-sample predictions for this specific example. In addition, note that the lowest cross-validation score for the Gaussian kernel smoother is lower (i.e., better) than the lowest cross-validation score for IDW. This suggests that the Gaussian kernel smoother is likely to be a better predictor than the IDW smoother for these data.\nCross-validation can also be used to compare models through their predictions, as the following Note 3.1 explains.\n\n\n\n\n\n\nFigure 3.3: The leave-one-out cross-validation score \\(CV_{(m)}\\) (see Note 3.1) for different values of \\(\\alpha\\) and \\(\\theta\\) when doing IDW prediction (left) and Gaussian kernel prediction (right) of maximum temperature in the NOAA data set in July 1993.\n\n\n\n\n\n\n\n\n\nNote 3.1: Cross-Validation\n\n\n\nCross-validation seeks to evaluate model predictions by splitting up the data into a training sample and a validation sample, then fitting the model with the training sample and evaluating it with the validation sample. In K-fold cross-validation, we randomly split the available data into \\(K\\) roughly equal-size components (or “folds”). Each fold is held out, the model is trained on the remaining \\(K-1\\) folds, and then the model is evaluated on the fold that was held out. Specifically, for \\(k=1,\\ldots,K\\) folds, fit the model with the \\(k\\)th fold removed, and obtain predictions \\(\\widehat{Z}^{(-k)}_i\\) for \\(i=1,\\ldots,m_k\\), where \\(m_k\\) is the number of data in the \\(k\\)th fold. We then select a metric by which we evaluate the predictions relative to the held-out samples. For example, if we were interested in the mean squared prediction error (MSPE), we would compute \\(MSPE_k = (1/m_k) \\sum_{i=1}^{m_k} (Z_i - \\widehat{Z}^{(-k)}_i)^2\\) for the \\(m_k\\) observations in the \\(k\\)th fold, \\(k=1,\\ldots,K\\). The \\(K\\)-fold cross-validation score is then\n\\[\nCV_{(K)} = \\frac{1}{K} \\sum_{k=1}^K MSPE_k.\n\\]\nIt has been shown empirically that good choices for the number of folds are \\(K=5\\) and \\(K=10\\).\nA special case of \\(K\\)-fold cross-validation occurs when \\(K=m\\). This is called leave-one-out cross-validation (LOOCV). In this case, only a single observation is used for validation and the remaining observations are used to make up the training set. This is repeated for all \\(m\\) observations. The LOOCV score is then\n\\[\nCV_{(m)} = \\frac{1}{m} \\sum_{i=1}^m MSPE_i.\n\\]\nLOOCV typically has low bias as an estimate of the expected squared error of a test sample, but it can also have high variance. This is why the choice of \\(K=5\\) or \\(K=10\\) often provides a better compromise between bias and variance. It is also the case that LOOCV can be computationally expensive to implement in general, since it requires the model to be fitted \\(m\\) times (although there are notable exceptions such as with the predicted residual error sum of squares (PRESS) statistic in multiple linear regression models; see Appendix B). For more details on cross-validation, see Hastie et al. (2009), Section 7.10.\n\n\n\n\n\n\n\n\nTip\n\n\n\n\\(K\\)-fold cross-validation is an “embarrassingly parallel” problem since all the \\(K\\) validations can be done simultaneously. There are several packages in R that enable this, with parallel and foreach among the most popular. The vignettes in these packages contain more information on how to use them for multicore computing.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Spatio-Temporal Statistical Models</span>"
    ]
  },
  {
    "objectID": "Chapter3.html#sec-reg_pred",
    "href": "Chapter3.html#sec-reg_pred",
    "title": "3  Spatio-Temporal Statistical Models",
    "section": "3.2 Regression (Trend-Surface) Estimation",
    "text": "3.2 Regression (Trend-Surface) Estimation\nIn Section 3.1 we presented some simple deterministic predictors to obtain predictions at spatio-temporal locations given a spatio-temporal data set. We can also use a basic statistical regression model to obtain predictions for such data, assuming that all of the spatio-temporal dependence can be accounted for by “trend” (i.e., covariate) terms. Such a model has the advantage of being exceptionally simple to implement in almost any software package. In addition, a regression model explicitly accounts for model error (usually assumed independent), and it also allows us to obtain a model-based prediction-error variance, although cross-validation scores still provide useful insight into model performance.\nConsider a regression model that attempts to account for spatial and temporal trends. To make the notation a bit simpler, we consider the case where we have observations at discrete times \\(\\{t_j: j =1,\\ldots,T\\}\\) for all spatial data locations \\(\\{\\mathbf{s}_i: i=1,\\ldots,m\\}\\). For example,\n\\[\nZ(\\mathbf{s}_i;t_j) = \\beta_0 + \\beta_1 X_1(\\mathbf{s}_i;t_j) + \\ldots + \\beta_p X_p(\\mathbf{s}_i;t_j) + e(\\mathbf{s}_i;t_j),\n\\tag{3.5}\\]\nwhere \\(\\beta_0\\) is the intercept and \\(\\beta_k~(k &gt; 0)\\) is a regression coefficient associated with \\(X_k(\\mathbf{s}_i;t_j)\\), the \\(k\\)th covariate at spatial location \\(\\mathbf{s}_i\\) and time \\(t_j\\). We also assume for the moment \\(iid\\) errors such that \\(e(\\mathbf{s}_i;t_j) \\sim \\; indep. \\; N(0,\\sigma^2_e)\\) for all \\(\\{\\mathbf{s}_i;t_j\\}\\) where there are data, and note that \\(N(\\mu,\\sigma^2)\\) corresponds to a normal distribution with mean \\(\\mu\\) and variance \\(\\sigma^2\\). The covariates \\(X_k(\\mathbf{s}_i;t_j)\\) may describe explanatory features, such as elevation, that vary spatially but are temporally invariant (on the scales of interest here), time trends (such as an overall seasonal effect) that are spatially invariant but temporally varying, or other variables such as humidity, that are both spatially and temporally varying. We might also consider spatio-temporal “basis functions” that can be used to reconstruct the observed data.\nWe take a little space here to discuss basis functions beyond the brief explanation given in Chapter 1. What are basis functions? Imagine that we have a complex curve or surface in space. We are often able to decompose this curve or surface as a linear combination of some “elemental” basis functions. For example,\n\\[\nY(\\mathbf{s}) = \\alpha_1 \\phi_1(\\mathbf{s}) + \\alpha_2 \\phi_2(\\mathbf{s}) + \\ldots + \\alpha_r \\phi_r(\\mathbf{s}),\n\\tag{3.6}\\]\nwhere \\(\\{\\alpha_i\\}\\) are constants and \\(\\{\\phi_i(\\mathbf{s})\\}\\) are known basis functions. We can think of the coefficients \\(\\{\\alpha_i\\}\\) as weights that describe how important each basis function is in representing the function \\(Y(\\mathbf{s})\\). The basis functions can be local with compact support, or can be global, taking values across the whole domain (see Figure 3.4). In statistics, when \\(Y(\\mathbf{s})\\) is a random process, we typically assume the basis functions are known and the coefficients (weights) are random. The expression in Equation 3.6 could be written as a function of time \\(t\\), or most generally as a function of \\(\\mathbf{s}\\) and \\(t\\). In time series, the domain over which the basis functions take their values is the one-dimensional real line, whereas in spatial statistics, the domain is typically one-dimensional space (see Figure 3.4) or two-dimensional space (see Figure 3.5); in spatio-temporal statistics, the domain is over both space and time. Examples of basis functions include polynomials, splines, wavelets, sines and cosines, among many others. We often construct spatio-temporal basis functions via a tensor product of spatial basis functions and temporal basis functions (see Note 4.1).\n\n\n\n\n\n\nFigure 3.4: Left: Local (top) and global (bottom) basis functions over a one-dimensional spatial domain. Different colors are used to denote different basis functions. Right: Linear combination (red curve) of the individual basis functions (dashed lines depicted in the left panels). In this case, the coefficients \\(\\{\\alpha_i\\}\\) give the relative importance of the basis functions (curves).\n\n\n\n\n\n\n\n\n\nFigure 3.5: Two-dimensional spatial basis functions and associated coefficients \\(\\boldsymbol{\\alpha}_1\\) and \\(\\boldsymbol{\\alpha}_2\\) that lead to two different spatial-process realizations, \\(Y_1(\\mathbf{s})\\) and \\(Y_2(\\mathbf{s})\\), respectively.\n\n\n\nNow consider the maximum daily temperature Tmax in the NOAA data set for the month of July 1993, where we have observations at \\(m = 138\\) common spatial locations \\(\\{\\mathbf{s}_i: i=1,\\ldots,m\\}\\) for \\(\\{t_j: j = 1,\\ldots, T= 31\\}\\) days. In this case, we could account for spatial trends by allowing the covariates \\(\\{X_k\\}\\) to correspond to the spatio-temporal coordinate, and/or their transformations and interactions. For example, let \\(\\mathbf{s}_i \\equiv (s_{1,i},s_{2,i})'\\), and consider a linear model with the following basis functions:\n\noverall mean: \\(X_0(\\mathbf{s}_i;t_j) =  1\\), for all \\(\\mathbf{s}_i\\) and \\(t_j\\);\nlinear in \\(lon\\)-coordinate: \\(X_1(\\mathbf{s}_i;t_j) =   s_{1,i}\\), for all \\(t_j\\),\nlinear in \\(lat\\)-coordinate: \\(X_2(\\mathbf{s}_i;t_j) = s_{2,i}\\), for all \\(t_j\\);\nlinear time (day) trend: \\(X_3(\\mathbf{s}_i;t_j) =  t_j\\), for all \\(\\mathbf{s}_i\\);\n\\(lon\\)–\\(lat\\) interaction: \\(X_4(\\mathbf{s}_i;t_j) =  s_{1,i} \\, s_{2,i}\\), for all \\(t_j\\);\n\\(lon\\)–\\(t\\) interaction: \\(X_5(\\mathbf{s}_i;t_j) = s_{1,i} \\, t_j\\), for all \\(s_{2,i}\\);\n\\(lat\\)–\\(t\\) interaction: \\(X_6(\\mathbf{s}_i;t_j) =  s_{2,i} \\, t_j\\), for all \\(s_{1,i}\\);\nadditional spatial-only basis functions: \\(X_k(\\mathbf{s}_i;t_j) = \\phi_{k-6}(\\mathbf{s}_i), k =7,\\dots,18\\), for all \\(t_j\\) (see Figure 3.13).\n\nNote that the space and time coordinates used in \\(X_0,\\ldots,X_6\\) can be thought of as basis functions; we choose the separate notation between these latitude, longitude, and time trend covariates and the spatial-only basis functions (denoted \\(\\{\\phi_k: k=1,\\ldots,12\\}\\)) given in Figure 3.13 for the sake of interpretability. In this example, there is an intercept and \\(p=18\\) regression coefficients.\n\n\n\n\n\n\nFigure 3.6: The time-invariant basis functions, \\(\\phi_1(\\mathbf{s}),\\dots,\\phi_{12}(\\mathbf{s})\\), used for regression prediction of maximum temperature data from the NOAA data set for July 1993.\n\n\n\nThe regression model given in Equation 3.16 can be fitted via ordinary least squares (OLS), in which case we find estimates of the parameters \\(\\beta_0, \\beta_1,\\ldots,\\beta_p\\) that minimize the residual sum of squares,\n\\[\nRSS = \\sum_{j=1}^T \\sum_{i=1}^m (Z(\\mathbf{s}_{i};t_j) - \\widehat{Z}(\\mathbf{s}_{i};t_j))^2.\n\\tag{3.7}\\]\nWe denote these estimates by \\(\\{\\widehat{\\beta}_0,\\widehat{\\beta}_1,\\ldots,\\widehat{\\beta}_p\\}\\) and we write \\(\\widehat{Z}(\\mathbf{s};t) = \\widehat{\\beta}_0 + \\widehat{\\beta}_1 X_1(\\mathbf{s};t) + \\ldots + \\widehat{\\beta}_p X_p(\\mathbf{s};t)\\). (We also obtain an estimate of the variance parameter, namely \\(\\widehat{\\sigma}^2_e = RSS/(mT-p-1)\\).) This then allows us to get predictions for a mean response, or a new response, \\(Z(\\mathbf{s}_0;t_0)\\), at any location \\(\\{\\mathbf{s}_0;t_0\\}\\) for which we have covariates. We can also obtain uncertainty estimates for these predictions. The formulas for these estimates and predictors are most easily seen from a matrix representation, as shown in Note 3.3. Figure 3.7 shows the predictions and the prediction standard errors (assuming the regression model with an intercept and \\(p=18\\)) for the maximum temperature data in the NOAA data set in July 1993, with 14 July 1993 omitted when fitting the model.\n\n\n\n\n\n\nFigure 3.7: Regression predictions (left) and associated prediction standard errors (right) of maximum temperature (in degrees Fahrenheit) within a square box enclosing the domain of interest for six individual days (each 5 days apart) in July 1993 using the R function lm. Data for 14 July 1993 were purposely omitted from the original data set during fitting.\n\n\n\nThe predictions are much smoother than those found using kernel smoothing (Figure 3.2), a direct result of using basis functions that are spatio-temporally smooth. This is not always the case, and using covariates that are highly spatially varying (e.g., from topography) will yield predictions that also vary substantially with space. Note also from Figure 3.7 that the prediction standard errors do not show much structure because the \\(X\\)s are accounting for most of the spatio-temporal variation in the data. Uncertainty increases at the domain edges where prediction becomes extrapolation.\nIt is important to mention here that the regression model given in Equation 3.16 does not explicitly account for measurement errorss in the responses, and thus that variation due to measurement error is confounded with the variation due to lack of fit in the residual variance \\(\\sigma^2_e\\). We account explicitly for this measurement-error variation (and small-scale spatio-temporal variation) in Chapters 4 and 5. In addition, note that the regression predictor can be considered a type of kernel predictor (see Appendix B).\n\n\n\n\n\n\nTip\n\n\n\nBasis functions such as those depicted in Figure 3.13 can be easily constructed using the package FRK, which we explore further in Chapter 4. Basis functions can be constructed at multiple resolutions, can be spatial-only (as used here) or also spatio-temporal. See Lab 3.2 for more details.\n\n\n\n3.2.1 Model Diagnostics: Dependent Errors\nWhen we first learn how to do regression modeling in statistics, we gain an appreciation for the importance of model diagnostics to verify the assumptions of the model. For example, we look for the presence of outliers, influential observations, non-constant error variance, non-normality, dependence in the errors, and so forth Kutner et al. (2004). It is particularly important to consider the possibility of dependent errors in the case where the data are indexed in space or time (see Chapter 6 for more detailed discussion about model evaluation). From an exploratory perspective, one can calculate the spatio-temporal covariogram (or semivariogram), discussed in Chapter 2, from the residuals, \\(\\widehat{e}(\\mathbf{s}_{i};t_j) \\equiv Z(\\mathbf{s}_{i};t_j) - \\widehat{Z}(\\mathbf{s}_{i};t_j)\\), and look for dependence structure as a function of spatial and temporal lags. As seen in Figure 3.8, there is ample spatial and temporal structure in the residuals. It is instructive to compare Figure 3.8 with the empirical semivariogram calculated from the original data set and given in Figure 2.17. The former has a lower sill, and therefore the basis functions and the other covariates have been able to explain some of the spatio-temporal variability in the data, but clearly not all of it.\n\n\n\n\n\n\nFigure 3.8: Empirical spatio-temporal semivariogram of the residuals after fitting a linear model to daily maximum temperatures in the NOAA data set during July 2003, computed using the function variogram in gstat.\n\n\n\nMore formally, one can apply a statistical test for temporal dependence such as the Durbin–Watson test (see Note 3.2), and if the data correspond to areal regions in two-dimensional space, one can use a test for spatial dependence such as Moran’s \\(I\\) test (see Note 3.2). In looking at spatio-temporal dependence, we can consider the “space-time index” (STI) approach of Henebry (1995), which is a type of Moran’s \\(I\\) statistic for spatio-temporal data (see Cressie & Wikle, 2011, p. 303). This approach was developed for areal regions that have a known adjacency structure. In principle, this can be extended to the case of spatio-temporal data with continuous spatial support; see Lab 3.2.\nAlternatively, we can consider a spatio-temporal analog to the Durbin–Watson test. Cressie & Wikle (2011, p. 131) give a statistic based on the empirical (spatial) semivariogram that can be extended to the spatio-temporal setting. In particular, let\n\\[\nF \\equiv \\left| \\frac{\\widehat{\\gamma}_e(||\\mathbf{h}_1||;\\tau_1)}{\\widehat{\\sigma}^2_e}-1\\right| ,\n\\]\nwhere \\(\\widehat{\\gamma}_e (||\\mathbf{h}_1||;\\tau_1)\\) is the empirical semivariogram estimate at the smallest possible spatial (\\(||\\mathbf{h}_1||\\)) and temporal (\\(\\tau_1\\)) lags (see Note 2.1), and \\(\\widehat{\\sigma}^2_e\\) is the regression-error-variance estimate (see Note 3.3). If this value of \\(F\\) is “large,” we reject the null hypothesis of spatio-temporal independence. We can evaluate what is “large” in this case by doing a permutation test of the null hypothesis of independence, which does not depend on any distributional assumptions on the test statistic, \\(F\\). In this case, the data locations (in space and time) are randomly permuted and \\(F\\) is calculated for many such permutation samples. If the statistic \\(F\\) calculated with the observed data is below the 2.5th percentile or above the 97.5th percentile of these permutation samples, then we reject the null hypothesis of spatio-temporal independence (at the 5% level of significance), which suggests that the data are dependent.\n\n\n\n\n\n\nNote 3.2: Durbin–Watson and Moran’s \\(I\\) Tests\n\n\n\nOne of the most used tests for serial dependence in time-series residuals is the Durbin–Watson test Kutner et al. (2004). Let \\(\\widehat{e}_t = Z_t - \\widehat{Z}_t\\) be the residual from some fitted time-series model for which we have \\(T\\) observations \\(\\{Z_t\\}\\). The Durbin–Watson test statistic is given by\n\\[\nd = \\frac{\\sum_{t=2}^T (\\widehat{e}_t - \\widehat{e}_{t-1})^2}{\\sum_{t=1}^T \\widehat{e}_t^2}.\n\\]\nThe intuition for this test is that if residuals are highly (positively) correlated, then \\(\\widehat{e}_t - \\widehat{e}_{t-1}\\) is small relative to \\(\\widehat{e}_t\\) and so, as \\(d\\) gets closer to 0, there is more evidence of positive serial dependence (e.g., a “rule of thumb” suggests that values less than 1 indicate strong positive serial dependence). In contrast, as the value of \\(d\\) gets larger (it is bounded above by 4), it is indicative of no positive serial dependence. This test can be formalized with appropriate upper and lower critical values for \\(d\\), and statistical software packages can easily calculate these, as well as the analogous test for negative serial dependence.\nOne of the most commonly used tests for spatial dependence for spatial lattice data is Moran’s \\(I\\) test Waller & Gotway (2004). This test can be applied to the data directly, or to the residuals from some spatial regression model. Let \\(\\{Z_i: i=1,\\ldots,m\\}\\) represent spatially referenced data (or residuals) for \\(m\\) spatial locations. Then, Moran’s \\(I\\) statistic is calculated as\n\\[\nI = \\frac{m \\sum_{i=1}^m \\sum_{j=1}^m w_{ij} (Z_i - \\bar{Z})(Z_j - \\bar{Z}) }{(\\sum_{i=1}^m \\sum_{j=1}^m w_{ij})(\\sum_{i=1}^m (Z_i - \\bar{Z})^2) },\n\\tag{3.8}\\]\nwhere \\(\\bar{Z} = (1/m)\\sum_{i=1}^m Z_i\\) is the spatial mean and \\(w_{ij}\\) are spatial adjacency “weights” between locations \\(i\\) and \\(j\\) (where we require \\(w_{ii} = 0\\), for all \\(i=1,\\ldots,m\\)). Thus, Moran’s \\(I\\) statistic is simply a weighted form of the usual Pearson correlation coefficient, where the weights are the spatial proximity weights, and it takes values between \\(-1\\) and \\(1\\). If Equation 3.8 is positive, then neighboring regions tend to have similar values, and if it is negative, then neighboring regions tend to have different values. Appropriate critical values or \\(p\\)-values are easily obtained in many software packages.\nNote that there are additional measures of temporal dependence (e.g., the Ljung–Box test; see Shumway & Stoffer (2006)) and spatial dependence (e.g., the Geary \\(C\\) test; see Waller & Gotway (2004)).\n\n\nIt is very common, when studying environmental phenomena, that a linear model of some covariates will not explain all the observed spatio-temporal variability. Thus, fitting such a model will frequently result in residuals that are spatially and temporally correlated. This is not surprising, since several environmental processes are certainly more complex than could be described by simple geographical and temporal trend terms. In Figure 3.9 we show the time series of the residuals at two observation locations \\((81.38^\\circ\\)W, \\(35.73^\\circ\\)N) and (\\(83.32^\\circ\\)W, \\(37.60^\\circ\\)N), respectively, and the spatial residuals between 24 July and 31 July 1993. The residual time series exhibit considerable temporal correlation (i.e., residuals close together in time tend to be more similar than residuals far apart in time), and the spatial residuals exhibit clear spatial correlation (i.e., residuals close together in space tend to be more similar than residuals far apart in space). In Lab 3.2 we go further and use the Durbin–Watson and Moran’s \\(I\\) tests to reject the null hypotheses of no temporal or spatial correlation in these residuals.\n\n\n\n\n\n\n\nFigure 3.9: Top: Temporal residuals at station 3810 (black line) and station 3889 (red line), and bottom: spatial residuals between 24 and 31 July 1993, inclusive, when fitting the regression (trend) model described in Section 3.2 to the maximum temperature data in the NOAA data set in July 1993 (but recall that in the fitting we excluded data from 14 July 1993). The triangles denote the two station locations.\n\n\n\nGiven that our diagnostics have suggested there is spatio-temporal dependence in the errors after fitting the trend surface, what can we do? Readers who are familiar with more complicated regression procedures might suggest that we could use a generalized least squares (GLS) procedure that explicitly accounts for the dependence in the errors. That is, GLS relaxes the assumption of independence in the errors, so that \\(e(\\mathbf{s}_i;t_j)\\) and \\(e(\\mathbf{s}_\\ell;t_k)\\) could be correlated. In this case, the vector of errors, \\(\\mathbf{e}\\equiv (e(\\mathbf{s}_1;t_1),\\ldots,e(\\mathbf{s}_m;t_T))'\\), has the multivariate normal distribution \\(\\mathbf{e}\\sim N(\\mathbf{0},\\mathbf{C}_e)\\), where \\(\\mathbf{C}_e\\) is a spatio-temporal covariance matrix. But do we know in advance what this covariance matrix is? Typically, no – and it is further complicated by the fact that to predict at spatio-temporal locations for which we do not have data, we need to know what the error dependence is between any two locations in time and space within our prediction domain, not just those for which we have observations! This aspect of spatio-temporal prediction will be a primary focus of Chapter 4.\nOne might ask, what is the problem with ignoring the dependence in the errors when doing OLS regression? The answer depends somewhat on the goal. It is fairly easy to show that the OLS parameter estimates and predictions are still unbiased even if one has ignored the dependence in the errors. But ignoring the dependence tends to give inappropriate standard errors and prediction standard errors. In the case of positive dependence (which is the most common case in spatio-temporal data – recall Tobler’s law), the standard errors and prediction standard errors are underestimated if one ignores dependence, giving a false sense of how good the estimates and predictions really are. This issue comes up again in Section 3.2.2.\n\n\n\n\n\n\nNote 3.3: Ordinary Least Squares Regression: Matrix Representation\n\n\n\nConsider an \\(m\\)-dimensional response vector, \\(\\mathbf{Z}= (Z_1,\\ldots,Z_m)'\\), and an \\(m \\times (p+1)\\) matrix of predictors, \\(\\mathbf{X}\\), where we assume that the first column of this matrix contains a vector of \\(1\\)s for the model intercept. That is,\n\\[\n\\mathbf{X}= \\left[\\begin{array}{cccc}\n1 & x_{11} & \\ldots & x_{1p} \\\\\n1 & x_{21} & \\ldots & x_{2p} \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\n1 & x_{m1} & \\ldots & x_{mp}\n\\end{array}\n\\right].\n\\]\nThen the regression equation is given by\n\\[\n\\mathbf{Z}= \\mathbf{X}\\boldsymbol{\\beta}+ \\mathbf{e},\n\\]\nwhere \\(\\boldsymbol{\\beta}\\) is a \\((p+1)\\)-dimensional parameter vector, and the error vector, \\(\\mathbf{e}= (e_1,\\ldots,e_m)'\\), has the multivariate normal distribution \\(\\mathbf{e}\\sim N(\\mathbf{0},\\sigma^2_e \\mathbf{I})\\), where \\(\\mathbf{I}\\) is an \\(m \\times m\\) identity matrix. The ordinary least squares parameter estimates are given by \\(\\widehat{\\boldsymbol{\\beta}} = (\\mathbf{X}' \\mathbf{X})^{-1} \\mathbf{X}' \\mathbf{Z}\\), and the variance–covariance matrix for these estimates is given by \\(\\widehat{\\sigma}^2_e (\\mathbf{X}' \\mathbf{X})^{-1}\\), with \\(\\widehat{\\sigma}^2_e = (1/(m-p-1)) \\sum_i (Z_i - \\widehat{Z}_i)^2\\). The estimated mean response and prediction, \\(\\widehat{Z}_i\\), is given by \\(\\widehat{Z}_i = \\mathbf{x}_i' \\widehat{\\boldsymbol{\\beta}}\\), where \\(\\mathbf{x}'_i\\) is the \\(i\\)th row of \\(\\mathbf{X}\\). Further, the variance of the \\(j\\)th regression-coefficient estimator, \\(\\widehat{\\beta}_j\\), is given by the \\(j\\)th diagonal element of \\(\\widehat{\\sigma}^2_\\epsilon (\\mathbf{X}' \\mathbf{X})^{-1}\\). If \\(\\widehat{Z}_i\\) is an estimate of the mean response, then an estimate of its variance is given by \\(\\widehat{\\sigma}^2_e  (\\mathbf{x}_i ' (\\mathbf{X}' \\mathbf{X})^{-1} \\mathbf{x}_i)\\). If one is predicting a new observation, say \\(Z_h\\), the prediction is \\(\\widehat{Z}_h= \\mathbf{x}_h' \\widehat{\\boldsymbol{\\beta}}\\), and the prediction variance is estimated by \\(\\widehat{\\sigma}^2_e (1 +  \\mathbf{x}_h ' (\\mathbf{X}' \\mathbf{X})^{-1} \\mathbf{x}_h)\\). Derivations and details can be found in textbooks on multiple regression (see for example Kutner et al., 2004).\n\n\n\n\n3.2.2 Parameter Inference for Spatio-Temporal Data\nIn many scientific applications of spatio-temporal modeling, one may only be interested in whether the covariates (the \\(X\\)s) are important in the model for explanation rather than for prediction. Such examples typically include scientifically meaningful covariates, such as a habitat covariate (\\(X\\)) related to the relative abundance (\\(Z\\)) of an animal in some area, or whether some demographic variable (\\(X\\)) is associated with household income (\\(Z\\)). In this section, for illustration we again consider the maximum temperature data in the NOAA data set – specifically, we consider the regression model given in Section 3.2, but our focus here is on the regression parameters. For example, do we need the longitude-by-latitude spatial interaction term (\\(X_4\\)) or the latitude-by-day term (\\(X_6\\)) in the regression?\n\n\n\nTable 3.1: Estimated regression coefficients and the standard errors (within parentheses) for the linear regression model of Section 3.2 using ordinary least squares (OLS) and generalized least squares (GLS). One, two, and three asterisks denote significance at the 10%, 5%, and 1% levels, respectively.\n\n\n\n\n\n\n\n\n\n\nTerm\n\\(\\hat\\beta_{\\textrm{ols}}\\) (SE)\n\\(\\hat\\beta_{\\textrm{gls}}\\) (SE)\n\n\n\n\nIntercept\n192.240** (97.854)\n195.320** (98.845)\n\n\nLongitude\n1.757 (1.088)\n1.780 (1.097)\n\n\nLatitude\n-1.317 (2.556)\n-0.974 (2.597)\n\n\nDay\n-1.216*** (0.134)\n-1.237*** (0.136)\n\n\nLongitude × Latitude\n-0.026 (0.028)\n-0.022 (0.029)\n\n\nLongitude × Day\n-0.023*** (0.001)\n-0.023*** (0.001)\n\n\nLatitude × Day\n-0.019*** (0.002)\n-0.019*** (0.002)\n\n\n\\(\\alpha_{1}\\)\n16.647*** (4.832)\n19.174*** (4.849)\n\n\n\\(\\alpha_{2}\\)\n18.528*** (3.056)\n16.224*** (3.125)\n\n\n\\(\\alpha_{3}\\)\n-6.607** (3.172)\n-4.204 (3.199)\n\n\n\\(\\alpha_{4}\\)\n30.545*** (4.370)\n27.500*** (4.493)\n\n\n\\(\\alpha_{5}\\)\n14.739*** (2.747)\n13.957*** (2.759)\n\n\n\\(\\alpha_{6}\\)\n-17.541*** (3.423)\n-15.779*** (3.461)\n\n\n\\(\\alpha_{7}\\)\n28.472*** (3.552)\n25.985*** (3.613)\n\n\n\\(\\alpha_{8}\\)\n-27.348*** (3.164)\n-25.230*** (3.202)\n\n\n\\(\\alpha_{9}\\)\n-10.235** (4.457)\n-7.401 (4.556)\n\n\n\\(\\alpha_{10}\\)\n10.558*** (3.327)\n8.561** (3.396)\n\n\n\\(\\alpha_{11}\\)\n-22.758*** (3.533)\n-19.834*** (3.569)\n\n\n\\(\\alpha_{12}\\)\n21.864*** (4.813)\n17.771*** (5.041)\n\n\nObservations\n3,989\n3,989\n\n\n\nNote: \\(^{*}p &lt; 0.1\\); \\(^{**}p &lt; 0.05\\); \\(^{***}p &lt; 0.01\\)\n\n\n\nThe middle column of Table 3.1 shows the OLS parameter estimates and their standard errors (i.e., square root of their variances) from the OLS fit of this regression model, assuming independent errors. The standard errors suggest that longitude, latitude, and the longitude–latitude interaction, are not important in the model given all of the other variables included in the model, based on the observation that their confidence intervals cover zero. It might be surprising to think that latitude is not important here, since we saw in Chapter 2 that there is a clear latitudinal dependence in temperature for these data (it is typically cooler the further north you go in the central USA). But recall that when interpreting parameters in multiple regression we are considering their importance in the presence of all of the other variables in the model. Thus, this result may be due to the fact that there are interactions of the latitude effect with longitude and/or time, or it could be due to other factors. We discuss some of these below.\nAs discussed in Section 3.2, the residuals from this regression fit exhibit spatio-temporal dependence, and thus the OLS assumption of independent errors is violated, which calls into question the validity of the standard errors given in the middle column of Table Table 3.1. As already mentioned, in the case of positive dependence (present in the residuals here) the standard errors are underestimated, potentially implying that a covariate is important in the model when it really is not. In the right-hand column we show the estimates and standard errors after fitting using GLS, where the covariance of the errors is assumed, a priori, to be a function of distance in space and time, specifically constructed from a Gaussian kernel with bandwidth 0.5 (see Lab 3.2 for details). Note that all the standard errors are larger, and some of our conclusions have changed regarding which effects are significant, and which are not.\nReaders who are familiar with regression analysis may also recall that there are other factors that might affect the standard errors given in Table 3.1. For example, the presence of moderate to serious multicollinearity in the covariates (e.g., when some linear combination of \\(X\\)s is approximately equal to one or more of the other \\(X\\) variables) can inflate the standard errors. In Lab 3.2, we see the effect of adding another basis function, \\(\\phi_{13}(\\mathbf{s})\\), that is a slightly noisy version of \\(\\phi_5(\\mathbf{s})\\). Without \\(\\phi_{13}(\\mathbf{s})\\), the effect of \\(\\phi_5(\\mathbf{s})\\) is considered significant in the model (see Table 3.1). However, the estimate of \\(\\alpha_5\\) is not significant at the 1% level when both \\(\\phi_5(\\mathbf{s})\\) and \\(\\phi_{13}(\\mathbf{s})\\) are included in the model.\nInference can also be affected by confounding, in which interpretation or significance is substantially altered when an important variable is ignored, or perhaps when an extraneous variable is included in the model. Since we typically do not know or have access to all of the important variables in a regression, this is often a problem. Indeed, one of the interpretations of dependent errors in spatial, time-series, and spatio-temporal models is that they probably represent the effects of covariates that were left out of the model. As we describe in Chapter 4, this implies that there can be confounding between the spatio-temporally dependent random errors and covariates of primary interest, which can affect parameter inference and accompanying interpretations. But, if our goal is spatio-temporal prediction, this confounding is not necessarily a bad thing, since building dependence into the model is somewhat of an “insurance policy” against our model missing important covariates.\n\n\n\n\n\n\nTip\n\n\n\nSeveral excellent packages can be used to neatly display results from models in R, such as xtable and stargazer. All tables in this chapter were produced using stargazer.\n\n\n\n\n3.2.3 Variable Selection\nAs mentioned in the previous section, it can be the case that when \\(p\\) (the number of covariates) is fairly large, we do not believe that all of them are truly related to the response, and we are interested in choosing which are the most important. This is generally called variable selection. Outside the context of regression, Chapter 6 considers the more general problem of model selection.\nIt would be ideal if we could test all possible combinations of all \\(p\\) covariates and determine which one gives the best predictive ability. This can be done if \\(p\\) is small, but it quickly becomes problematic for large \\(p\\) as there are \\(2^p\\) possible models that would have to be considered, assuming all of them have an intercept parameter. Alternatively, we can consider a best subsets procedure that uses a special algorithm (such as the “leaps and bounds algorithm”) to efficiently find a few of the best models for a given number of covariates (see, for example, Kutner et al., 2004).\nAnother option is to use an automated selection algorithm such as forward selection. In this case, we start with a model that includes just the intercept, and then we find which covariate reduces the error sums of squares (or some other chosen model-selection criterion) the most. That covariate is added to the model, and we then consider which of the remaining \\((p-1)\\) gives the best two-variable model. We continue this until some pre-specified stopping rule is reached. In the context of the regression with the NOAA data set, Table 3.2 shows the best candidate models for one to four variables (in addition to the intercept), as obtained by the forward-selection algorithm using the function step in R; here the Akaike information criterion (AIC, see Section 6.4.4) was adopted as the model-selection criterion. Note how the residual standard error decreases sharply with the inclusion of one covariate (in this case, latitude) and slowly thereafter. We have already seen that there is considerable correlation between maximum temperature and latitude, so this is not surprising. As further evidence of this, note that latitude, which was not significant in the full model, is the single most important variable according to forward stepwise selection. But when the latitude-by-day interaction term enters the model, the parameter estimate for latitude decreases noticeably. For comparison, Table 3.3 shows the same forward-selection analysis but now using the residual sum of squares (RSS) as the model-selection criterion. Note that this still has latitude as the most important single variable, but the longitude-by-day interaction is the second variable entered into the model (followed by the latitude-by-day variable), and the day variable is not included. This shows that the choice of criterion can make a substantial difference when doing stepwise selection: the AIC criterion penalizes for model complexity (i.e., the number of variables in the model), whereas the RSS criterion does not.\nAlternative stepwise methods include backward-selection and mixed-selection algorithms (see James et al., 2013, Chapter 6). Note that no stepwise procedure is guaranteed to give the best model other than for the single-covariate case, but these methods can provide potential candidate models that are reasonable. It is also important to realize that the forward-selection procedure can be used in the “large \\(p\\), small \\(n\\)” case where one has more covariates \\(p\\) than observations \\(n\\), at least up to models of size \\(n-1\\), which is increasingly common in “big data” statistical-learning applications (James et al., 2013). (Note that in this book we prefer to use \\(m\\) instead of \\(n\\) to represent sample size for spatio-temporal data.)\nThe subset-selection methods discussed above penalize model complexity at the expense of model fit by removing variables. This is a manifestation of a common problem in statistics, balancing the trade-off between variance and bias. That is, these methods trade some bias for variance reduction by removing variables. Another approach to this problem in regression is to constrain the least squares estimates in such a way that the regression coefficients are regularized (or shrunk) towards zero, hence adding bias. The two most-used approaches for regularization in regression are ridge regression and the lasso. These are briefly described in Note 3.4.\n\n\n\nTable 3.2: Estimated regression coefficients for the linear regression model of Section 3.2 when using ordinary least squares to estimate the parameters and forward selection based on the AIC, starting from the intercept-only model. One, two, and three asterisks are used to denote significance at the 10%, 5%, and 1% levels of significance, respectively. Note that the residual standard error when fitting the full model (\\(p=18\\)) was \\(4.230\\).\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n(1)\n(2)\n(3)\n(4)\n(5)\n\n\n\n\nIntercept\n88.673***\n148.940***\n147.840***\n136.810***\n138.420***\n\n\nLatitude\n\n-1.559***\n-1.559***\n-1.274***\n-1.273***\n\n\nDay\n\n\n0.069***\n0.755***\n0.755***\n\n\nLatitude × Day\n\n\n\n-0.018***\n-0.018***\n\n\nLongitude\n\n\n\n\n0.019\n\n\nObservations\n3,989\n3,989\n3,989\n3,989\n3,989\n\n\nResidual Std. Error\n7.726\n4.710\n4.669\n4.626\n4.625\n\n\n\nNote: \\(^{*}p &lt; 0.1\\); \\(^{**}p &lt; 0.05\\); \\(^{***}p &lt; 0.01\\)\n\n\n\n\n\n\nTable 3.3: Same as Table 3.2 but using a forward-selection criterion given by the total residual sum of squares.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n(1)\n(2)\n(3)\n(4)\n(5)\n\n\n\n\nIntercept\n88.673***\n148.940***\n147.780***\n140.420***\n122.020***\n\n\nLatitude\n\n-1.559***\n-1.560***\n-1.366***\n-0.838***\n\n\nLongitude × Day\n\n\n-0.001***\n-0.006***\n-0.011***\n\n\nLatitude × Day\n\n\n\n-0.012***\n-0.023***\n\n\n\\(\\alpha_{10}\\)\n\n\n\n\n-6.927***\n\n\nObservations\n3,989\n3,989\n3,989\n3,989\n3,989\n\n\nResidual Std. Error\n7.726\n4.710\n4.661\n4.607\n4.470\n\n\n\nNote: \\(^{*}p &lt; 0.1\\); \\(^{**}p &lt; 0.05\\); \\(^{***}p &lt; 0.01\\)\n\n\n\n\n\n\n\n\n\nNote 3.4: Ridge and Lasso Regression\n\n\n\nRecall that the OLS spatio-temporal regression estimates are found by minimizing the RSS given in Equation 3.7. One can consider a regularization in which a penalty term is added to the RSS that effectively shrinks the regression parameter estimates towards zero. Specifically, consider estimates of \\(\\boldsymbol{\\beta}\\) that come from a penalized (regularization) form of the RSS given by\n\\[\n\\sum_{j=1}^T \\sum_{i=1}^{m} \\left[Z(\\mathbf{s}_{i};t_j) - (\\beta_0 + \\beta_1 X_1(\\mathbf{s}_{i};t_j) + \\ldots + \\beta_p X_p(\\mathbf{s}_{i};t_j))\\right]^2 +  \\lambda \\sum_{\\ell=1}^p |\\beta_\\ell|^q,\n\\tag{3.9}\\]\nwhere \\(\\lambda\\) is a tuning parameter and \\(\\sum_{\\ell=1}^p |\\beta_\\ell|^q\\) is the penalty term. Note that the penalty term does not include the intercept parameter \\(\\beta_0\\). When \\(q=2\\), the estimates, say \\(\\widehat{\\boldsymbol{\\beta}}_R\\), are said to be ridge regression estimates, and when \\(q=1\\) the estimates, say \\(\\widehat{\\boldsymbol{\\beta}}_L\\), are lasso estimates. Clearly, \\(q=2\\) corresponds to the square of an \\(L_2\\)-norm penalty and \\(q=1\\) corresponds to an \\(L_1\\)-norm penalty; recall that the \\(L_2\\)-norm of a vector \\(\\mathbf{a}= (a_1,\\ldots,a_q)'\\) is given by \\(\\sqrt{\\sum_{k=1}^q a_k^2}\\), and the \\(L_1\\)-norm is given by \\(\\sum_{k=1}^q | a_k|\\).\nThus, minimizing Equation 3.9 with respect to the regression coefficients subject to these penalty constraints attempts to balance the model fit (variance) given by the first term and shrinking the parameters towards zero (adding bias) via the penalty term. It is clear that both the ridge-regression estimates, \\(\\widehat{\\boldsymbol{\\beta}}_R\\), and the lasso estimates, \\(\\widehat{\\boldsymbol{\\beta}}_L\\), should be closer to zero than the equivalent OLS estimates. (When \\(\\lambda = 0\\), the ridge or lasso estimates are just the OLS estimates.) A potential advantage of the lasso is that it can shrink parameters exactly to zero (unlike ridge regression, which only shrinks towards zero). This provides a more explicit form of variable selection. More general {regularization} in regression can be achieved by assigning prior distributions to the parameters \\(\\boldsymbol{\\beta}\\) and considering the analysis from a Bayesian perspective. Indeed, both ridge and lasso regression have equivalent Bayesian formulations. In practice, one selects the tuning parameter \\(\\lambda\\) by cross-validation. Note also that these penalized regression estimates are not scale-invariant, so one typically scales (and centers) the \\(X\\)s when implementing ridge or lasso regression. See James et al. (2013) for more information about these procedures.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Spatio-Temporal Statistical Models</span>"
    ]
  },
  {
    "objectID": "Chapter3.html#spatio-temporal-forecasting",
    "href": "Chapter3.html#spatio-temporal-forecasting",
    "title": "3  Spatio-Temporal Statistical Models",
    "section": "3.3 Spatio-Temporal Forecasting",
    "text": "3.3 Spatio-Temporal Forecasting\nAs an example of the third goal of spatio-temporal modeling, suppose we want to forecast the sea surface temperature (SST) in the tropical Pacific Ocean six months from now. For example, the top left panel of Figure 3.10 shows SST anomalies, which are just deviations from long-term monthly averages, for April 1997, and the bottom right panel shows the SST anomalies for October 1997. You might ask why we would be interested in predicting SST six months ahead. As it turns out, the so-called El Niño Southern Oscillation (ENSO) phenomenon is in this region, which is characterized by frequent (but not regular) periods of warmer-than-normal and cooler-than-normal ocean temperatures, and ENSO has a dramatic effect on worldwide weather patterns and associated impacts (e.g., droughts, floods, tropical storms, tornadoes). Thus, being able to predict these warmer (El Niño) or cooler (La Niña) periods can help with resource and disaster planning. The series of plots shown in Figure 3.10 corresponds to a major El Niño event.\n\n\n\n\n\n\nFigure 3.10: Tropical Pacific Ocean SST anomalies for April, June, August, and October 1997. This series of plots shows the onset of the extreme El Niño event that happened in the Northern Hemisphere in the fall of 1997.\n\n\n\nOne way we might try to forecast the SST anomalies into the future is to use regression. For example, the Southern Oscillation Index (SOI) is a well-known indicator of ENSO that is regularly recorded; here we consider it at monthly time steps. In what follows, we use the SOI index at time \\(t\\) (e.g., April 1997) to forecast the SST at time \\(t+\\tau\\) (e.g., October 1997, where \\(\\tau = 6\\) months). We do this for each spatial location separately, so that each oceanic pixel in the domain shown in Figure 3.10 gets its own simple linear regression (including an intercept coefficient and a coefficient corresponding to the lagged SOI value). The top panels in Figure 3.11 show the intercept (left) and SOI regression coefficient (right) for the regression fit at each location. Note the fairly distinct pattern in these coefficients that corresponds to the El Niño warm region in Figure 3.10 – clearly, these estimated regression coefficients exhibit quite strong spatial dependencies. The middle panels in Figure 3.11 show contour plots of the actual anomalies for October 1997 (left), as well as the pixelwise simple-linear-regression forecast based on SOI from April 1997 (right; note the different color scale). The associated regression-forecast prediction standard error (see Note 3.3) is given in the bottom panel.\nIt is clear that although the forecast in the middle-right panel of Figure 3.10 captures the broad El Niño feature, it is very biased towards a cooler anomaly than that observed. This illustrates that we likely need additional information to perform a long-lead forecast of SST, something we discuss in more detail using dynamic models in Chapter 5. This example also shows that it might be helpful to account for the fact that these regression-coefficient estimates show such strong spatial dependence. This is often the case in time-series regressions at nearby spatial locations, and we shall see another example of this in Section 4.4.\n\n\n\n\n\n\nFigure 3.11: Top: Spatially varying estimated intercept (left) and spatially varying estimated regression coefficient of lagged SOI (right) in a simple linear regression of lagged SOI on the SST anomalies at each oceanic spatial location (fitted individually). Middle: Contour plots of SST anomalies for October 1997 (left) and six-month-ahead forecast (right) based on a simple linear regression model regressed on the SOI value in April 1997. Note the different color scales. Bottom: Prediction standard error for the forecast (see Note 3.3).\n\n\n\n\n\n\n\n\n\nTip\n\n\n\nFitting multiple models to groups of data in a single data frame in long format has been made easy and computationally efficient using functions in the packages tidyr, purrr, and broom. Take a look at Labs 3.2 and 3.3 to see how multiple models, predictions, and tests can be easily carried out using these packages.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Spatio-Temporal Statistical Models</span>"
    ]
  },
  {
    "objectID": "Chapter3.html#non-gaussian-errors",
    "href": "Chapter3.html#non-gaussian-errors",
    "title": "3  Spatio-Temporal Statistical Models",
    "section": "3.4 Non-Gaussian Errors",
    "text": "3.4 Non-Gaussian Errors\nYou have probably already heard about the normal distribution that was used to describe the regression errors in the previous sections. The name “normal” seems to imply that any other distribution is abnormal – not so! Data that are binary or counts or skewed are common and of great interest to scientists and statisticians. Consequently, in spatial and spatio-temporal statistics we use the terminology Gaussian distribution and “Gau” instead of “\\(N\\),” which falls into line with the well-known Gaussian processes defined in time or in Euclidean space (see Section 4.2). There are many off-the-shelf methods that can be used for non-Gaussian modeling – both from the statistics perspective and from the machine-learning perspective. By “machine learning” we are referring to methods that do not explicitly account for the random spatio-temporal nature of the data. From the statistical perspective, we could simply use a generalized linear model (GLM) or a generalized additive model (GAM) to analyze spatio-temporal data.\n\n3.4.1 Generalized Linear Models and Generalized Additive Models\nThe basic GLM has two components, a random component and a systematic component. The random component assumes that observations, conditioned on their respective means and (in some cases) scaling parameters, are independent and come from the exponential family of distributions. That is,\n\\[\nZ(\\mathbf{s}_{i};t_j) | Y(\\mathbf{s}_{i};t_j), \\gamma \\; \\sim \\; \\text{indep.} \\; EF(Y(\\mathbf{s}_{i};t_j);\\gamma),\n\\tag{3.10}\\]\nwhere \\(EF( \\cdot)\\) refers to the exponential family, \\(Y(\\mathbf{s}_{i};t_j)\\) is the mean, and \\(\\gamma\\) is a scale parameter (see McCulloch & Searle, 2001 for for details). Members of the exponential family include common distributions such as the normal (Gaussian), Poisson, binomial, and gamma distributions.\nThe systematic component of the GLM then specifies a relationship between the mean response and the covariates. In particular, the systematic component consists of a link function that transforms the mean response and then expresses this transformed mean in terms of a linear function of the covariates. In our notation, this is given by\n\\[\ng(Y(\\mathbf{s};t)) = \\beta_0 + \\beta_1 X_1(\\mathbf{s};t) + \\beta_2 X_2(\\mathbf{s};t) + \\ldots + \\beta_p X_p(\\mathbf{s};t),\n\\tag{3.11}\\]\nwhere \\(g(\\cdot)\\) is some specified monotonic link function. Note that in a classic GLM there is no additive random effect term in Equation 3.11, but this can be added to make the model a generalized linear mixed model (GLMM), where “mixed” refers to having both fixed and random effects in the model for \\(g(Y(\\mathbf{s};t))\\).\nThe GAM is also composed of a random component and a systematic component. The random component is the same as for the GLM, namely Equation 3.10. In addition, like the GLM, the systematic component of the GAM also considers a transformation of the mean response related to the covariates, but it assumes a more flexible function of the covariates. That is,\n\\[\ng(Y(\\mathbf{s};t)) = \\beta_0 +  f_1(X_1(\\mathbf{s};t)) + f_2(X_2(\\mathbf{s};t)) + \\ldots + f_p(X_p(\\mathbf{s};t)),\n\\tag{3.12}\\]\nwhere the functions \\(\\{f_k(\\cdot)\\}\\) can have a specified parametric form (such as polynomials in the covariate), or, more generally, they can be some smooth function specified semi-parametrically or nonparametrically. Often, \\(f_k(\\cdot)\\) is written as a basis expansion (see Wood, 2017 for more details). Thus, the GLM is a special parametric case of the GAM. These models can be quite flexible. Again, note that a random effect can be added to Equation 3.12, as with the GLM, in which case the model becomes a generalized additive mixed model (GAMM).\nAs with normal (Gaussian) error regression, so long as covariates (or functions of these in the case of GAMs) are available at any location in the space-time domain, GLMs or GAMs can be used for spatio-temporal prediction. Whether or not this accommodates sufficiently the dependence in the observations depends on the specific data set and the covariates that are available. A straightforward way to fit a GLM in R is to use the function glm. In Lab 3.4 we fit a GLM to the Carolina wren counts in the BBS data set, where we assume a Poisson response and a log link. We consider the same classes of covariates used in the regression example in Section 3.2, where the response was Tmax in the NOAA data set. The latent mean surface is given by Equation 3.11 (with estimated regression parameters \\(\\boldsymbol{\\beta}\\)) and is illustrated in Figure 3.12. This latent spatial surface captures the large-scale trends, but it is unable to reproduce the small-scale spatial and temporal fluctuations in the Carolina wren intensity, and the residuals show both temporal and spatial correlation. We could accommodate this additional dependence structure by adding more basis functions and treating their regression coefficients as fixed effects, but this will likely result in overfitting. In Chapter 4 we explore the use of random effects to circumvent this problem.\n\n\n\n\n\n\nFigure 3.12: Prediction of \\(\\log Y(\\cdot)\\) for the Carolina wren sighting data set on a grid between \\(t=1\\) (the year 1994) and \\(t=21\\) (2014) based on a Poisson response model, implemented with the function glm. The log of the observed count is shown in circles using the same color scale.\n\n\n\nRecall that it is useful to consider residuals in the linear-regression context to evaluate the model fit and potential violations of model assumptions. In the context of GLMs, we typically consider a special type of residual when the data are not assumed to come from a Gaussian distribution. Note 3.5 defines so-called deviance residuals and Pearson (chi-squared) residuals, which are often used for GLM model evaluation (see, for example, McCullagh & Nelder, 1989). Heuristically, examining these residuals for spatio-temporal structure can often suggest that additional spatial, temporal, or spatio-temporal random effects are needed in the model, or that a different response model is warranted (e.g., to account for over-dispersion; see Lab 3.4).\n\n\n\n\n\n\nNote 3.5: Deviance and Pearson Residuals\n\n\n\nOne way to consider the agreement between a model and data is to compare the predictions of the model to a “saturated” model that fits the data exactly. In GLMs, this corresponds to the notion of deviance. Specifically, suppose we have a model for an \\(m\\)-dimensional vector of data \\(\\mathbf{Z}\\) that depends on parameters \\(\\boldsymbol{\\theta}_{\\mathrm{model}}\\) and has a log-likelihood given by \\(\\ell(\\mathbf{Z};\\boldsymbol{\\theta}_{\\mathrm{model}})\\). We then define the deviance as\n\\[\nD(\\mathbf{Z};\\widehat{\\boldsymbol{\\theta}}_{\\mathrm{model}}) = 2 \\{ \\ell(\\mathbf{Z}; \\widehat{\\boldsymbol{\\theta}}_{\\mathrm{sat}}) - \\ell(\\mathbf{Z};\\widehat{\\boldsymbol{\\theta}}_{\\mathrm{model}}) \\} =   \\sum_{i=1}^m D(Z_i;\\widehat{\\boldsymbol{\\theta}}_{\\mathrm{model}}),\n\\]\nwhere \\(\\ell(\\mathbf{Z};\\widehat{\\boldsymbol{\\theta}}_{\\mathrm{sat}})\\) is the log-likelihood for the so-called saturated model, which is the model that has one parameter per observation (i.e., that fits the data exactly). Note that \\(D(Z_i;\\widehat{\\boldsymbol{\\theta}}_{\\mathrm{model}})\\) corresponds to the contribution of observation \\(Z_i\\) to the deviance given the parameter estimates \\(\\boldsymbol{\\theta}_{\\mathrm{model}}\\). The deviance is just 2 times the log-likelihood ratio of the full (saturated) model relative to the reduced model of interest. We then define the deviance residual as\n\\[\nr_{d,i} \\equiv \\mbox{sign}(Z_i - \\widehat{\\mu}_i) \\sqrt{D(Z_i;\\widehat{\\boldsymbol{\\theta}}_{\\mathrm{model}})},\n\\tag{3.13}\\]\nwhere \\(\\widehat{\\mu}_i\\) corresponds to \\(E(Z_i | \\widehat{\\boldsymbol{\\theta}}_{\\mathrm{model}})\\), the estimate of the mean response from the model given parameter estimates, \\(\\widehat{\\boldsymbol{\\theta}}_{\\mathrm{model}}\\). The \\(\\mbox{sign}(\\cdot)\\) function in Equation 3.13 assigns the sign of the residual to indicate whether the mean response is less than or greater than the observation. In practice, we often consider standardized deviance residuals (see, for example, McCullagh & Nelder, 1989).\nAlternatively, we can define a standardized residual that more directly considers the difference between the data and the estimated mean response. That is,\n\\[\nr_{p,i} \\equiv \\frac{(Z_i - \\widehat{\\mu}_i)^2}{V(\\widehat{\\mu}_i)},\n\\]\nwhere \\(V(\\widehat{\\mu}_i)\\) is called the variance function, and it is generally a function of the mean response (except when the likelihood is Gaussian). The specific form of the variance function depends on the form of the data likelihood. The unsigned residual, \\(r_{p,i}\\), is known as a Pearson residual (or Pearson chi-squared residual) because the sum of these residuals for all \\(i=1,\\ldots,m\\) gives a Pearson chi-squared statistic, which can be used for formal hypothesis tests of model adequacy (see, for example, McCullagh & Nelder, 1989).",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Spatio-Temporal Statistical Models</span>"
    ]
  },
  {
    "objectID": "Chapter3.html#sec-HSTmods",
    "href": "Chapter3.html#sec-HSTmods",
    "title": "3  Spatio-Temporal Statistical Models",
    "section": "3.5 Hierarchical Spatio-Temporal Statistical Models",
    "text": "3.5 Hierarchical Spatio-Temporal Statistical Models\nThe previous sections showed that it may be possible to accomplish the goals of spatio-temporal modeling without using specialized methodology. However, it was also clear from those examples that there are some serious limitations with the standard methodology. In particular, our methods should be able to include measurement uncertainty explicitly, they should have the ability to predict at locations in time or space, and they should allow us to perform parameter inference when there are dependent errors. In the remainder of this book, we shall describe models that can deal with these problems.\nTo put our spatio-temporal statistical models into perspective, we consider a hierarchical spatio-temporal model that includes at least two stages. Specifically,\n\\[\n\\text{observations} \\; = \\; \\text{true process} \\;\\; + \\;\\; \\text{observation error}\n\\tag{3.14}\\] \\[\n\\text{true process} \\; = \\; \\text{regression component} \\;\\; + \\;\\; \\text{dependent random process},\n\\tag{3.15}\\]\nwhere Equation 3.14 and Equation 3.15 are the first two stages of the hierarchical-statistical-model paradigm presented in Chapter 1. There are two general approaches to modeling the last term in Equation 3.15: the descriptive approach and the dynamic approach; see Section 1.2.1. The descriptive approach is considered in Chapter 4 and offers a more traditional perspective. In that case, the dependent random process in Equation 3.15 is defined in terms of the first-order and second-order moments (means, variances, and covariances) of its marginal distribution. This framework is not particularly concerned with the underlying causal structure that leads to dependence in the random process. Rather, it is most useful for the first two goals presented in Section 1.2: spatio-temporal prediction and parameter inference.\nIn contrast, we consider the dynamic approach in Chapter 5. In that case, the modeling effort is focused on conditional distributions that describe the evolution of the dependent random process in time; it is most useful for the third goal - forecasting (but also can be used for the other two goals). We note that the conditional perspective can also be considered in the context of mixed-effects descriptive models, with or without a dynamic specification, as we discuss in Section 4.4.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Spatio-Temporal Statistical Models</span>"
    ]
  },
  {
    "objectID": "Chapter3.html#chapter-3-wrap-up",
    "href": "Chapter3.html#chapter-3-wrap-up",
    "title": "3  Spatio-Temporal Statistical Models",
    "section": "3.6 Chapter 3 Wrap-Up",
    "text": "3.6 Chapter 3 Wrap-Up\nThe primary purpose of this chapter was to discuss in detail the three goals of spatio-temporal statistical modeling: predicting at a new location in space given spatio-temporal data; doing parameter inference with spatio-temporal data; and forecasting a new value at a future time. We have also emphasized the importance of quantifying the uncertainty in our predictions, parameter estimates, and forecasts. We showed that deterministic methods for spatio-temporal prediction are sensible in that they typically follow Tobler’s law and give more weight to nearby observations in space and time; however, they do not provide direct estimates of the prediction uncertainty. We then showed that one could use a (linear) regression model with spatio-temporal data and that, as long as the residuals do not have spatio-temporal dependence, it is easy to obtain statistically optimal predictions and, potentially, statistically optimal forecasts. With respect to parameter inference, we showed that the linear-regression approach is again relevant but that our inference can be misleading in the presence of unmodeled extra variation, dependent errors, multicollinearity, and confounding. Finally, we showed that standard generalized linear models or generalized additive models can be used for many problems with non-Gaussian data. But again, without random effects to account for extra variation and dependence, these models are likely to give inappropriate prediction uncertainty and inferences.\nThe methods presented in this chapter are very common throughout the literature, and the references provided in the chapter are excellent places to find additional background material. Of course, topics such as interpolation, regression, and generalized linear models are discussed in a wide variety of textbooks and online resources, and the interested reader should have no trouble finding additional references.\nIn the next two chapters, we explore what to do when there is spatio-temporal dependence beyond what can be explained by covariates. We shall cover descriptive models that focus more on the specification of spatio-temporal covariance functions in Chapter 4, and dynamic models that focus explicitly on the evolution of spatial processes through time in Chapter 5. These two chapters together make up the “protein” in the book, and the material in them will have a decidedly more technical flavor. More powerful, more flexible, but more complex, dependent processes require a higher technical level than is usually found in introductory statistical-modeling courses. That said, we maintain an emphasis on describing the motivations for our methods and on their implementation in the associated R Labs.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Spatio-Temporal Statistical Models</span>"
    ]
  },
  {
    "objectID": "Chapter3.html#lab-3.1-deterministic-prediction-methods",
    "href": "Chapter3.html#lab-3.1-deterministic-prediction-methods",
    "title": "3  Spatio-Temporal Statistical Models",
    "section": "Lab 3.1: Deterministic Prediction Methods",
    "text": "Lab 3.1: Deterministic Prediction Methods\n\nInverse Distance Weighting\nInverse distance weighting (IDW) is one of the simplest deterministic spatio-temporal interpolation methods. It can be implemented easily in R using the function idw in the package gstat, or from scratch, and in this Lab we shall demonstrate both approaches. We require the following packages.\n\nlibrary(\"dplyr\")\nlibrary(\"fields\")\nlibrary(\"ggplot2\")\nlibrary(\"gstat\")\nlibrary(\"RColorBrewer\")\nlibrary(\"sp\")\nlibrary(\"spacetime\")\nlibrary(\"STRbook\")\n\nWe consider the maximum temperature field in the NOAA data set for the month of July 1993. These data can be obtained from the data NOAA_df_1990 using the filter function in dplyr.\n\ndata(\"NOAA_df_1990\", package = \"STRbook\")\nTmax &lt;- filter(NOAA_df_1990,       # subset the data\n              proc == \"Tmax\" &     # only max temperature\n              month == 7 &         # July\n              year == 1993)        # year of 1993\n\nWe next construct the three-dimensional spatio-temporal prediction grid using expand.grid. We consider a 20 \\(\\times\\) 20 grid in longitude and latitude and a sequence of 6 days regularly arranged in the month.\n\npred_grid &lt;- expand.grid(lon = seq(-100, -80, length = 20),\n                         lat = seq(32, 46, length = 20),\n                         day = seq(4, 29, length = 6))\n\nThe function in gstat that does the inverse distance weighting, idw, takes the following arguments: formula, which identifies the variable to interpolate; locations, which identifies the spatial and temporal variables; data, which can take the data in a data frame; newdata, which contains the space-time grid locations at which to interpolate; and idp, which corresponds to \\(\\alpha\\) in Equation 3.3. The larger \\(\\alpha\\) (idp) is, the less the smoothing. This parameter is typically set using cross-validation, which we explore later in this Lab; here we fix \\(\\alpha = 5\\). We run idw below with the variable Tmax, omitting data on 14 July 1993.\n\nTmax_no_14 &lt;- filter(Tmax, !(day == 14))         # remove day 14\nTmax_July_idw &lt;- idw(formula = z ~ 1,            # dep. variable\n                     locations = ~ lon + lat + day, # inputs\n                     data = Tmax_no_14,          # data set\n                     newdata = pred_grid,        # prediction grid\n                     idp = 5)                    # inv. dist. pow.\n\nThe output Tmax_July_idw contains the fields lon, lat, day, and var1.pred corresponding to the IDW interpolation over the prediction grid. This data frame can be plotted using ggplot2 commands as follows.\n\nggplot(Tmax_July_idw) +\n    geom_tile(aes(x = lon, y = lat,\n                  fill = var1.pred)) +\n    fill_scale(name = \"degF\") +    # attach color scale\n    xlab(\"Longitude (deg)\") +           # x-axis label\n    ylab(\"Latitude (deg)\") +            # y-axis label\n    facet_wrap(~ day, ncol = 3) +       # facet by day\n    coord_fixed(xlim = c(-100, -80),\n                ylim = c(32, 46))  +    # zoom in\n    theme_bw()                          # B&W theme\n\nA similar plot to the one above, but produced using stplot instead, is shown in the left panel of Figure 3.2. Notice how the day with missing data is “smoothed out” when compared to the others. As an exercise, you can redo IDW including the 14 July 1993 in the data set, and observe how the prediction changes for that day.\n\nImplementing IDW from First Principles\nIt is often preferable to implement simple algorithms, like IDW, from scratch, as doing so increases code versatility (e.g., it facilitates implementation of a cross-validation study). Reducing dependence on other packages will also help the code last the test of time (as it becomes immune to package changes).\nWe showed that IDW is a kernel predictor and yields the kernel weights given by Equation 3.1. To construct these kernel weights we first need to find the distances between all prediction locations and data locations, take their reciprocals and raise them to the power (idp) of \\(\\alpha\\). Pairwise distances between two arbitrary sets of points are most easily computed using the rdist function in the package fields. Since we wish to generate these kernel weights for different observation and prediction sets and different bandwidth parameters, we create a function Wt_IDW that generates the required kernel-weights matrix.\n\npred_obs_dist_mat &lt;- rdist(select(pred_grid, lon, lat, day),\n                           select(Tmax_no_14, lon, lat, day))\nWt_IDW &lt;- function(theta, dist_mat) 1/dist_mat^theta\nWtilde &lt;- Wt_IDW(theta = 5, dist_mat = pred_obs_dist_mat)\n\nThe matrix Wtilde now contains all the \\(\\tilde{w}_{ij}\\) described in Equation 3.3; that is, the \\((k,l)\\)th element in Wtilde contains the distance between the \\(k\\)th prediction location and the \\(l\\)th observation location, raised to the power of 5, and reciprocated.\nNext, we compute the weights in Equation 3.2. These are just the kernel weights normalized by the sum of all kernel weights associated with each prediction location. Normalizing the weights at every location can be done easily using rowSums in R.\n\nWtilde_rsums &lt;- rowSums(Wtilde)\nW &lt;- Wtilde/Wtilde_rsums\n\nThe resulting matrix W is the weight matrix, sometimes known as the influence matrix. The predictions are then given by Equation 3.1, which is just the influence matrix multiplied by the data.\n\nz_pred_IDW &lt;- as.numeric(W %*% Tmax_no_14$z)\n\nOne can informally verify the computed predictions by comparing them to those given by idw in gstat. We see that the two results are very close; numerical mismatches of this order of magnitude are likely to arise from the slightly different way the IDW weights are computed in gstat (and it is possible that you get different, but still small, mismatches on your computer).\n\nsummary(Tmax_July_idw$var1.pred - z_pred_IDW)\n\n      Min.    1st Qu.     Median       Mean    3rd Qu.       Max. \n-1.080e-12 -1.421e-13  1.421e-14  2.428e-15  1.563e-13  1.037e-12 \n\n\n\n\n\nGeneric Kernel Smoothing and Cross-Validation\nOne advantage of implementing IDW from scratch is that now we can change the kernel function to whatever we want and compare predictions from different kernel functions. We implement a kernel smoother below, where the kernel is a Gaussian radial basis function given by Equation 3.4 with \\(\\theta = 0.5\\).\n\ntheta &lt;- 0.5                       # set bandwidth\nWt_Gauss &lt;- function(theta, dist_mat) exp(-dist_mat^2/theta)\nWtilde &lt;- Wt_Gauss(theta = 0.5, dist_mat = pred_obs_dist_mat)\nWtilde_rsums &lt;- rowSums(Wtilde)    # normalizing factors\nW &lt;- Wtilde/Wtilde_rsums           # normalized kernel weights\nz_pred2 &lt;- W %*% Tmax_no_14$z      # predictions\n\nThe vector z_pred2 can be assigned to the prediction grid pred_grid and plotted using ggplot2 as shown above. Note that the predictions are similar, but not identical, to those produced by IDW. But which predictions are the best in terms of squared prediction error? A method commonly applied to assess goodness of fit is known as cross-validation (CV). CV also allows us to choose bandwidth parameters (i.e., \\(\\alpha\\) or \\(\\theta\\)) that are optimal for a given data set. See Section 6.1.3 for more discussion on CV.\nTo carry out CV, we need to fit the model using a subset of the data (known as the training set), predict at the data locations that were omitted (known as the validation set), and compute a discrepancy, usually the squared error, between the predicted and observed values. If we leave one data point out at a time, the procedure is known as leave-one-out cross-validation (LOOCV). We denote the mean of the discrepancies for a particular bandwidth parameter \\(\\theta\\) as the LOOCV score, \\(CV_{(m)}(\\theta)\\) (note that \\(m\\), here, is the number of folds used in the cross-validation; in LOOCV, the number of folds is equal to the number of data points, \\(m\\)).\nThe LOOCV for simple predictors, like kernel smoothers, can be computed analytically without having to refit; see Appendix B. Since the data set is reasonably small, it is feasible here to do the refitting with each data point omitted (since each prediction is just an inner product of two vectors). The simplest way to do LOOCV in this context is to compute the pairwise distances between all observation locations and the associated kernel-weight matrix, and then to select the appropriate rows and columns from the resulting matrix to do prediction at a left-out observation; this is repeated for every observation.\nThe distances between all observations are computed as follows.\n\nobs_obs_dist_mat &lt;- rdist(select(Tmax, lon, lat, day),\n                          select(Tmax, lon, lat, day))\n\nA function that computes the LOOCV score is given as follows.\n\nLOOCV_score &lt;- function(Wt_fun, theta, dist_mat, Z) {\n  Wtilde &lt;- Wt_fun(theta, dist_mat)\n  CV &lt;- 0\n  for(i in 1:length(Z)) {\n    Wtilde2 &lt;- Wtilde[i,-i]\n    W2 &lt;- Wtilde2 / sum(Wtilde2)\n    z_pred &lt;- W2 %*% Z[-i]\n    CV[i] &lt;- (z_pred - Z[i])^2\n  }\n  mean(CV)\n}\n\nThe function takes as arguments the kernel function that computes the kernel weights Wt_fun; the kernel bandwidth parameter theta; the full distance matrix dist_mat; and the data Z. The function first constructs the kernel-weights matrix for the given bandwidth. Then, for the \\(i\\)th observation, it selects the \\(i\\)th row and excludes the \\(i\\)th column from the kernel-weights matrix and assigns the resulting vector to Wtilde2. This vector contains the kernel weights for the \\(i\\)th observation location (which is now a prediction location) with the weights contributed by this \\(i\\)th observation removed. This vector is normalized and then cross-multiplied with the data to yield the prediction. This is done for all \\(i = 1,\\dots,n\\), and then the mean of the squared errors is returned. To see which of the two predictors is “better,” we now simply call LOOCV_score with the two different kernel functions and bandwidths.\n\nLOOCV_score(Wt_fun = Wt_IDW,\n            theta = 5,\n            dist_mat = obs_obs_dist_mat,\n            Z = Tmax$z)\n\n[1] 7.775333\n\nLOOCV_score(Wt_fun = Wt_Gauss,\n            theta = 0.5,\n            dist_mat = obs_obs_dist_mat,\n            Z = Tmax$z)\n\n[1] 7.526056\n\n\nClearly the Gaussian kernel smoother has performed marginally better than IDW in this case. But how do we know the chosen kernel bandwidths are suitable? Currently we do not, as these were set by simply “eye-balling” the predictions and assessing visually whether they looked suitable or not. An objective way to set the bandwidth parameters is to put them equal to those values that minimize the LOOCV scores. This can be done by simply computing LOOCV_score for a set, say 21, of plausible bandwidths and finding the minimum. We do this below for both IDW and the Gaussian kernel.\n\ntheta_IDW &lt;- seq(4, 6, length = 21)\ntheta_Gauss &lt;- seq(0.1, 2.1, length = 21)\nCV_IDW &lt;- CV_Gauss &lt;- 0\nfor(i in seq_along(theta_IDW)) {\n  CV_IDW[i] &lt;- LOOCV_score(Wt_fun = Wt_IDW,\n                           theta = theta_IDW[i],\n                           dist_mat = obs_obs_dist_mat,\n                           Z = Tmax$z)\n  CV_Gauss[i] &lt;- LOOCV_score(Wt_fun = Wt_Gauss,\n                          theta = theta_Gauss[i],\n                          dist_mat = obs_obs_dist_mat,\n                          Z = Tmax$z)\n}\n\nThe plots showing the LOOCV scores as a function of \\(\\alpha\\) and \\(\\theta\\) for the IDW and Gaussian kernels, respectively, exhibit clear minima when plotted, which is very typical of plots of this kind.\n\npar(mfrow = c(1,2))\nplot(theta_IDW, CV_IDW,\n     xlab = expression(alpha),\n     ylab = expression(CV[(m)](alpha)),\n     ylim = c(7.4, 8.5), type = 'o')\nplot(theta_Gauss, CV_Gauss,\n     xlab = expression(theta),\n     ylab = expression(CV[(m)](theta)),\n     ylim = c(7.4, 8.5), type = 'o')\n\nThe optimal inverse-power and minimum LOOCV score for IDW are\n\ntheta_IDW[which.min(CV_IDW)]\n\n[1] 5\n\nmin(CV_IDW)\n\n[1] 7.775333\n\n\nThe optimal bandwidth and minimum LOOCV score for the Gaussian kernel smoother are\n\ntheta_Gauss[which.min(CV_Gauss)]\n\n[1] 0.6\n\nmin(CV_Gauss)\n\n[1] 7.468624\n\n\nOur choice of \\(\\alpha = 5\\) was therefore (sufficiently close to) optimal when doing IDW, while a bandwidth of \\(\\theta = 0.6\\) is better for the Gaussian kernel than our initial choice of \\(\\theta = 0.5\\). It is clear from the results that the Gaussian kernel predictor with \\(\\theta = 0.6\\) has, in this example, provided superior performance to IDW with \\(\\alpha = 5\\), in terms of mean-squared-prediction error.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Spatio-Temporal Statistical Models</span>"
    ]
  },
  {
    "objectID": "Chapter3.html#lab-3.2-trend-prediction",
    "href": "Chapter3.html#lab-3.2-trend-prediction",
    "title": "3  Spatio-Temporal Statistical Models",
    "section": "Lab 3.2: Trend Prediction",
    "text": "Lab 3.2: Trend Prediction\nThere is considerable in-built functionality in R for linear regression and for carrying out hypothesis tests associated with linear models. Several packages have also been written to extend functionality, and in this Lab we shall make use of leaps, which contains functionality for stepwise regression; lmtest, which contains a suite of tests to carry out on fitted linear models; and nlme, which is a package generally used for fitting nonlinear mixed effects models (but we shall use it to fit linear models in the presence of correlated errors).\n\nlibrary(\"leaps\")\nlibrary(\"lmtest\")\nlibrary(\"nlme\")\n\nIn addition, we use ape, which is one of several packages that contain functionality for testing spatial or spatio-temporal independence with Moran’s \\(I\\) statistic; and we use FRK, which contains functionality for constructing the basis functions shown in Figure 3.13. We also make use of broom and purrr to easily carry out multiple tests on groups within our data set.\n\nlibrary(\"ape\")\nlibrary(\"broom\")\nlibrary(\"FRK\")\nlibrary(\"purrr\")\n\nWe need the following for plotting purposes.\n\nlibrary(\"lattice\")\nlibrary(\"ggplot2\")\nlibrary(\"RColorBrewer\")\n\nWe also need the usual packages for data wrangling and handling of spatial/spatio-temporal objects as in the previous Labs.\n\nlibrary(\"dplyr\")\nlibrary(\"gstat\")\nlibrary(\"sp\")\nlibrary(\"spacetime\")\nlibrary(\"STRbook\")\nlibrary(\"tidyr\")\n\n\nFitting the Model\nFor this Lab we again consider the NOAA data set, specifically the maximum temperature data for the month of July 1993. These data can be extracted as follows.\n\ndata(\"NOAA_df_1990\", package = \"STRbook\")\nTmax &lt;- filter(NOAA_df_1990,       # subset the data\n              proc == \"Tmax\" &     # only max temperature\n              month == 7 &         # July\n              year == 1993)        # year of 1993\n\nThe linear model we fit has the form\n\\[\nZ(\\mathbf{s}_i;t) = \\beta_0 + \\beta_1 X_1(\\mathbf{s}_i;t) + \\ldots + \\beta_p X_p(\\mathbf{s}_i;t) + e(\\mathbf{s}_i;t),\n\\tag{3.16}\\]\nfor \\(i=1,\\ldots,n\\) and \\(t=1,\\ldots,T\\), where \\(\\beta_0\\) is the intercept and \\(\\beta_j~(j &gt; 0)\\) is a regression coefficient associated with \\(X_j(\\mathbf{s}_i;t)\\), the \\(j\\)th covariate at spatial location \\(\\mathbf{s}_i\\) and time \\(t\\). We also assume independent errors such that \\(e(\\mathbf{s}_i;t) \\sim \\; indep. \\; N(0,\\sigma^2_e)\\). We consider a model with linear space-time interactions and a set of basis functions that fill the spatial domain:\n\nlinear in \\(lon\\)-coordinate: \\(X_1(\\mathbf{s}_i;t) = s_{1,i}\\), for all \\(t\\);\nlinear in \\(lat\\)-coordinate: \\(X_2(\\mathbf{s}_i;t) = s_{2,i}\\), for all \\(t\\);\nlinear time (day) trend: \\(X_3(\\mathbf{s}_i;t) = t\\), for all \\(\\mathbf{s}_i\\);\n\\(lon\\)–\\(lat\\) interaction: \\(X_4(\\mathbf{s}_i;t) = s_{1,i}s_{2,i}\\), for all \\(t\\);\n\\(lon\\)–\\(t\\) interaction: \\(X_5(\\mathbf{s}_i;t) = s_{1,i}t_i\\), for all \\(s_{2,i}\\);\n\\(lat\\)–\\(t\\) interaction: \\(X_6(\\mathbf{s}_i;t) = s_{2,i}t_i\\), for all \\(s_{1,i}\\);\nspatial basis functions: \\(X_j(\\mathbf{s}_i;t) = \\phi_{j - 6}(\\mathbf{s}_i), j =7,\\dots,18\\), for all \\(t\\).\n\nThe set of basis functions can be constructed using the function auto_basis in FRK. The function takes as arguments data, which is a spatial object; nres, which is the number of “resolutions” to construct; and type, which indicates the type of basis function to use. Here we consider a single resolution of the Gaussian radial basis function; see Figure 3.13.\n\n\n\n\n\n\nFigure 3.13: The time-invariant basis functions \\(\\phi_1(\\cdot),\\dots,\\phi_{12}(\\cdot)\\) used for regression prediction of the NOAA maximum temperature data in July 1993.\n\n\n\n\nG &lt;- auto_basis(data = Tmax[,c(\"lon\",\"lat\")] %&gt;%  # Take Tmax\n                       SpatialPoints(),           # To sp obj\n                nres = 1,                         # One resolution\n                type = \"Gaussian\")                # Gaussian BFs\n\nThese basis functions evaluated at data locations are then the covariates we seek for fitting the data. The functions are evaluated at any arbitrary location using the function eval_basis. This function requires the locations as a matrix object, and it returns the evaluations as an object of class Matrix, which can be easily converted to a matrix as follows.\n\nS &lt;- eval_basis(basis = G,                     # basis functions\n                s = Tmax[,c(\"lon\",\"lat\")] %&gt;%  # spat locations\n                     as.matrix()) %&gt;%          # conv. to matrix\n     as.matrix()                               # results as matrix\ncolnames(S) &lt;- paste0(\"B\", 1:ncol(S)) # assign column names\n\nWhen fitting the linear model we shall use the convenient notation “.” to denote “all variables in the data frame” as covariates. This is particularly useful when we have many covariates, such as the 12 basis functions above. Therefore, we first remove all variables (except the field id that we shall omit manually later) that we do not wish to include in the model, and we save the resulting data frame as Tmax2.\n\nTmax2 &lt;- cbind(Tmax, S) %&gt;%             # append S to Tmax\n         select(-year, -month, -proc,   # and remove vars we\n                -julian, -date)         # will not be using in\n                                        # the model\n\nAs we did in Lab 3.1, we also remove 14 July 1993 to see how predictions on this day are affected, given that we have no data on that day.\n\nTmax_no_14 &lt;- filter(Tmax2, !(day == 14))  # remove day 14\n\nWe now fit the linear model using lm. The formula we use is z ~ (lon + lat + day)^2 + . which indicates that we have as covariates longitude, latitude, day, and all the interactions between them, as well as the other covariates in the data frame (the 12 basis functions) without interactions.\n\nTmax_July_lm &lt;- lm(z ~ (lon + lat + day)^2 + .,     # model\n                   data = select(Tmax_no_14, -id))  # omit id\n\nThe results of this fit can be viewed using summary. Note that latitude is no longer considered a significant effect, largely because of the presence of the latitude-by-day interaction in the model, which is considered significant. The output from summary corresponds to what is shown in Table Table 3.1.\n\nTmax_July_lm %&gt;% summary()\n\n\nCall:\nlm(formula = z ~ (lon + lat + day)^2 + ., data = select(Tmax_no_14, \n    -id))\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-17.5136  -2.4797   0.1098   2.6644  14.1659 \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 192.243242  97.854126   1.965 0.049531 *  \nlon           1.756918   1.088175   1.615 0.106486    \nlat          -1.317402   2.555626  -0.515 0.606239    \nday          -1.216456   0.133547  -9.109  &lt; 2e-16 ***\nB1           16.646617   4.832399   3.445 0.000577 ***\nB2           18.528159   3.056082   6.063 1.46e-09 ***\nB3           -6.606896   3.171759  -2.083 0.037312 *  \nB4           30.545361   4.369591   6.990 3.20e-12 ***\nB5           14.739147   2.746866   5.366 8.52e-08 ***\nB6          -17.541177   3.423081  -5.124 3.13e-07 ***\nB7           28.472198   3.551900   8.016 1.42e-15 ***\nB8          -27.348145   3.164317  -8.643  &lt; 2e-16 ***\nB9          -10.234777   4.456735  -2.296 0.021701 *  \nB10          10.558234   3.327370   3.173 0.001519 ** \nB11         -22.757661   3.532508  -6.442 1.32e-10 ***\nB12          21.864383   4.812940   4.543 5.72e-06 ***\nlon:lat      -0.026021   0.028232  -0.922 0.356755    \nlon:day      -0.022696   0.001288 -17.615  &lt; 2e-16 ***\nlat:day      -0.019032   0.001876 -10.147  &lt; 2e-16 ***\n---\nSignif. codes:  \n0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 4.225 on 3970 degrees of freedom\nMultiple R-squared:  0.7023,    Adjusted R-squared:  0.701 \nF-statistic: 520.4 on 18 and 3970 DF,  p-value: &lt; 2.2e-16\n\n\n\nCorrelated Errors\nAs we show later in this Lab, there is clearly correlation in the residuals, indicating that the fixed effects are not able to explain the spatio-temporal variability in the data. If we knew the spatio-temporal covariance function of these errors, we could then use generalized least squares to fit the model. For example, if we knew that the covariance function was a Gaussian function, isotropic, and with a range of \\(0.5\\) (see Chapter 4 for more details on covariance functions), then we could fit the model as follows.\n\nTmax_July_gls &lt;- gls(z ~ (lon + lat + day)^2 + .,\n                     data = select(Tmax_no_14, -id),\n                     correlation = corGaus(value = 0.5,\n                                        form = ~ lon + lat + day,\n                                        fixed = TRUE))\n\nResults of the linear fitting can be seen using summary. Note that the estimated coefficients are quite similar to those using linear regression, but the standard errors are larger. The output from summary should correspond to what is shown in Table Table 3.1.\n\n\nStepwise Selection\nStepwise selection is a procedure used to find a parsimonious model (where parsimony refers to a model with as few parameters as possible for a given criterion) from a large selection of explanatory variables, such that each variable is included or excluded in a step. In the simplest of cases, a step is the introduction of a variable (always the case in forward selection) or the removal of a variable (always the case in backward selection).\nThe function step takes as arguments the initial (usually the intercept) model as an lm object, the full model as its scope and, if direction = \"forward\", starts from an intercept model and at each step introduces a new variable that minimizes the Akaike information criterion (AIC) (see Section 6.4.4) of the fitted model. The following for loop retrieves the fitted model for each step of the stepwise AIC forward-selection method.\n\nTmax_July_lm4 &lt;- list()   # initialize\nfor(i in 0:4) {           # for four steps (after intercept model)\n   ## Carry out stepwise forward selection for i steps\n   Tmax_July_lm4[[i+1]] &lt;- step(lm(z ~ 1,\n                               data = select(Tmax_no_14, -id)),\n                               scope = z ~(lon + lat + day)^2 + .,\n                               direction = 'forward',\n                               steps = i)\n}\n\nEach model in the list can be analyzed using summary, as above.\nNotice from the output of summary that Tmax_July_lm4[[5]] contains the covariate lon whose effect is not significant. This is fairly common with stepwise AIC procedures. One is more likely to include covariates whose effects are significant when minimizing the residual sum of squares at each step. This can be carried out using the function regsubsets from the leaps package, which can be called as follows.\n\nregfit.full = regsubsets(z ~ 1 + (lon + lat + day)^2 + .,  # model\n                         data = select(Tmax_no_14, -id),\n                         method = \"forward\",\n                         nvmax = 4)                  # 4 steps\n\nAll information from the stepwise-selection procedure is available in the object returned by the summary function.\n\nregfit.summary &lt;- summary(regfit.full)\n\nYou can type regfit.summary to see which covariates were selected in each step of the algorithm. The outputs from step and regsubsets are shown in Tables Table 3.2 and Table 3.3, respectively.\n\n\nMulticollinearity\nIt is fairly common in spatio-temporal modeling to have multicollinearity, both in space and in time. For example, in a spatial setting, average salary might be highly correlated with unemployment levels, but both could be included in a model to explain life expectancy. It is beyond the scope of this book to discuss methods to deal with multicollinearity, but it is important to be aware of its implications.\nConsider, for example, a setting where we have a 13th basis function that is simply the 5th basis function corrupted by some noise.\n\nset.seed(1) # Fix seed for reproducibility\nTmax_no_14_2 &lt;- Tmax_no_14 %&gt;%\n                mutate(B13 = B5 + 0.01*rnorm(nrow(Tmax_no_14)))\n\nIf we fit the same linear model, but this time we include the 13th basis function, then the effects of both the 5th and the 13th basis functions are no longer considered significant at the 1% level, although the effect of the 5th basis function was considered very significant initially (without the 13th basis function being present).\n\nTmax_July_lm3 &lt;- lm(z ~ (lon + lat + day)^2 + .,\n                   data = Tmax_no_14_2 %&gt;%\n                          select(-id))\n\n\nsummary(Tmax_July_lm3)\n\n\nCall:\nlm(formula = z ~ (lon + lat + day)^2 + ., data = Tmax_no_14_2 %&gt;% \n    select(-id))\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-17.7869  -2.4946   0.1034   2.6743  14.3179 \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 195.365087  97.819550   1.997 0.045872 *  \nlon           1.785474   1.087752   1.641 0.100786    \nlat          -1.414127   2.554836  -0.554 0.579946    \nday          -1.215853   0.133485  -9.109  &lt; 2e-16 ***\nB1           16.675811   4.830184   3.452 0.000561 ***\nB2           18.379495   3.055443   6.015 1.96e-09 ***\nB3           -6.470935   3.170917  -2.041 0.041345 *  \nB4           30.303993   4.368998   6.936 4.68e-12 ***\nB5            0.603294   7.092149   0.085 0.932214    \nB6          -17.322017   3.423000  -5.060 4.37e-07 ***\nB7           28.135550   3.553672   7.917 3.12e-15 ***\nB8          -27.002158   3.166901  -8.526  &lt; 2e-16 ***\nB9          -10.181760   4.454742  -2.286 0.022330 *  \nB10          10.379666   3.326858   3.120 0.001822 ** \nB11         -22.419432   3.534340  -6.343 2.50e-10 ***\nB12          21.664508   4.811603   4.503 6.91e-06 ***\nB13          13.998555   6.475621   2.162 0.030698 *  \nlon:lat      -0.026943   0.028222  -0.955 0.339805    \nlon:day      -0.022625   0.001288 -17.563  &lt; 2e-16 ***\nlat:day      -0.018884   0.001876 -10.066  &lt; 2e-16 ***\n---\nSignif. codes:  \n0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 4.223 on 3969 degrees of freedom\nMultiple R-squared:  0.7027,    Adjusted R-squared:  0.7013 \nF-statistic: 493.7 on 19 and 3969 DF,  p-value: &lt; 2.2e-16\n\n\nThe introduction of the 13th basis function will not adversely affect the predictions and prediction standard errors, but it does compromise our ability to correctly interpret the fixed effects. Multicollinearity will result in a high positive or negative correlation between the estimators of the regression coefficients. For example, the correlation matrix of the estimators of the fixed effects corresponding to these two basis functions is given by\n\nvcov(Tmax_July_lm3)[c(\"B5\", \"B13\"),c(\"B5\", \"B13\")] %&gt;%\n    cov2cor()\n\n            B5        B13\nB5   1.0000000 -0.9220244\nB13 -0.9220244  1.0000000\n\n\n\n\nAnalyzing the Residuals\nHaving fitted a spatio-temporal model, it is good practice to check the residuals. If they are still spatio-temporally correlated, then our model will not have captured adequately the spatial and temporal variability in the data. We extract the residuals from our linear model using the function residuals.\n\nTmax_no_14$residuals &lt;- residuals(Tmax_July_lm)\n\nNow let us plot the residuals of the last eight days. Notice how these residuals, shown in the bottom panel of Figure 3.9, are strongly spatially correlated. The triangles in the image correspond to the two stations whose time series of residuals we shall analyze later.\n\ng &lt;- ggplot(filter(Tmax_no_14, day %in% 24:31)) +\n  geom_point(aes(lon, lat, colour = residuals)) +\n  facet_wrap(~ day, ncol=4) +\n  col_scale(name = \"degF\") +\n  geom_point(data = filter(Tmax_no_14,day %in% 24:31 &\n                                        id %in% c(3810, 3889)),\n               aes(lon, lat), colour = \"black\",\n               pch = 2, size = 2.5) +\n  theme_bw()\n\n\nprint(g)\n\nOne of the most used tests for spatial dependence for lattice spatial data is Moran’s \\(I\\)G test. This can be applied to the data directly, or to the residuals from some spatial regression model. Let \\(Z_i\\) represent spatially referenced data (or residuals) for \\(i=1,\\ldots,n\\) spatial locations. Then Moran’s \\(I\\) is calculated as\n\\[\nI = \\frac{n \\sum_{i=1}^n \\sum_{j=1}^n w_{ij} (Z_i - \\bar{Z})(Z_j - \\bar{Z}) }{(\\sum_{i=1}^n \\sum_{j=1}^n w_{ij})(\\sum_{i=1}^n (Z_i - \\bar{Z})^2) },\n\\tag{3.17}\\]\nwhere \\(\\bar{Z} = (1/n)\\sum_{i=1}^n Z_i\\) is the spatial mean and \\(w_{ij}\\) are spatial adjacency “weights” between location \\(i\\) and location \\(j\\) (where we require \\(w_{ii} = 0\\) for all \\(i=1,\\ldots,n\\)). Thus, Moran’s \\(I\\) statistic is simply a weighted form of the usual Pearson correlation coefficient, where the weights are the spatial proximity weights, and it takes values between \\(-1\\) and \\(1\\). If Equation 3.17 is positive, then neighboring values tend to have similar values, and if it is negative, then neighboring regions tend to have different values. We can use Moran’s \\(I\\) test, described in Note 3.2, to test for spatial dependence in the residuals on each day. In the following code, we take each day in our data set, compute the distances, form the weight matrix, and carry out Moran’s \\(I\\) test using the function Moran.I from the package ape.\n\nP &lt;- list()                                 # init list\ndays &lt;- c(1:13, 15:31)                      # set of days\nfor(i in seq_along(days)) {                 # for each day\n  Tmax_day &lt;- filter(Tmax_no_14,\n                     day == days[i])        # filter by day\n  station.dists &lt;- Tmax_day %&gt;%             # take the data\n    select(lon, lat) %&gt;%                    # extract coords.\n    dist() %&gt;%                              # comp. dists.\n    as.matrix()                             # conv. to matrix\n  station.dists.inv &lt;- 1/station.dists      # weight matrix\n  diag(station.dists.inv) &lt;- 0              # 0 on diag\n  P[[i]] &lt;- Moran.I(Tmax_day$residuals,     # run Moran's I\n                    station.dists.inv) %&gt;%\n            do.call(\"cbind\", .)             # conv. to df\n}\n\nThe object P is a list of single-row data frames that can be collapsed into a single data frame by calling do.call and proceeding to row-bind the elements of each list item together. We print the first six records of the resulting data frame below.\n\ndo.call(\"rbind\", P) %&gt;% head()\n\n      observed     expected         sd p.value\n[1,] 0.2716679 -0.007575758 0.01235583       0\n[2,] 0.2264147 -0.007575758 0.01236886       0\n[3,] 0.2113966 -0.007575758 0.01236374       0\n[4,] 0.1626477 -0.007575758 0.01238872       0\n[5,] 0.2580333 -0.007575758 0.01241006       0\n[6,] 0.1216741 -0.007575758 0.01227211       0\n\n\nThe maximum \\(p\\)-value from the 30 tests is 8.0401863^{-6}, which is very small. Since we are in a multiple-hypothesis setting, we need to control the familywise error rate and, for a level of significance \\(\\alpha\\), reject the null hypothesis of no correlation only if the \\(p\\)-value is less than \\(c(\\alpha)\\) \\((&lt; \\alpha)\\), where \\(c(\\cdot)\\) is a correction function. In this case, even the very conservative Bonferroni correction (for which \\(c(\\alpha) = \\alpha / T\\), where \\(T\\) is the number of time points) will result in rejecting the null hypothesis at each time point.\nIt is straightforward to extend Moran’s \\(I\\) test to the spatio-temporal setting, as one need only extend the concept of “spatial distance” to “spatio-temporal distance.” We are faced with the usual problem of how to appropriately scale time to make a Euclidean distance across space and time have a realistic interpretation. One way to do this is to fit a dependence model that allows for scaling in time, and subsequently scale time by an estimate of the scaling factor prior to computing the Euclidean distance. We shall work with one such model, which uses an anisotropic covariance function, in Chapter 4. For now, as we did with IDW, we do not scale time and compute distances on the spatio-temporal domain (which happens to be reasonable for these data).\n\nstation.dists &lt;- Tmax_no_14 %&gt;%  # take the data\n  select(lon, lat, day) %&gt;%      # extract coordinates\n  dist() %&gt;%                     # compute distances\n  as.matrix()                    # convert to matrix\n\nWe now need to compute the weights from the distances, set the diagonal to zero and call Moran.I.\n\nstation.dists.inv &lt;- 1/station.dists\ndiag(station.dists.inv) &lt;- 0\nMoran.I(Tmax_no_14$residuals, station.dists.inv)$p.value\n\n[1] 0\n\n\nUnsurprisingly, given what we saw when analyzing individual time slices, the \\(p\\)-value is very small, strongly suggesting that there is spatio-temporal dependence in the data.\nWhen the data are regularly spaced in time, as is the case here, one may also look at the “temporal” residuals at some location and test for temporal correlation in these residuals using the Durbin–Watson test. For example, consider the maximum temperature (Tmax) residuals at stations 3810 and 3889.\n\nTS1 &lt;- filter(Tmax_no_14, id == 3810)$residuals\nTS2 &lt;- filter(Tmax_no_14, id == 3889)$residuals\n\nThese residuals can be easily plotted using base R graphics as follows; see the top panel of Figure 3.9.\n\npar(mar=c(4, 4, 1, 1))\nplot(TS1,                           # Station 3810 residuals\n     xlab = \"day of July 1993\",\n     ylab = \"residuals (degF)\",\n     type = 'o', ylim = c(-8, 7))\nlines(TS2,                          # Station 3889 residuals\n      xlab = \"day of July 1993\",\n      ylab = \"residuals (degF)\",\n      type = 'o', col = \"red\")\n\nNote that there is clear temporal correlation in the residuals; that is, residuals close to each other in time tend to be more similar than residuals further apart. It is also interesting to note that the residuals are correlated between the stations; that is, at the same time point, the residuals at both stations are more similar than at different time points. This is due to the spatial correlation in the residuals that was tested for above (these two stations happen to be quite close to each other; recall the previous image of the spatial residuals). One may also look at the correlogram (the empirical autocorrelation function) of the residuals by typing acf(TS1) and acf(TS2), respectively. From these plots it can be clearly seen that there is significant lag-1 correlation in both these residual time series.\nNow let us proceed with carrying out a Durbin–Watson test for the residuals at every station. This can be done using a for loop as we did with Moran’s \\(I\\) test; however, we shall now introduce a more sophisticated way of carrying out multiple tests and predictions on groups of data within a data frame, using the packages tidyr, purrr, and broom, which will also be used in Lab 3.3.\nIn Lab 2.1 we investigated data wrangling techniques for putting data that are in a data frame into groups using group_by, and then we performed an operation on each of those groups using summarise. The grouped data frame returned by group_by is simply the original frame with each row associated with a group. A more elaborate representation of these data is in a nested data frame, where we have a data frame containing one row for each group. The “nested” property comes from the fact that we may have a data frame, conventionally under the field name “data,” for each group. For example, if we group Tmax_no_14 by lon and lat, we obtain the following first three records.\n\nnested_Tmax_no_14 &lt;- group_by(Tmax_no_14, lon, lat) %&gt;% nest()\nhead(nested_Tmax_no_14, 3)\n\n# A tibble: 3 × 3\n# Groups:   lon, lat [3]\n    lat   lon data              \n  &lt;dbl&gt; &lt;dbl&gt; &lt;list&gt;            \n1  39.3 -81.4 &lt;tibble [30 × 16]&gt;\n2  35.7 -81.4 &lt;tibble [30 × 16]&gt;\n3  35.6 -88.9 &lt;tibble [30 × 16]&gt;\n\n\nNote the third column, data, which is a column of tibbles (which, for the purposes of this book, should be treated as sophisticated data frames). Next we define a function that takes the data frame associated with a single group, carries out the test (in this case the Durbin–Watson test), and returns the results. The function dwtest takes an R formula as the first argument and the data as the second argument. In this case, we test for autocorrelation in the residuals after removing a temporal (constant) trend and by using the formula residuals ~ 1.\n\ndwtest_one_station &lt;- function(data)\n                        dwtest(residuals ~ 1, data = data)\n\nCalling dwtest_one_station for the data in the first record will carry out the test at the first station, in the second record at the second station, and so on. For example,\n\ndwtest_one_station(nested_Tmax_no_14$data[[1]])\n\ncarries out the Durbin–Watson test on the residuals at the first station.\nTo carry out the test on each record in the nested data frame, we use the function map from the package purrr. For example, the command\n\nmap(nested_Tmax_no_14$data, dwtest_one_station) %&gt;% head()\n\nThe test results for the first six stations can be assigned to another column within our nested data frame using mutate. These results are of class htest and not easy to analyze or visualize in their native form. We therefore use the function tidy from the package broom to extract the key information from the test (in this case the statistic, the \\(p\\)-value, the method, and the hypothesis) and put it into a data frame. For example,\n\ndwtest_one_station_tidy &lt;- nested_Tmax_no_14$data[[1]] %&gt;%\n                           dwtest_one_station() %&gt;%\n                           tidy()\n\ntidies up the results at the first station. The first three columns of the returned data are\n\ndwtest_one_station_tidy[, 1:3]\n\n# A tibble: 1 × 3\n  statistic p.value method            \n      &lt;dbl&gt;   &lt;dbl&gt; &lt;chr&gt;             \n1     0.982 0.00122 Durbin-Watson test\n\n\nTo assign the test results to each record in the nested data frame as added fields (instead of as another data frame), we then use the function unnest. In summary, the code\n\nTmax_DW_no_14 &lt;- nested_Tmax_no_14 %&gt;%\n    mutate(dwtest = map(data, dwtest_one_station)) %&gt;%\n    mutate(test_df = map(dwtest, tidy)) %&gt;%\n    unnest(test_df)\n\nprovides all the information we need. The first three records, excluding the last two columns, are\n\nTmax_DW_no_14 %&gt;% select(-method, -alternative) %&gt;% head(3)\n\n# A tibble: 3 × 6\n# Groups:   lon, lat [3]\n    lat   lon data     dwtest  statistic  p.value\n  &lt;dbl&gt; &lt;dbl&gt; &lt;list&gt;   &lt;list&gt;      &lt;dbl&gt;    &lt;dbl&gt;\n1  39.3 -81.4 &lt;tibble&gt; &lt;htest&gt;     0.982 0.00122 \n2  35.7 -81.4 &lt;tibble&gt; &lt;htest&gt;     0.921 0.000586\n3  35.6 -88.9 &lt;tibble&gt; &lt;htest&gt;     1.59  0.129   \n\n\nThe proportion of \\(p\\)-values below the 5% level of significance divided by the number of tests (Bonferroni correction) is\n\nmean(Tmax_DW_no_14$p.value &lt; 0.05/nrow(Tmax_DW_no_14)) * 100\n\n[1] 21.8\n\n\nThis proportion of 21.8% is reasonably high and provides evidence that there is considerable temporal autocorrelation in the residuals, as expected.\nFinally, we can also compute and visualize the empirical spatio-temporal semivariogram of the residuals. Recall that in Lab 2.1 we put the maximum temperature data in the NOAA data set into an STFDF object that we labeled STObj3. We now load these data and subset the month of July 1993.\n\ndata(\"STObj3\", package = \"STRbook\")\nSTObj4 &lt;- STObj3[, \"1993-07-01::1993-07-31\"]\n\nAll we need to do is merge Tmax_no_14, which contains the residuals, with the STFDF object STObj4, so that the empirical semivariogram of the residuals can be computed. This can be done quickly, and safely, using the function left_join.\n\nSTObj4@data &lt;- left_join(STObj4@data, Tmax_no_14)\n\nAs in Lab 2.3, we now compute the empirical semivariogram with the function variogram.\n\nvv &lt;- variogram(object = residuals ~ 1, # fixed effect component\n                data = STObj4,     # July data\n                width = 80,        # spatial bin (80 km)\n                cutoff = 1000,     # consider pts &lt; 1000 km apart\n                tlags = 0.01:6.01) # 0 days to 6 days\n\nThe command plot(vv) displays the empirical semivariogram of the residuals, which is shown in Figure 3.8. This empirical semivariogram is clearly different from that of the data Figure 2.17 and has a lower sill, but it suggests that there is still spatial and temporal correlation in the residuals.\n\n\n\nPredictions\nPrediction from linear or generalized linear models in R is carried out using the function predict. As in Lab 3.1, we use the following prediction grid.\n\npred_grid &lt;- expand.grid(lon = seq(-100, -80, length = 20),\n                         lat = seq(32, 46, length = 20),\n                         day = seq(4, 29, length = 6))\n\nWe require all the covariate values at all the prediction locations. Hence, the 12 basis functions need to be evaluated on this grid. As above, we do this by calling eval_basis and converting the result to a matrix, which we then attach to our prediction grid.\n\nSpred &lt;- eval_basis(basis = G,                      # basis functs\n                s = pred_grid[,c(\"lon\",\"lat\")] %&gt;%  # pred locs\n                     as.matrix()) %&gt;%         # conv. to matrix\n     as.matrix()                              # results as matrix\ncolnames(Spred) &lt;- paste0(\"B\", 1:ncol(Spred)) # assign col names\npred_grid &lt;- cbind(pred_grid, Spred)          # attach to grid\n\nNow that we have all the covariates in place, we can call predict. We supply predict with the model Tmax_July_lm, the prediction grid, and the argument interval = \"prediction\", so that predict returns the prediction intervals.\n\nlinreg_pred &lt;- predict(Tmax_July_lm,\n                       newdata = pred_grid,\n                       interval = \"prediction\")\n\nWhen predict is called as above, it returns a matrix containing three columns with names fit, lwr, and upr, which contain the prediction and the lower and upper bounds of the 95% prediction interval, respectively. Since in this case the prediction interval is the prediction \\(\\pm\\) 1.96 times the prediction standard error, we can calculate the prediction standard error from the given interval as follows.\n\n## Assign prediction and prediction s.e. to the prediction grid\npred_grid$z_pred &lt;- linreg_pred[,1]\npred_grid$z_err &lt;- (linreg_pred[,3] - linreg_pred[,2]) / (2*1.96)\n\nPlotting the prediction and prediction standard error proceeds in a straightforward fashion using ggplot2; see Figure 3.7. This is left as an exercise for the reader.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Spatio-Temporal Statistical Models</span>"
    ]
  },
  {
    "objectID": "Chapter3.html#lab-3.3-regression-models-for-forecasting",
    "href": "Chapter3.html#lab-3.3-regression-models-for-forecasting",
    "title": "3  Spatio-Temporal Statistical Models",
    "section": "Lab 3.3: Regression Models for Forecasting",
    "text": "Lab 3.3: Regression Models for Forecasting\nIn this Lab we fit a simple linear model to every pixel in the SST data set, and we use these models to predict SST for a month in which we have no SST data. The models will simply contain an intercept and a single covariate, namely the Southern Oscillation Index (SOI). The SOI data we use here are supplied with STRbook and were retrieved from https://www.esrl.noaa.gov/psd/gcos_wgsp/Timeseries/SOI/.\nFor this Lab we need the usual data-wrangling and plotting packages, as well as the packages broom and purrr for fitting and predicting with multiple models simultaneously.\n\nlibrary(\"broom\")\nlibrary(\"dplyr\")\nlibrary(\"ggplot2\")\nlibrary(\"STRbook\")\nlibrary(\"purrr\")\nlibrary(\"tidyr\")\n\nIn the first section of this Lab we tidy up the data to obtain the SST data frame from the raw data. You may also skip this section by loading SST_df from STRbook, and fast-forwarding to the section that is concerned with fitting the data.\n\ndata(\"SST_df\", package = \"STRbook\")\n\n\nTidying Up the Data\nThe first task in this Lab is to wrangle the SST data into a long-format data frame that is amenable to linear fitting and plotting. Recall from Lab 2.3 that the SST data are provided in three data frames, one describing the land mask, one containing the SST values in wide format, and one containing the coordinates.\n\ndata(\"SSTlandmask\", package = \"STRbook\")\ndata(\"SSTdata\", package = \"STRbook\")\ndata(\"SSTlonlat\", package = \"STRbook\")\n\nWe first combine the land mask data with the coordinates data frame.\n\nlonlatmask_df &lt;- data.frame(cbind(SSTlonlat, SSTlandmask))\nnames(lonlatmask_df) &lt;- c(\"lon\", \"lat\", \"mask\")\n\nThen we form our SST data frame in wide format by attaching SSTdata to the coordinate-mask data frame.\n\nSSTdata &lt;- cbind(lonlatmask_df, SSTdata)\n\nFinally, we use gather to put the data frame into long format.\n\nSST_df &lt;- gather(SSTdata, date, sst, -lon, -lat, -mask)\n\nOur data frame now contains the SST data, but the date field contains as entries V1, V2, …, which were the names of the columns in SSTdata.\n\nSST_df %&gt;% head(3)\n\n  lon lat mask date      sst\n1 124 -29    1   V1 -0.36289\n2 126 -29    1   V1 -0.28461\n3 128 -29    1   V1 -0.19195\n\n\nWe replace this date field with two fields, one containing the month and one containing the year. We can do this by first creating a mapping table that links V1 to January 1970, V2 to February 1970, and so on, and then merging using left_join.\n\ndate_grid &lt;- expand.grid(Month = c(\"Jan\", \"Feb\", \"Mar\", \"Apr\",\n                                   \"May\", \"Jun\", \"Jul\", \"Aug\",\n                                   \"Sep\", \"Oct\", \"Nov\", \"Dec\"),\n                         Year = 1970:2002,\n                         stringsAsFactors =  FALSE)\ndate_grid$date &lt;- paste0(\"V\", 1:396)\nSST_df &lt;- left_join(SST_df, date_grid) %&gt;%\n          select(-date)\n\nFor good measure, we also add in the date field again but this time in month–year format.\n\nSST_df$date &lt;- paste(SST_df$Month, SST_df$Year)\nSST_df %&gt;% head(3)\n\n  lon lat mask      sst Month Year     date\n1 124 -29    1 -0.36289   Jan 1970 Jan 1970\n2 126 -29    1 -0.28461   Jan 1970 Jan 1970\n3 128 -29    1 -0.19195   Jan 1970 Jan 1970\n\n\nNext, we set SST data that are coincident with land locations to NA:\n\nSST_df$sst &lt;- ifelse(SST_df$mask == 0, SST_df$sst, NA)\n\nOur SST data frame is now in place. The following code plots a series of SSTs leading up to the 1997 El Niño event; see Figure 3.10.\n\ng &lt;- ggplot(filter(SST_df, Year == 1997 &  # subset by month/year\n                      Month %in% c(\"Apr\",\"Aug\",\"Jun\",\"Oct\"))) +\n    geom_tile(aes(lon, lat,\n                  fill = pmin(sst, 4))) +  # clamp SST at 4deg\n    facet_wrap(~date, dir = \"v\") +         # facet by date\n    fill_scale(limits = c(-4, 4),          # color limits\n               name = \"degC\") +            # legend title\n    theme_bw() + coord_fixed()             # fix scale and theme\n\nNow we need to add the SOI data to the SST data frame. The SOI time series is available as a 14-column data frame, with the first column containing the year, the next 12 columns containing the SOI for each month in the respective year, and the last column containing the mean SOI for that year. In the following, we remove the annual average from the data frame, which is in wide format, and then put it into long format using gather.\n\ndata(\"SOI\", package = \"STRbook\")\nSOI_df &lt;- select(SOI, -Ann) %&gt;%\n          gather(Month, soi, -Year)\n\nFinally, we use left_join to merge the SOI data and the SST data.\n\nSST_df &lt;- left_join(SST_df, SOI_df,\n                    by = c(\"Month\", \"Year\"))\n\n\nFitting the Models Pixelwise\nIn this section we fit linear time-series models to the SSTs in each pixel using data up to April 1997. We first create a data frame containing the SST data between January 1970 and April 1997.\n\nSST_pre_May &lt;- filter(SST_df, Year &lt;= 1997) %&gt;%\n               filter(!(Year == 1997 &\n                        Month %in% c(\"May\", \"Jun\", \"Jul\",\n                                     \"Aug\", \"Sep\", \"Oct\",\n                                     \"Nov\", \"Dec\")))\n\nNext, as in Lab 3.2, we use purrr and broom to construct a nested data frame that contains a linear model fitted to every pixel. We name the function that fits the linear model at a single pixel to the data over time as fit_one_pixel.\n\nfit_one_pixel &lt;- function(data)\n                 mod &lt;- lm(sst ~ 1 + soi, data = data)\n\npixel_lms &lt;- SST_pre_May %&gt;%\n             filter(!is.na(sst)) %&gt;%\n             group_by(lon, lat) %&gt;%\n             nest() %&gt;%\n             mutate(model = map(data, fit_one_pixel)) %&gt;%\n             mutate(model_df = map(model, tidy))\n\nThe string of commands above describes an operation that is practically identical to what we did in Lab 3.2. We take the data, filter them to remove missing data, group by pixel, create a nested data frame, fit a model to each pixel, and extract a data frame containing information on the linear fit by pixel. The first three records of the nested data frame are as follows.\n\npixel_lms %&gt;% head(3)\n\n# A tibble: 3 × 5\n# Groups:   lon, lat [3]\n    lon   lat data               model  model_df\n  &lt;dbl&gt; &lt;dbl&gt; &lt;list&gt;             &lt;list&gt; &lt;list&gt;  \n1   154   -29 &lt;tibble [328 × 6]&gt; &lt;lm&gt;   &lt;tibble&gt;\n2   156   -29 &lt;tibble [328 × 6]&gt; &lt;lm&gt;   &lt;tibble&gt;\n3   158   -29 &lt;tibble [328 × 6]&gt; &lt;lm&gt;   &lt;tibble&gt;\n\n\nTo extract the model parameters from the linear-fit data frames, we use unnest:\n\nlm_pars &lt;- pixel_lms %&gt;%\n           unnest(model_df)\n\nFor each pixel, we now have an estimate of the intercept and the effect associated with the covariate soi, as well as other information such as the \\(p\\)-values.\n\nhead(lm_pars, 3)\n\n# A tibble: 3 × 9\n# Groups:   lon, lat [2]\n    lon   lat data               model  term        estimate\n  &lt;dbl&gt; &lt;dbl&gt; &lt;list&gt;             &lt;list&gt; &lt;chr&gt;          &lt;dbl&gt;\n1   154   -29 &lt;tibble [328 × 6]&gt; &lt;lm&gt;   (Intercept)   0.132 \n2   154   -29 &lt;tibble [328 × 6]&gt; &lt;lm&gt;   soi           0.0277\n3   156   -29 &lt;tibble [328 × 6]&gt; &lt;lm&gt;   (Intercept)   0.0365\n  std.error statistic    p.value\n      &lt;dbl&gt;     &lt;dbl&gt;      &lt;dbl&gt;\n1    0.0266      4.96 0.00000113\n2    0.0223      1.24 0.216     \n3    0.0262      1.40 0.164     \n\n\nWe can plot spatial maps of the intercept and the regression coefficient associated with soi directly. We first merge this data frame with the coordinates data frame using left_join, which also contains land pixels. In this way, regression coefficients over land pixels are marked as NA, which is appropriate.\n\nlm_pars &lt;- left_join(lonlatmask_df, lm_pars)\n\nThe following code plots the spatial intercept and the spatial regression coefficient associated with soi; see the top panels of Figure 3.11.\n\ng2 &lt;- ggplot(filter(lm_pars, term == \"(Intercept)\" | mask == 1)) +\n    geom_tile(aes(lon, lat, fill = estimate)) +\n    fill_scale() +\n    theme_bw() + coord_fixed()\n\ng3 &lt;- ggplot(filter(lm_pars, term == \"soi\" | mask == 1)) +\n    geom_tile(aes(lon, lat, fill = estimate)) +\n    fill_scale() +\n    theme_bw() + coord_fixed()\n\n\n\n\nPredicting SST Pixelwise\nWe now use the linear models at the pixel level to predict the SST in October 1997 using the SOI index for that month. The SOI for that month is extracted from SOI_df as follows.\n\nsoi_pred &lt;- filter(SOI_df, Month == \"Oct\" & Year == \"1997\") %&gt;%\n            select(soi)\n\nWe next define the function that carries out the prediction at the pixel level. The function takes a linear model lm and the SOI at the prediction date soi_pred, runs the predict function for this date, and returns a data frame containing the prediction and the prediction standard error.\n\npredict_one_pixel &lt;- function(lm, soi_pred) {\n    predict(lm,                           # linear model\n            newdata = soi_pred,           # pred. covariates\n            interval = \"prediction\") %&gt;%  # output intervals\n    data.frame() %&gt;%                      # convert to df\n    mutate(se = (upr-lwr)/(2 * 1.96)) %&gt;% # comp pred. se\n    select(fit, se)                       # return fit & se\n  }\n\nPrediction proceeds at each pixel by calling predict_one_pixel on each row in our nested data frame pixel_lms.\n\nSST_Oct_1997 &lt;- pixel_lms %&gt;%\n                mutate(preds = map(model,\n                                   predict_one_pixel,\n                                   soi_pred = soi_pred)) %&gt;%\n                unnest(preds)\n\nWe have unnested the preds data frame above to save the fit and prediction standard error as fields in the SST_Oct_1997 data frame. You can type SST_Oct_1997 %&gt;% head(3) to have a look at the first three records. It is straightforward to plot the prediction and prediction standard error from SST_Oct_1997; see the middle and bottom panels of Figure 3.11. This is left as an exercise for the reader.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Spatio-Temporal Statistical Models</span>"
    ]
  },
  {
    "objectID": "Chapter3.html#lab-3.4-generalized-linear-spatio-temporal-regression",
    "href": "Chapter3.html#lab-3.4-generalized-linear-spatio-temporal-regression",
    "title": "3  Spatio-Temporal Statistical Models",
    "section": "Lab 3.4: Generalized Linear Spatio-Temporal Regression",
    "text": "Lab 3.4: Generalized Linear Spatio-Temporal Regression\nIn this Lab we fit a generalized linear spatio-temporal model to yearly counts of Carolina wren in and around the state of Missouri between 1994 and 2014. These counts are part of the BBS data set. We need gstat, sp, and spacetime for fitting an empirical semivariogram to the residuals, FRK to construct the basis functions (as in Lab 3.2), ape for running Moran’s \\(I\\) test, and the usual packages for wrangling and plotting.\n\nlibrary(\"ape\")\nlibrary(\"dplyr\")\nlibrary(\"FRK\")\nlibrary(\"ggplot2\")\nlibrary(\"gstat\")\nlibrary(\"sp\")\nlibrary(\"spacetime\")\nlibrary(\"STRbook\")\nlibrary(\"tidyr\")\n\n\nFitting the Model\nThe Carolina wren counts in the BBS data set, in both wide and long format, are supplied with STRbook. Here we load the data directly in long format and remove any records that contain missing observations.\n\ndata(\"MOcarolinawren_long\", package = \"STRbook\")\nMOcarolinawren_long &lt;- MOcarolinawren_long %&gt;%\n                       filter(!is.na(cnt))\n\nWe use the same covariates to fit these data as we did to fit the maximum temperature, Tmax, in Lab 3.2. Twelve of these covariates were basis functions constructed using auto_basis from the package FRK; see Lab 3.2 for details. The matrix S below then contains the basis functions evaluated at the Carolina wren observation locations.\n\nG &lt;- auto_basis(data = MOcarolinawren_long[,c(\"lon\",\"lat\")] %&gt;%\n                       SpatialPoints(),           # To sp obj\n                nres = 1,                         # One resolution\n                type = \"Gaussian\")                # Gaussian BFs\n\nS &lt;- eval_basis(basis = G,                       # basis functions\n                s = MOcarolinawren_long[,c(\"lon\",\"lat\")] %&gt;%\n                     as.matrix()) %&gt;%            # conv. to matrix\n     as.matrix()                                 # conv. to matrix\ncolnames(S) &lt;- paste0(\"B\", 1:ncol(S)) # assign column names\n\nNext, we attach the basis-function covariate information to the data frame containing the counts, and remove the fields loc.ID and t, which we will not explicitly use when fitting the model. We list the first five columns of the first three records of our constructed data frame Wren_df as follows.\n\nWren_df &lt;- cbind(MOcarolinawren_long,S) %&gt;%\n  select(-loc.ID, -t)\nWren_df[1:3, 1:5]\n\n  cnt  lat   lon year      B1\n1   4 36.8 -89.2 1994 0.00258\n2   2 36.6 -90.7 1994 0.03551\n3   8 36.9 -91.7 1994 0.11588\n\n\nGeneralized linear models (GLMs) are fitted in R using the function glm. The function works similarly to lm, but in addition it requires one to specify the exponential-family model that is used (in this first instance we consider the Poisson family), as well as the link function (here we use the log function, which is the canonical link). The glm function is called as follows (note that we have used the same formula as in Lab 3.2).\n\nWren_GLM &lt;- glm(cnt ~ (lon + lat + year)^2 + ., # formula\n                family = poisson(\"log\"),     # Poisson + log link\n                data = Wren_df)              # data set\n\nThe mean and variance of a random variable that has a Poisson distribution are the same. In cases where the variance in the data is greater than that suggested by this model, the data are said to exhibit “over-dispersion.” An estimate of the dispersion is given by the ratio of the deviance to the total degrees of freedom (the number of data points minus the number of covariates). In this case the dispersion estimate is\n\nWren_GLM$deviance / Wren_GLM$df.residual\n\n[1] 3.78\n\n\nwhich is greater than 1, a sign of over-dispersion.\nAnother way to obtain an estimate of the dispersion parameter (and, to account for it if present) is to replace poisson with quasipoisson when calling glm, and then type summary(Wren_GLM). The quasi-Poisson model assumes that the variance is proportional to the mean, and that the constant of the proportionality is the over-dispersion parameter. Note from the output of summary that the dispersion parameter is 3.9, which is close to what we estimated above.\nIt can be shown that under the null hypothesis of no over-dispersion, the deviance is approximately chi-squared distributed with degrees of freedom equal to \\(m - p - 1\\).\n\nWren_GLM$df.residual\n\n[1] 764\n\n\nThe observed deviance is\n\nWren_GLM$deviance\n\n[1] 2890\n\n\nThe probability of observing such a large or larger deviance under the null hypothesis of no over-dispersion (i.e., the \\(p\\)-value) is\n\n1 - pchisq(q = Wren_GLM$deviance, df = Wren_GLM$df.residual)\n\n[1] 0\n\n\nTherefore, we reject the null hypothesis of no over-dispersion at the usual levels of significance (10%, 5%, and 1%). One may use other models in the exponential family, such as the negative-binomial distribution, to account explicitly for the over-dispersion. For convenience, in this Lab we proceed with the Poisson family. We use the negative-binomial distribution in Lab 4.4.\n\nPrediction\nAs in the other Labs, prediction proceeds through use of the function predict. We first generate our space-time prediction grid, which is an 80 \\(\\times\\) 80 \\(\\times\\) 21 grid in degrees \\(\\times\\) degrees \\(\\times\\) years, covering the observations in space and in time.\n\npred_grid &lt;- expand.grid(lon = seq(\n                             min(MOcarolinawren_long$lon) - 0.2,\n                             max(MOcarolinawren_long$lon) + 0.2,\n                             length.out = 80),\n                         lat = seq(\n                             min(MOcarolinawren_long$lat) - 0.2,\n                             max(MOcarolinawren_long$lat) + 0.2,\n                             length.out = 80),\n                         year = 1994:2014)\n\nAs in Lab 3.2, we now evaluate the basis functions at the prediction locations.\n\nS_pred &lt;- eval_basis(basis = G,                    # basis functs\n                s = pred_grid[,c(\"lon\",\"lat\")] %&gt;% # pred locs\n                     as.matrix()) %&gt;%            # conv. to matrix\n     as.matrix()                                 # as matrix\ncolnames(S_pred) &lt;- paste0(\"B\", 1:ncol(S_pred))  # assign  names\npred_grid &lt;- cbind(pred_grid,S_pred)             # attach to grid\n\nIn the call to predict below, we specify type = \"link\" to indicate that we predict the link function of the response and not the response (analogous to the log-intensity of the process).\n\nwren_preds &lt;- predict(Wren_GLM,\n                      newdata = pred_grid,\n                      type = \"link\",\n                      se.fit = TRUE)\n\nThe predictions and prediction standard errors of the link function of the response are then attached to our prediction grid for plotting; see Figure 3.12. Plotting to obtain Figure 3.12 is left as an exercise for the reader.\n\npred_grid &lt;- pred_grid %&gt;%\n             mutate(log_cnt = wren_preds$fit,\n                    se = wren_preds$se.fit)\n\nWhen fitting GLMs, it is good practice to check the deviance residuals and inspect them for any residual correlation. The default GLM residuals returned by residuals are deviance residuals.\n\nWren_df$residuals &lt;- residuals(Wren_GLM)\n\nInterestingly, the plot of the deviance residuals in Figure 3.14 is “noisy,” indicating a lack of spatial correlation.\n\ng2 &lt;- ggplot(Wren_df) +\n    geom_point(aes(lon, lat, colour = residuals)) +\n    col_scale(name = \"residuals\") +\n    facet_wrap(~year, nrow = 3) + theme_bw()\n\n\n\n\n\n\n\nFigure 3.14: The deviance residuals from the fitted GLM between \\(t=1\\) (the year 1994) and \\(t=21\\) (2014).\n\n\n\nWe can test for spatial correlation of the deviance residuals by running Moran’s \\(I\\) test on the spatial deviance residuals for each year. The code below follows closely that for Moran’s \\(I\\) test in Lab 3.2 and then summarizes the \\(p\\)-values obtained for each year.\n\nP &lt;- list()                                 # init list\nyears &lt;- 1994:2014\nfor(i in seq_along(years)) {                # for each day\n  Wren_year &lt;- filter(Wren_df,\n                     year == years[i])      # filter by year\n  obs_dists &lt;- Wren_year %&gt;%                # take the data\n    select(lon,lat) %&gt;%                     # extract coords.\n    dist() %&gt;%                              # comp. dists.\n    as.matrix()                             # conv. to matrix\n  obs_dists.inv &lt;- 1/obs_dists              # weight matrix\n  diag(obs_dists.inv) &lt;- 0                  # 0 on diag\n  P[[i]] &lt;- Moran.I(Wren_year$residuals,    # run Moran's I\n                    obs_dists.inv) %&gt;%\n            do.call(\"cbind\", .)             # conv. to df\n}\ndo.call(\"rbind\",P) %&gt;% summary(digits = 2)\n\n    observed         expected            sd       \n Min.   :-0.084   Min.   :-0.040   Min.   :0.025  \n 1st Qu.:-0.059   1st Qu.:-0.029   1st Qu.:0.028  \n Median :-0.044   Median :-0.029   Median :0.030  \n Mean   :-0.041   Mean   :-0.028   Mean   :0.031  \n 3rd Qu.:-0.022   3rd Qu.:-0.025   3rd Qu.:0.033  \n Max.   : 0.010   Max.   :-0.023   Max.   :0.041  \n    p.value    \n Min.   :0.06  \n 1st Qu.:0.24  \n Median :0.42  \n Mean   :0.47  \n 3rd Qu.:0.68  \n Max.   :0.94  \n\n\nHence, at the 5% level of significance, the null hypothesis (of no spatial correlation in these deviance residuals) is not rejected. This was expected from the visualization in Figure 3.14.\nMore insight can be obtained by looking at the empirical semivariogram of the deviance residuals. To do this we first construct an STIDF, thereby casting the irregular space-time data into a spacetime object.\n\nWren_STIDF &lt;- STIDF(sp = SpatialPoints(\n                            Wren_df[,c(\"lon\",\"lat\")],\n                            proj4string = CRS(\"+proj=longlat\")),\n                    time = as.Date(Wren_df[, \"year\"] %&gt;%\n                                       as.character(),\n                                   format = \"%Y\"),\n                    data = Wren_df)\n\nThen we compute the empirical semivariogram using variogram. We consider time bins of width 1 year (i.e., of width 52.1429 weeks). Bins specified in units of weeks are required, as this is the largest temporal unit recognized by variogram.\n\ntlags &lt;- seq(0.01, 52.1429*6 + 0.01, by = 52.1429)\nvv &lt;- variogram(object = residuals ~ 1, # fixed effect component\n                data = Wren_STIDF,      # data set\n                tlags = tlags,          # temp. bins\n                width = 25,             # spatial bin (25 km)\n                cutoff = 150,           # use pts &lt; 150 km apart\n                tunit = \"weeks\")        # time unit\n\nThe empirical semivariogram can be plotted using plot(vv). Notice how there is little evidence of spatial correlation but ample evidence of temporal correlation in the residuals. (The variance of the differences over a large range of time lags at the same spatial location is small.) This is a clear sign that a more sophisticated spatio-temporal random-effects model should be considered for these data.\n\n\n\n\nBox, G. E. P. (1976). Science and statistics. Journal of the American Statistical Association, 71(356), 791–799.\n\n\nBox, G. E. P. (1979). Robustness in the strategy of scientific model building. In R. L. Launer & G. L. Wilkinson (Eds.), Robustness in statistics (pp. 201–236). Academic Press.\n\n\nCressie, N. (1993). Statistics for spatial data (revised). John Wiley & Sons.\n\n\nCressie, N., & Wikle, C. K. (2011). Statistics for spatio-temporal data. John Wiley & Sons.\n\n\nHastie, T., Tibshirani, R., & Friedman, J. (2009). The elements of statistical learning (2nd ed.). Springer.\n\n\nHenebry, G. M. (1995). Spatial model error analysis using autocorrelation indices. Ecological Modelling, 82(1), 75–91.\n\n\nJames, G., Witten, D., Hastie, T., & Tibshirani, R. (2013). An introduction to statistical learning. Springer.\n\n\nKutner, H., M, Nachtsheim, C. J., & Neter, J. (2004). Applied multiple regression models. McGraw-Hill.\n\n\nMcCullagh, P., & Nelder, J. A. (1989). Generalized linear models. Cambridge University Press.\n\n\nMcCulloch, C. E., & Searle, S. R. (2001). Generalized, linear, and mixed models. John Wiley & Sons.\n\n\nShumway, R. H., & Stoffer, D. S. (2006). Time series analysis and its applications with r examples (2nd ed.). Springer.\n\n\nWaller, L. A., & Gotway, C. A. (2004). Applied spatial statistics for public health data. John Wiley & Sons.\n\n\nWood, S. N. (2017). Generalized additive models: An introduction with r (2nd ed.). Chapman & Hall/CRC.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Spatio-Temporal Statistical Models</span>"
    ]
  },
  {
    "objectID": "Chapter4.html",
    "href": "Chapter4.html",
    "title": "4  Descriptive Spatio-Temporal Statistical Models",
    "section": "",
    "text": "4.1 Additive Measurement Error and Process Models\nChapter 3 is the linchpin for the “two Ds” of spatio-temporal statistical modeling, which are now upon us in this chapter (the first “D,” namely “descriptive”) and the next chapter (the second “D,” namely “dynamic”). We hope to have eased you from the free form of spatio-temporal exploratory data analysis presented in Chapter 2 into the “rigor” needed to build a coherent statistical model. The independent probability structure assumed in Chapter 3 was a place-holder for the sorts of probability structures that respect Tobler’s law, discussed in the previous chapters: in our context, this says that a set of values at nearby spatio-temporal locations should not be assumed independent. As we shall see, there is a “descriptive” way (this chapter, Chapter 4) and a “dynamic” way (Chapter 5) to incorporate spatio-temporal statistical dependence into models.\nIn this chapter we focus on two of the goals of spatio-temporal modeling given in Chapter 3: prediction at some location in space within the time span of the observations and, to a lesser extent, parameter inference for spatio-temporal covariates. For both goals we assume that our observations can be decomposed into a true (latent) spatio-temporal process plus observation error. We then assume that the true process can be written in terms of spatio-temporal fixed effects due to covariates plus a spatio-temporally dependent random process. We call this a descriptive approach because its main concern is to specify (or describe) the dependence structure in the random process. This is in contrast to the dynamic approach presented in Chapter 5 that models the evolution of the dependent random process through time. To implement the prediction and inference approaches discussed herein we must perform estimation. We mention the most popular and relevant estimation approaches and algorithms as they come up, but omit most of the details. The interested reader can explore these details in the references given in Section 4.6. Finally, we note that these discussions require a bit more statistical formality and mathematical notation, and so the presentations in this and the next chapter are at a higher technical level than those in Chapter 3.\nIn this section we describe more formally a two-stage model that considers additive measurement error in a data (observation) model, and a process model that is decomposed into a fixed- (covariate-) effect term and a random-process term. This general decomposition is the basis for the models that we present in this and the next chapter.\nRecall that at each time \\(t \\in \\{t_1,\\ldots,t_T\\}\\) we have \\(m_{t_j}\\) observations. With a slight abuse of notation, we write the number of observations at time \\(t_j\\) as \\(m_j\\). The vector of all observations is then given by\n\\[\n\\mathbf{Z}= \\left(Z(\\mathbf{s}_{11};t_1), Z(\\mathbf{s}_{21};t_1),\\ldots, Z(\\mathbf{s}_{m_11};t_1),\\ldots, Z(\\mathbf{s}_{1T};t_T),\\ldots,Z(\\mathbf{s}_{m_TT};t_T)\\right)'.\n\\]\nThat is, different numbers of irregular spatial observations are allowed for each time (note that if there are no observations at a given time, \\(t_j\\), the set of spatial locations is empty for that time and \\(m_j = 0\\)). We seek a prediction at some spatio-temporal location \\((\\mathbf{s}_0;t_0)\\). As described in Chapter 1, if \\(t_0 &lt; t_T\\), so that we have all data available to us, then we are in a smoothing situation; if we only have data up to time \\(t_0\\) then we are in a filtering situation; and if \\(t_0 &gt; t_T\\) then we are in a forecasting situation. We seek statistically optimal predictions for an underlying latent (i.e., hidden) random spatio-temporal process. We denote this process by \\(\\{Y(\\mathbf{s};t): \\mathbf{s}\\in D_s,\\ t \\in D_t\\}\\), for spatial location \\(\\mathbf{s}\\) in spatial domain \\(D_s\\) (a subset of \\(d\\)-dimensional Euclidean space), and time index \\(t\\) in temporal domain \\(D_t\\) (along the one-dimensional real line).\nMore specifically, suppose we represent the data in terms of the latent spatio-temporal process of interest plus a measurement error. For example,\n\\[\nZ(\\mathbf{s}_{ij};t_{j}) = Y(\\mathbf{s}_{ij};t_j) + \\epsilon(\\mathbf{s}_{ij};t_{j}), \\quad i=1,\\ldots,m_j;\\ j=1,\\ldots,T,\n\\tag{4.1}\\]\nwhere the errors \\(\\{\\epsilon(\\mathbf{s}_{ij};t_j)\\}\\) represent iid mean-zero measurement error that is independent of \\(Y(\\cdot;\\cdot)\\) and has variance \\(\\sigma^2_\\epsilon\\). So, in the simple data model Equation 4.1 we assume that the data are noisy observations of the latent process \\(Y\\) at a finite collection of locations in the space-time domain, where typically we have not observed data at all locations of interest. Consequently, we would like to predict the latent value \\(Y(\\mathbf{s}_0;t_0)\\) at a spatio-temporal location \\((\\mathbf{s}_{0};t_0)\\) as a function of the data vector represented by \\(\\mathbf{Z}\\) (or some subset of these observations), which is of dimension \\(\\sum_{j=1}^T m_j\\). To simplify the notation that follows, we shall sometimes assume that data were observed at the same set of \\(m\\) locations for each of the \\(T\\) times, in which case \\(\\mathbf{Z}\\) is of length \\(m T\\).\nNow suppose that the latent process follows the model\n\\[\nY(\\mathbf{s};t) = \\mu(\\mathbf{s};t) + \\eta(\\mathbf{s};t),\n\\tag{4.2}\\]\nfor all \\((\\mathbf{s};t)\\) in our space-time domain of interest (e.g., \\(D_s \\times D_t\\)), where each component in Equation 4.2 has a special role to play. In Equation 4.2, \\(\\mu(\\mathbf{s};t)\\) represents the process mean, which is not random, and \\(\\eta(\\mathbf{s};t)\\) represents a mean-zero random process with spatial and temporal statistical dependence. Our goal here is to find the optimal linear predictor in the sense that it minimizes the mean squared prediction error between \\(Y(\\mathbf{s}_0;t_0)\\) and our prediction, which we write as \\(\\widehat{Y}(\\mathbf{s}_0;t_0).\\) Depending on the problem at hand, we may choose to let \\(\\mu(\\mathbf{s};t)\\) be: (i) known, (ii) constant but unknown, or (iii) modeled in terms of \\(p\\) covariates, \\(\\mu(\\mathbf{s};t) = \\mathbf{x}(\\mathbf{s};t)'\\boldsymbol{\\beta}\\), where the \\(p\\)-dimensional vector of parameters \\(\\boldsymbol{\\beta}\\) is unknown. In the context of the descriptive methods considered in this chapter, these choices result in spatio-temporal (S-T) (i) simple, (ii) ordinary, and (iii) universal kriging, respectively. Note that in this and subsequent chapters, the covariate vector \\(\\mathbf{x}(\\mathbf{s};t)\\) could include the variable “1,” which models an intercept in the multivariable regression.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Descriptive Spatio-Temporal Statistical Models</span>"
    ]
  },
  {
    "objectID": "Chapter4.html#sec-GaussDataProc",
    "href": "Chapter4.html#sec-GaussDataProc",
    "title": "4  Descriptive Spatio-Temporal Statistical Models",
    "section": "4.2 Prediction for Gaussian Data and Processes",
    "text": "4.2 Prediction for Gaussian Data and Processes\nRecall from Chapter 3 that when we interpolate with spatio-temporal data we specify that the value of the process at some location is simply a weighted combination of nearby observations. We described a couple of deterministic methods to obtain such weights (inverse distance weighting and kernel smoothing). Here we are concerned with determining the statistically “optimal” weights in this linear combination. At this point, it is worth taking a step back and looking at the big picture.\nIn the case of predicting statistically within the domain of our space-time observation locations (smoothing), we are just interpolating our observations \\(\\mathbf{Z}\\) to the location \\((\\mathbf{s}_0;t_0)\\) in a way that respects that we have observational uncertainty. For example, in the special case where \\((\\mathbf{s}_0;t_0)\\) corresponds to an observation location, we are simply smoothing out this observation uncertainty. Unlike the deterministic approaches to spatio-temporal prediction in Chapter 3, we seek the weights in a linear predictor that minimize the interpolation error on average. This optimization criterion is \\(E(Y(\\mathbf{s}_0;t_0) - \\widehat{Y}(\\mathbf{s}_0;t_0))^2\\), the mean square prediction error (MSPE). The best linear unbiased predictor that minimizes the MSPE is referred to as the kriging predictor. As we shall see, the kriging weights are determined by the statistical dependence (i.e., covariances) between observation locations (roughly, the greater the covariability, the greater the weight), yet respect the measurement uncertainty.\nThere are several different approaches to deriving the form of the optimal linear predictor, which we henceforth call S-T . Given that we are just focusing on the first two moments in the descriptive approach (i.e., the means, variances, and covariances of \\(Y(\\cdot;\\cdot)\\)), it is convenient to assume that the underlying process is a Gaussian process and the has a Gaussian distribution. We take this approach in this book.\nWhat is a Gaussian process? Consider a stochastic process denoted by \\(\\{Y(\\mathbf{r}): \\mathbf{r}\\in D\\}\\), where \\(\\mathbf{r}\\) is a spatial, temporal, or spatio-temporal location in \\(D\\), a subset of \\(d\\)-dimensional space. This process is said to be a Gaussian process, often denoted \\(Y(\\mathbf{r}) \\sim GP(\\mu(\\mathbf{r}), c(\\cdot;\\cdot))\\), if the process has all its finite-dimensional distributions Gaussian, determined by a mean function \\(\\mu(\\mathbf{r})\\) and a covariance function \\(c(\\mathbf{r},\\mathbf{r}') = \\textrm{cov}(Y(\\mathbf{r}),Y(\\mathbf{r}'))\\) for any location \\(\\{\\mathbf{r},\\mathbf{r}'\\} \\in D\\). (Note that in spatio-temporal statistics it is common to use \\(Gau(\\cdot,\\cdot)\\) instead of \\(GP(\\cdot,\\cdot)\\), and we follow that convention in this book.) There are two important points to make about the Gaussian process. First, because the Gaussian process determines a probability distribution over functions, it exists everywhere in the domain of interest \\(D\\); so, if the mean and covariance functions are known, the process can be described anywhere in the domain. Second, only finite distributions need to be considered in practice because of the fundamental property that any finite collection of Gaussian process random variables \\(\\{Y(\\mathbf{r}_i)\\}\\) has a joint multivariate normal (Gaussian) distribution. This allows the use of traditional machinery of multivariate normal distributions when performing prediction and inference. Gaussian processes are fundamental to the theoretical and practical foundation of spatial and spatio-temporal statistics and, since the first decade of the twenty-first century, have become increasingly important and popular modeling tools in the machine-learning community (e.g., Rasmussen & Williams, 2006).\nIn the context of S-T , time is implicitly treated as another dimension, and we consider covariance functions that describe covariability between any two space-time locations (where in general we should use covariance functions that respect that durations in time are different from distances in space). We can write the data model in terms of vectors,\n\\[\n\\mathbf{Z}= \\mathbf{Y}+ \\boldsymbol{\\varepsilon},\n\\tag{4.3}\\]\nwhere \\(\\mathbf{Y}\\equiv (Y(\\mathbf{s}_{11};t_1),\\ldots,Y(\\mathbf{s}_{m_TT};t_T))'\\) and \\(\\boldsymbol{\\varepsilon}\\equiv (\\epsilon(\\mathbf{s}_{11};t_1),\\ldots,\\epsilon(\\mathbf{s}_{m_TT};t_T))'\\). Similarly, the vector form of the process model for \\(\\mathbf{Y}\\) is written\n\\[\n\\mathbf{Y}= \\boldsymbol{\\mu}+ \\boldsymbol{\\eta},\n\\tag{4.4}\\]\nwhere \\(\\boldsymbol{\\mu}\\equiv (\\mu(\\mathbf{s}_{11};t_1),\\ldots,\\mu(\\mathbf{s}_{m_TT};t_T))' = \\mathbf{X}\\boldsymbol{\\beta}\\), and \\(\\boldsymbol{\\eta}\\equiv (\\eta(\\mathbf{s}_{11};t_1),\\ldots,\\eta(\\mathbf{s}_{m_TT};t_T))'\\). Note that \\(\\textrm{cov}({\\mathbf{Y}}) \\equiv \\mathbf{C}_y = \\mathbf{C}_\\eta\\), \\(\\textrm{cov}({\\boldsymbol{\\varepsilon}}) \\equiv \\mathbf{C}_{\\epsilon}\\), and \\(\\textrm{cov}({\\mathbf{Z}}) \\equiv \\mathbf{C}_z = \\mathbf{C}_y + \\mathbf{C}_{\\epsilon}\\).\nNow, defining \\(\\mathbf{c}_0' \\equiv \\textrm{cov}(Y(\\mathbf{s}_0;t_0),{\\mathbf{Z}})\\), \\(c_{0,0} \\equiv \\textrm{var}(Y(\\mathbf{s}_0;t_0))\\), and \\(\\mathbf{X}\\) the \\((\\sum_{j=1}^T m_j) \\times p\\) matrix given by \\(\\mathbf{X}\\equiv [\\mathbf{x}(\\mathbf{s}_{ij};t_j)': i=1,\\ldots,m_j;\\ j=1,\\ldots,T]\\), consider the joint Gaussian distribution,\n\\[\n\\left[\\begin{array}{c} Y(\\mathbf{s}_0;t_0) \\\\ \\mathbf{Z}\\end{array}\\right] \\; \\sim \\; Gau\\left(\\left[\\begin{array}{c} \\mathbf{x}(\\mathbf{s}_0;t_0)' \\\\ \\mathbf{X}\\end{array}\\right] \\boldsymbol{\\beta}\\; , \\; \\left[\\begin{array}{cc} c_{0,0} & \\mathbf{c}_0' \\\\ \\mathbf{c}_0 & \\mathbf{C}_z \\end{array}\\right] \\right).\n\\]\nUsing well-known results for conditional distributions from a joint multivariate normal (Gaussian) distribution (e.g., Johnson & Wichern, 1992), and assuming (for the moment) that \\(\\boldsymbol{\\beta}\\) is known (recall that this is called S-T simple kriging), one can obtain the conditional distribution,\n\\[\nY(\\mathbf{s}_0;t_0) \\; \\mid  \\mathbf{Z}\\; \\sim \\; Gau(\\mathbf{x}(\\mathbf{s}_0;t_0)'\\boldsymbol{\\beta}+ \\mathbf{c}_0' \\mathbf{C}_z^{-1} (\\mathbf{Z}- \\mathbf{X}\\boldsymbol{\\beta})\\; , \\; c_{0,0} - \\mathbf{c}_0' \\mathbf{C}_z^{-1} \\mathbf{c}_0),\n\\tag{4.5}\\]\nfor which the mean is the S-T simple kriging predictor,\n\\[\n\\widehat{Y}(\\mathbf{s}_0;t_0) = \\mathbf{x}(\\mathbf{s}_0;t_0)'\\boldsymbol{\\beta}+ \\mathbf{c}_0' \\mathbf{C}_z^{-1} (\\mathbf{Z}- \\mathbf{X}\\boldsymbol{\\beta}),\n\\tag{4.6}\\]\nand the variance is the S-T simple kriging variance,\n\\[\n\\sigma^2_{Y,sk}(\\mathbf{s}_0;t_0) = c_{0,0} - \\mathbf{c}_0' \\mathbf{C}_z^{-1} \\mathbf{c}_0.\n\\tag{4.7}\\]\nNote that we call \\(\\sigma_{Y,sk}(\\mathbf{s}_0;t_0)\\) the S-T simple kriging prediction standard error, and it has the same units as \\(\\widehat{Y}(\\mathbf{s}_0;t_0)\\).\nIt is fundamentally important in kriging that one be able to specify the covariance between the process at any two locations in the domain of interest (i.e., \\(\\mathbf{c}_0\\)). That is, we assume that the process is defined for an uncountable set of locations and the data correspond to a partial realization of this process. As mentioned above, this is the benefit of considering S-T kriging from the Gaussian-process perspective. That is, if we assume we have a Gaussian process, then we can specify a valid finite-dimensional Gaussian distribution for any finite subset of locations.\nAnother important observation to make here is that Equation 4.6 is a predictor of the hidden value, \\(Y(\\mathbf{s}_0;t_0)\\), not of \\(Z(\\mathbf{s}_0;t_0)\\). The form of the conditional distribution given by Equation 4.5 helps clarify the intuition behind S-T kriging. In particular, note that the conditional mean takes the residuals between the observations and their marginal means (i.e., \\(\\mathbf{Z}- \\mathbf{X}\\boldsymbol{\\beta}\\)), weights them according to \\(\\mathbf{w}' \\equiv \\mathbf{c}_0' \\mathbf{C}_z^{-1}\\), and adds the result back onto the marginal mean corresponding to the prediction location (i.e., \\(\\mathbf{x}(\\mathbf{s}_0;t_0)'\\boldsymbol{\\beta}\\)). Furthermore, the weights, \\(\\mathbf{w}\\), are only a function of the covariances and the measurement-error variance. Another way to think of this is that the trend term \\(\\mathbf{x}(\\mathbf{s}_0;t_0)' \\boldsymbol{\\beta}\\) is the mean of \\(Y(\\mathbf{s}_0;t_0)\\) prior to considering the observations; then the simple S-T kriging predictor combines this prior mean with a weighted average of the mean-corrected observations to get a new, conditional, mean. Similarly, if one interprets \\(c_{0,0}\\) as the variance prior to considering the observations, then the conditional (on the data) variance reduces this initial variance by an amount given by \\(\\mathbf{c}_0' \\mathbf{C}_z^{-1} \\mathbf{c}_0\\). Consider the following numerical example.\n\nExample: Simple S-T Kriging\nSuppose we have four observations in a one-dimensional space and a one-dimensional time domain: \\(Z(2;0.2) = 15\\), \\(Z(2;1.0) = 22\\), \\(Z(6;0.2) = 17\\), and \\(Z(6;0.9) = 23\\). We seek an S-T simple kriging prediction for \\(Y(s_0;t_0) = Y(3;0.5)\\). The data locations and prediction location are shown in Figure 4.1. Let \\(x(s;t) = 1\\) for all \\(s\\) and \\(t\\), \\(\\beta = 20\\), and \\(\\textrm{var}(Y(s;t)) = 2\\) for all \\(s\\) and \\(t\\). Using the spatio-temporal covariance function Equation 4.13 discussed below (with parameters \\(a=2\\), \\(b=0.2\\), \\(\\sigma^2 = c_{0,0} = 2.0\\), and \\(d=1\\)), the covariance (between data) matrix \\(\\mathbf{C}_z\\), the covariance (between the data and the latent \\(Y(\\cdot;\\cdot)\\) at the prediction location) vector \\(\\mathbf{c}_0\\), and the weights \\(\\mathbf{w}' = \\mathbf{c}_0' \\mathbf{C}_z^{-1}\\) are given by\n\\[\n\\mathbf{C}_z = \\left[\\begin{array}{rrrr}\n    2.0000  &  1.0600   & 1.0546  &  0.9364 \\\\\n    1.0600  &  2.0000  &  0.8856  &  1.0599 \\\\\n    1.0546  &  0.8856  &  2.0000  &  1.1625 \\\\\n    0.9364  &  1.0599 &   1.1625  &  2.0000\n\\end{array} \\right],  \\;\\;\n\\mathbf{c}_0 = \\left[\\begin{array}{r} 1.6653 \\\\   1.3862 \\\\  1.3161  \\\\  1.2539 \\end{array}\n\\right], \\;\\;\n\\mathbf{w}= \\left[\\begin{array}{r} 0.5377  \\\\  0.2565 \\\\   0.1841 \\\\   0.1323\n\\end{array}\n\\right].\n\\]\nSubstituting these matrices, vectors, and the data vector, \\(\\mathbf{Z}= (15,22,17,23)'\\), into the formulas for the S-T kriging predictor Equation 4.6 and prediction variance Equation 4.7, we obtain\n\\[\n\\begin{aligned}\n  \\widehat{Y}(3;0.5) &= 17.67, \\\\\n  \\widehat{\\sigma}^2_{Y,sk} &= 0.34.\n\\end{aligned}\n\\]\nNote that the S-T simple kriging prediction \\((17.67)\\) is substantially smaller than the prior mean \\((20)\\), mainly because the highest weights are associated with the earlier times, which have smaller values. In addition, the S-T simple kriging prediction variance (\\(0.34\\)) is much less than the prior variance \\((2)\\), as expected when there is strong spatio-temporal dependence.\n\n\n\n\n\n\nFigure 4.1: Data locations (blue dots) and prediction location (red dot) in a (one-dimensional) space-time domain for an example of S-T simple kriging. The data values and the S-T simple kriging prediction are given next to the locations. The S-T simple kriging weights associated with each data location are given in parentheses next to the dashed lines connecting the data locations to the prediction location.\n\n\n\nIn most real-world problems, one would not know \\(\\boldsymbol{\\beta}\\). In this case, our optimal prediction problem is analogous to the estimation of effects in a linear mixed model, that is, in a model that considers the response in terms of both fixed effects (e.g., regression terms) and random effects, \\(\\boldsymbol{\\eta}\\). It is straightforward to show that the optimal linear unbiased predictor, or S-T universal kriging predictor of \\(Y(\\mathbf{s}_0;t_0)\\) is\n\\[\n\\widehat{Y}(\\mathbf{s}_0;t_0) = \\mathbf{x}(\\mathbf{s}_0;t_0)' \\widehat{\\boldsymbol{\\beta}}_{\\mathrm{gls}} + \\mathbf{c}_0' \\mathbf{C}_z^{-1} (\\mathbf{Z}- \\mathbf{X}\\widehat{\\boldsymbol{\\beta}}_{\\mathrm{gls}}),\n\\tag{4.8}\\]\nwhere the generalized least squares (gls) estimator of \\(\\boldsymbol{\\beta}\\) is given by\n\\[\n\\widehat{\\boldsymbol{\\beta}}_{\\mathrm{gls}} \\equiv (\\mathbf{X}' \\mathbf{C}_z^{-1} \\mathbf{X})^{-1} \\mathbf{X}' \\mathbf{C}_z^{-1} \\mathbf{Z}.\n\\tag{4.9}\\]\nThe associated S-T universal kriging variance is given by\n\\[\n\\sigma^2_{Y,\\mathrm{uk}}(\\mathbf{s}_0;t_0) = c_{0,0} - \\mathbf{c}_0' \\mathbf{C}_z^{-1} \\mathbf{c}_0 + \\kappa,\n\\tag{4.10}\\]\nwhere\n\\[\n\\kappa \\equiv (\\mathbf{x}(\\mathbf{s}_0;t_0) - \\mathbf{X}' \\mathbf{C}_z^{-1} \\mathbf{c}_0)' (\\mathbf{X}' \\mathbf{C}_z^{-1} \\mathbf{X})^{-1} (\\mathbf{x}(\\mathbf{s}_0;t_0) - \\mathbf{X}' \\mathbf{C}_z^{-1} \\mathbf{c}_0)\n\\]\nrepresents the additional uncertainty brought to the prediction (relative to S-T simple kriging) due to the estimation of \\(\\boldsymbol{\\beta}\\). We call \\(\\sigma_{Y,\\mathrm{uk}}(\\mathbf{s}_0;t_0)\\) the S-T universal kriging prediction standard error.\nBoth the S-T simple and universal kriging equations can be extended easily to accommodate prediction at many locations in space and time, including those at which we have observations. For example, in Figure 4.2, we show predictions of maximum temperature from data in the NOAA data set in July 1993 on a space-time grid (using a separable spatio-temporal covariance function, defined in Section 4.2.1), with 14 July deliberately omitted from the data set. The respective prediction standard errors are shown in Figure 4.2, where those for 14 July are substantially larger. We produce these figures in Lab 4.1.\nFor readers who have some experience with spatial statistics, particularly geostatistics, the development given above in the spatio-temporal context will look very familiar. S-T simple, ordinary, and universal are the same as their spatial counterparts, but now in space and time.\nSo far, we have assumed that we know the variances and covariances that make up \\(\\mathbf{C}_y\\), \\(\\mathbf{C}_\\epsilon\\) (recall that \\(\\mathbf{C}_z = \\mathbf{C}_y + \\mathbf{C}_\\epsilon\\)), \\(\\mathbf{c}_0\\), and \\(c_{0,0}\\). Of course, in reality we would rarely (if ever) know these. The seemingly simple solution is to parameterize them, say in terms of parameters \\(\\boldsymbol{\\theta}\\), and then estimate them through maximum likelihood, restricted maximum likelihood (see Note 4.2) as in the classical linear mixed model, or perhaps through a fully implementation, in which case one specifies prior distributions for the elements of \\(\\boldsymbol{\\theta}\\) (see Section 4.2.3). As in spatial statistics, the parameterization of these covariance functions is one of the most challenging problems in spatio-temporal statistics.\n\n\n\n\n\n\nFigure 4.2: Left: S-T universal predictions and right: prediction standard errors of maximum temperature (in degrees Fahrenheit) within a square lat-lon box enclosing the domain of interest for six days (each 5 days apart) in July 1993 using the R package gstat. Data for 14 July 1993 were omitted from the original data set.\n\n\n\n\n\n4.2.1 Spatio-Temporal Covariance Functions\nWe saw in the previous section that S-T predictors require that we know \\(\\mathbf{C}_z\\) and \\(\\mathbf{c}_0\\), and hence we need to know the spatio-temporal covariances between the hidden random process evaluated at any two locations in space and time. It is important to note that not any function can be used as a covariance function. Let a general spatio-temporal covariance function be denoted by\n\\[\nc_*(\\mathbf{s},\\mathbf{s}';t,t') \\equiv \\textrm{cov}(Y(\\mathbf{s};t),Y(\\mathbf{s}';t')),\n\\tag{4.11}\\]\nwhich is appropriate only if the function is valid (i.e., non-negative-definite, which guarantees that the variances are non-negative). (Note that in Equation 4.11 the primes are not transposes, but are used to denote different spatio-temporal locations.)\nIn practice, classical- implementations assume second-order stationarity: the random process is said to be second-order (or weakly) stationary if it has a constant expectation \\(\\mu\\) (say) and a covariance function that can be expressed in terms of spatial and temporal lags:\n\\[\nc_*(\\mathbf{s},\\mathbf{s}';t,t') = c(\\mathbf{s}' - \\mathbf{s}; t' - t) = c(\\mathbf{h}; \\tau),\n\\]\nwhere \\(\\mathbf{h}\\equiv \\mathbf{s}' - \\mathbf{s}\\) and \\(\\tau \\equiv t - t'\\) are the spatial and temporal lags, respectively. Recall from Chapter 2 that if the dependence on spatial lag is only a function of \\(||\\mathbf{h}||\\), we say there is spatial . Arguably, the two biggest benefits of the second-order stationarity assumption are that it allows for more parsimonious parameterizations of the covariance function, and that it provides pseudo-replication of dependencies at given lags in space and time, both of which facilitate estimation of the covariance function’s parameters. \\((\\)In practice, it is unlikely that the spatio-temporal stationary covariance function is completely known and it is usually specified in terms of some parameters \\(\\boldsymbol{\\theta}.)\\)\nThe next question is how to obtain valid stationary (or non-stationary) spatio-temporal covariance functions. Mathematically speaking, how do we ensure that the functions we choose are non-negative-definite?\n\nSeparable (in Space and Time) Covariance Functions\nSeparable classes of spatio-temporal covariance functions have often been used in spatio-temporal modeling because they offer a convenient way to guarantee validity. The class is given by\n\\[\nc(\\mathbf{h};\\tau) \\equiv c^{(s)}(\\mathbf{h}) \\cdot c^{(t)}(\\tau),\n\\]\nwhich is valid if both the spatial covariance function, \\(c^{(s)}(\\mathbf{h})\\), and the temporal covariance function, \\(c^{(t)}(\\tau)\\), are valid. There are a large number of classes of valid spatial and valid temporal covariance functions in the literature (e.g., the Matérn, power exponential, and Gaussian classes, to name a few). For example, the exponential covariance function (which is a special case of both the Matérn covariance function and the power exponential covariance function) is given by\n\\[\nc^{(s)}(\\mathbf{h}) = \\sigma_s^2 \\exp\\left\\{- \\frac{||\\mathbf{h}||}{a_s}  \\right\\},\n\\]\nwhere \\(\\sigma_s^2\\) is the variance parameter and \\(a_s\\) is the spatial-dependence (or scale) parameter in units of distance. The larger \\(a_s\\) is, the more dependent the spatial process is. Similarly, \\(c^{(t)}(\\tau) = \\sigma_t^2 \\exp\\{-|\\tau|/a_t\\}\\) is a valid temporal covariance function (see Figure 4.3 for an example).\n\n\n\n\n\n\nFigure 4.3: Exponential covariance function for time lag \\(\\tau\\), \\(\\sigma_t^2 = 2.5\\), and \\(a_t = 2\\).\n\n\n\nA consequence of separability is that the resulting spatio-temporal correlation function, \\(\\rho(\\mathbf{h};\\tau) \\equiv c(\\mathbf{h};\\tau)/c({\\mathbf{0}};0)\\), is given by\n\\[\n\\rho(\\mathbf{h};\\tau) = \\rho^{(s)}(\\mathbf{h};0) \\cdot \\rho^{(t)}({\\mathbf{0}};\\tau),\n\\]\nwhere \\(\\rho^{(s)}(\\mathbf{h};0)\\) and \\(\\rho^{(t)}({\\mathbf{0}};\\tau)\\) are the corresponding marginal spatial and temporal correlation functions, respectively. Thus, one only needs the marginal spatial and temporal correlation functions to obtain the joint spatio-temporal correlation function under separability. In addition, models facilitate computation. Notice (e.g., from Equation 4.6 and Equation 4.7) that the inverse \\(\\mathbf{C}_z^{-1}\\) is ubiquitous in S-T equations. Separability can allow one to consider the inverse of the spatial and temporal components separately. For example, assume that \\(Z(\\mathbf{s}_{ij};t_j)\\) is observed at the same \\(i=1,\\ldots,m_j = m\\) locations at each time point, \\(j=1,\\ldots,T\\). In this case, one can write \\(\\mathbf{C}_z = \\mathbf{C}_z^{(t)} \\otimes \\mathbf{C}_z^{(s)}\\), where \\(\\otimes\\) is the (see Note 4.1), \\(\\mathbf{C}_z^{(t)}\\) is the \\(T \\times T\\) temporal covariance matrix, and \\(\\mathbf{C}_z^{(s)}\\) is the \\(m \\times m\\) spatial covariance matrix. Taking advantage of a useful property of s (see Note 4.1), \\(\\mathbf{C}_z^{-1} = (\\mathbf{C}_z^{(t)})^{-1} \\otimes (\\mathbf{C}_z^{(s)})^{-1}\\), which shows that to take the inverse of the \\(m T \\times m T\\) matrix \\(\\mathbf{C}_z\\), one only has to take the inverses of \\(T \\times T\\) and \\(m \\times m\\) matrices.\nConsider the maximum-temperature observations (Tmax) from the NOAA data set presented in Chapter 2. After removing the obvious linear trend in latitude, we consider the empirical isotropic spatio-temporal covariance function (discussed in Section 2.4.2) calculated for the residuals, shown in Figure 4.4 (top left panel), and we compare that to the empirical model in Figure 4.4 (top right panel). That is, we are simply considering the product of \\(\\hat{c}(0;|\\tau|)\\) and \\(\\hat{c}(\\|\\mathbf{h}\\|;0)\\). Note that these two plots are remarkably similar, giving visual support for a model in this case. We shall discuss the lower two panels of this figure in Section 4.2.3. See Crujeiras et al. (2010) and references therein for formal tests of separability.\n\n\n\n\n\n\nFigure 4.4: Contour plot of the empirical covariance function (top left), fitted separable covariance function obtained by taking the product of \\(\\hat{c}^{(s)}(\\|\\mathbf{h}\\|)\\) and \\(\\hat{c}^{(t)}(|\\tau|)\\) (top right), fitted separable covariance function using the spatio-temporal separable model given in Equation 4.18 and Equation 4.19 (bottom left) and fitted covariance function using the non-separable model given in Equation 4.20 (bottom right).\n\n\n\nA consequence of the separability property is that the temporal evolution of the process at a given spatial location does not depend directly on the process’ temporal evolution at other locations. As we discuss in Chapter 5, this is very seldom the case for real-world processes as it implies no interaction across space and time. The question then becomes, “how can we obtain other classes of spatio-temporal covariance functions?” Several approaches that have been developed in the literature: (i) sums-and-products formulation; (ii) construction by a spectral representation through Bochner’s theorem (which formally relates the spectral representation to the covariance representation; e.g., the inverse Fourier transform is a special case); and (iii) covariance functions from the solution of stochastic partial differential equations (SPDEs). We discuss these briefly below.\n\n\n\n\n\n\nNote 4.1: Kronecker Products\n\n\n\nConsider two matrices, an \\(n_a \\times m_a\\) matrix, \\(\\mathbf{A}\\), and an \\(n_b \\times m_b\\) matrix, \\(\\mathbf{B}\\). The Kronecker product is given by the \\(n_a n_b \\times m_a m_b\\) matrix \\(\\mathbf{A}\\otimes \\mathbf{B}\\) defined as \\[\n\\mathbf{A}\\otimes \\mathbf{B}= \\left[\\begin{array}{ccc}\na_{11} \\mathbf{B}& \\cdots & a_{1 m_a} \\mathbf{B}\\\\\n\\vdots & \\vdots & \\vdots \\\\\na_{n_a 1} \\mathbf{B}& \\cdots & a_{n_a m_a} \\mathbf{B}\n\\end{array}\\right].\n\\] The Kronecker product has some nice properties that facilitate matrix representations. For example, if \\(\\mathbf{A}\\) is \\(n_a \\times n_a\\) and \\(\\mathbf{B}\\) is \\(n_b \\times n_b\\), the inverse and determinants can be expressed in terms of the individual matrices: \\[\\begin{align*}\n(\\mathbf{A}\\otimes \\mathbf{B})^{-1} &= \\mathbf{A}^{-1} \\otimes \\mathbf{B}^{-1},\\\\\n|\\mathbf{A}\\otimes \\mathbf{B}| &= |\\mathbf{A}|^{n_b} \\; |\\mathbf{B}|^{n_a}.\n\\end{align*}\\]\nIn the context of spatio-temporal processes, Kronecker products are useful in at least two ways. First, they provide a convenient way to represent spatio-temporal covariance matrices for processes. That is, consider \\(\\{Y(\\mathbf{s}_i;t_j): i=1,\\ldots,m;\\ j=1,\\ldots,T\\}\\) and define \\(\\mathbf{C}_y^{(s)}\\) to be the \\(m \\times m\\) matrix of purely spatial covariances and \\(\\mathbf{C}_y^{(t)}\\) to be the \\(T \\times T\\) matrix of purely temporal covariances. Then the \\(mT \\times mT\\) spatio-temporal covariance matrix can be written as, \\(\\mathbf{C}_y = \\mathbf{C}_y^{(t)} \\otimes \\mathbf{C}_y^{(s)}\\) if the process is . Although this may not be realistic for many processes, it is advantageous because of the inverse property, \\(\\mathbf{C}_y^{-1} = (\\mathbf{C}_y^{(t)})^{-1} \\otimes (\\mathbf{C}_y^{(s)})^{-1}\\); see Section 4.2.1.\nThe second way that s are useful for spatio-temporal modeling is for forming spatio-temporal , which we discuss in Section 4.4. In particular, if we construct an \\(m \\times n_{\\alpha,s}\\) matrix \\(\\boldsymbol{\\Phi}\\) by evaluating \\(n_{\\alpha,s}\\) spatial basis functions at \\(m\\) spatial locations, and a \\(T \\times n_{\\alpha,t}\\) matrix \\(\\boldsymbol{\\Psi}\\) by evaluating \\(n_{\\alpha,t}\\) temporal basis functions at \\(T\\) temporal locations, then the matrix constructed from spatio-temporal basis functions formed through the tensor product of the spatial and temporal basis functions and evaluated at all combinations of spatial and temporal locations is given by the \\(m T \\times n_{\\alpha,s} n_{\\alpha,t}\\) matrix \\(\\mathbf{B}= \\boldsymbol{\\Psi}\\otimes \\boldsymbol{\\Phi}\\). Basis functions can be used to construct spatio-temporal covariance functions. Note that using a set of basis functions constructed through the yields a class of spatio-temporal covariance functions that are in general not separable.\n\n\n\n\nSums-and-Products Formulation\nThere is a useful result in mathematics that states that, as well as the product, the sum of two non-negative-definite functions is non-negative-definite. This allows us to construct valid spatio-temporal covariance functions as the product and/or sum of valid covariance functions. For example,\n\\[\nc(\\mathbf{h};\\tau) \\equiv p \\; c_1^{(s)}(\\mathbf{h}) \\cdot c_1^{(t)}(\\tau) + q \\; c_2^{(s)}(\\mathbf{h}) + r  \\; c_2^{(t)}(\\tau)\n\\tag{4.12}\\]\nis a valid spatio-temporal covariance function when \\(p &gt; 0\\), \\(q \\ge 0\\), \\(r \\ge 0\\); \\(c_1^{(s)}(\\mathbf{h})\\) and \\(c_2^{(s)}(\\mathbf{h})\\) are valid spatial covariance functions; and \\(c_1^{(t)}(\\tau)\\) and \\(c_2^{(t)}(\\tau)\\) are valid temporal covariance functions. Of course, Equation 4.12 can be extended to include the sum of many terms and the result is non-negative definite if each component covariance function is non-negative-definite.\nThe sums-and-products formulation above points to connections between separable covariance functions and other special cases. For example, consider the fully symmetric spatio-temporal covariance functions: a spatio-temporal random process \\(\\{Y(\\mathbf{s};t)\\}\\) is said to have a fully symmetric spatio-temporal covariance function if, for all spatial locations \\(\\mathbf{s}, \\mathbf{s}'\\) in the spatial domain of interest and time points \\(t, t'\\) in the temporal domain of interest, we can write\n\\[\n\\textrm{cov}(Y(\\mathbf{s};t),Y(\\mathbf{s}';t')) = \\textrm{cov}(Y(\\mathbf{s};t'),Y(\\mathbf{s}';t)).\n\\]\nUsing such covariances to model spatio-temporal dependence is not always reasonable for real-world processes. For example, is it reasonable that the covariance between yesterday’s temperature in London and today’s temperature in Paris is the same as that between yesterday’s temperature in Paris and today’s temperature in London? Such a relationship might be appropriate under certain meteorological conditions, but not in general (imagine a weather system moving from northwest to southeast across Europe). So, for scientific reasons or as a result of an exploratory data analysis, the fully symmetric covariance function may not be an appropriate choice.\nNow, note that the covariance given by Equation 4.12 is an example of a fully symmetric covariance, but it is only separable if \\(q=r=0\\). In general, separable covariance functions are always fully symmetric, while the converse is not true.\n\n\nConstruction via a Spectral Representation\nAn important example of the construction approach to spatio-temporal covariance function development was given by Cressie & Huang (1999). They were able to cast the problem in the spectral domain so that one only needs to choose a one-dimensional positive-definite function of time lag in order to obtain a class of valid non-separable spatio-temporal covariance functions. In their Example 1, they construct the stationary spatio-temporal covariance function,\n\\[\nc(\\mathbf{h};\\tau) = \\sigma^2 \\exp\\{-b^2 ||\\mathbf{h}||^2/(a^2 \\tau^2 + 1)\\}/(a^2 \\tau^2 + 1)^{d/2},\n\\tag{4.13}\\]\nwhere \\(\\sigma^2 =c({\\mathbf{0}};0)\\), \\(d\\) corresponds to the spatial dimension (often \\(d=2\\)), and \\(a \\ge 0\\) and \\(b \\ge 0\\) are scale parameters in space and time, respectively. There are other classes of such spatio-temporal models, and this has been an active area of research in the past few decades (see the overview in Montero et al., 2015).\n\n\n\n\n\n\nTip\n\n\n\nIn this book we limit our focus to gstat when doing S-T kriging. However, there are numerous other packages in R that could be used. Among these CompRandFld and RandomFields are worth noting because of the large selection of non-separable spatio-temporal covariance functions they make available to the user.\n\n\n\n\nStochastic Partial Differential Equation (SPDE) Approach\nThe SPDE approach to deriving spatio-temporal covariance functions was originally inspired by statistical physics, where physical equations forced by random processes that describe advective, diffusive, and decay behavior were used to describe the second moments of macro-scale processes, at least in principle. A famous example of this approach in spatial statistics resulted in the ubiquitous Matérn spatial covariance function, which was originally derived as the solution to a fractional stochastic diffusion equation and has been extended by several authors (e.g., Montero et al., 2015).\nAlthough such an approach can suggest non-separable spatio-temporal covariance functions, only a few special (simple) cases lead to closed-form functions Cressie & Wikle (2011). Perhaps more importantly, although these models appear to have a physical basis through the SPDE, macro-scale real-world processes of interest are seldom this simple (e.g., linear and stationary in space and/or time). That is, the spatio-temporal covariance functions that can be obtained in closed form from SPDEs are seldom directly appropriate models for physical processes (but may still provide good fits to data).\n\n\n\n4.2.2 Spatio-Temporal Semivariograms\nHistorically, it has been common in the area of spatial statistics known as geostatistics to consider dependence through the variogram. In the context of a spatio-temporal random process \\(\\{Y(\\mathbf{s};t)\\}\\), the spatio-temporal variogram is defined as\n\\[\n\\textrm{var}(Y(\\mathbf{s};t) - Y(\\mathbf{s}';t')) \\equiv 2 \\gamma(\\mathbf{s},\\mathbf{s}';t,t'),\n\\tag{4.14}\\]\nwhere \\(\\gamma( \\cdot)\\) is called the  (see Note 2.1). The stationary version of the spatio-temporal variogram is denoted by \\(2 \\gamma(\\mathbf{h};\\tau)\\), where \\(\\mathbf{h}= \\mathbf{s}' - \\mathbf{s}\\) and \\(\\tau = t' - t\\), analogous to the stationary-covariance representation given previously. The underlying process \\(Y\\) is considered to be intrinsically stationary if it has a constant expectation and a stationary variogram. When the process is second-order stationary (second-order stationarity is a stronger restriction than intrinsic stationarity), there is a useful and simple relationship between the spatio-temporal and the covariance function, namely,\n\\[\n\\gamma(\\mathbf{h};\\tau) = c(\\mathbf{0};0) - c(\\mathbf{h};\\tau).\n\\tag{4.15}\\]\nNotice that strong spatio-temporal dependence corresponds to small values of the . Thus, contour plots of \\(\\{\\gamma(\\mathbf{h};\\tau)\\}\\) in Equation 4.15 start near zero close to the origin \\((\\mathbf{h};\\tau) = (\\mathbf{0},0)\\), and they rise to a constant value (the “sill”) as both \\(\\mathbf{h}\\) and \\(\\tau\\) move away from the origin.\nAlthough there has been a preference to consider dependence through the variogram in geostatistics, this has not been the case in more mainstream spatio-temporal statistical analyses. The primary reason for this is that most real-world processes are best characterized in the context of local second-order stationarity. The difference between intrinsic stationarity and second-order stationarity is most appreciated when the lags \\(\\mathbf{h}\\) and \\(\\tau\\) are large. If only local stationarity is expected and modeled, the extra generality given by the variogram is not needed. Still, the empirical semivariogram offers a useful way to summarize the spatio-temporal dependence in the data and to fit a spatio-temporal covariance function.\nOn a theoretical level, the stationary variogram allows S-T for a larger class of processes (i.e., intrinsically stationary processes) than the second-order stationary processes. A price to pay for this extra generality is the extreme caution needed when using the variogram to find optimal coefficients. Cressie & Wikle (2011, p. 148) point out that the universal- weights may not sum to 1 and, in situations where they do not, the resulting variogram-based predictor will not be optimal. However, when using the covariance-based predictor, there are no such issues and it is always optimal.\nIn addition, on a more practical level, most spatio-temporal analyses consider models that are specified from a likelihood perspective or a perspective, where covariance matrices are needed. The variogram by itself does not specify the covariance matrix, since one also needs to model the variance function \\(\\sigma^2(\\mathbf{s};t) \\equiv \\textrm{var}(Y(\\mathbf{s};t))\\), which is usually impractical unless it is stationary and does not depend on \\(\\mathbf{s}\\) and \\(t\\). Some software packages that perform S-T , such as gstat, fit variogram functions to data, mainly for historical reasons and because of the implicit assumption in Equation 4.14 that a constant mean need not be assumed when estimating the variogram. (This is generally a good thing because the constant mean assumption is tenuous in practice, since the mean for real-world processes typically depends on exogenous covariates that vary with space and time.)\n\n\n4.2.3 Gaussian Spatio-Temporal Model Estimation\nThe spatio-temporal covariance and variogram functions presented above depend on unknown parameters. These are almost never known in practice and must be estimated from the data. There is a history in spatial statistics of fitting covariance functions (or semivariograms) directly to the empirical estimates – for example, by using a least squares or weighted least squares approach (see Cressie (1993) for an overview). However, in the spatio-temporal context we prefer to consider fully parameterized covariance models and infer the parameters through likelihood-based methods or through fully methods. This follows closely the approaches in mixed-linear-model parameter estimation; for an overview, see McCulloch & Searle (2001). We briefly describe the likelihood-based approach and the approach below.\n\nLikelihood Estimation\nGiven the data model Equation 4.3, note that \\(\\mathbf{C}_z = \\mathbf{C}_y + \\mathbf{C}_\\epsilon\\). Then, in obvious notation, \\(\\mathbf{C}_z\\) depends on parameters \\(\\boldsymbol{\\theta}\\equiv \\{\\boldsymbol{\\theta}_y,\\boldsymbol{\\theta}_\\epsilon\\}\\) for the covariance functions of the hidden process \\(Y\\) and the measurement-error process \\(\\epsilon\\), respectively. The likelihood can then be written as\n\\[\nL(\\boldsymbol{\\beta},\\boldsymbol{\\theta}; \\mathbf{Z}) \\propto |\\mathbf{C}_z(\\boldsymbol{\\theta})|^{-1/2} \\exp\\left\\{-\\frac{1}{2}(\\mathbf{Z}- \\mathbf{X}\\boldsymbol{\\beta})'(\\mathbf{C}_z(\\boldsymbol{\\theta}))^{-1}(\\mathbf{Z}- \\mathbf{X}\\boldsymbol{\\beta})\\right\\},\n\\tag{4.16}\\]\nand we maximize this with respect to \\(\\{\\boldsymbol{\\beta},\\boldsymbol{\\theta}\\}\\), thus obtaining the maximum likelihood estimates (MLEs), \\(\\{\\widehat{\\boldsymbol{\\beta}}_{\\mathrm{mle}}, \\widehat{\\boldsymbol{\\theta}}_{\\mathrm{mle}}\\}\\). Because the covariance parameters appear in the matrix inverse and determinant in Equation 4.16, analytical maximization for most parametric covariance models is not possible, but numerical methods can be used. To reduce the number of parameters in this maximization, we often consider “profiling,” where we replace \\(\\boldsymbol{\\beta}\\) in Equation 4.16 with the generalized least squares estimator, \\({\\boldsymbol{\\beta}}_{\\mathrm{gls}} = (\\mathbf{X}' \\mathbf{C}_z(\\boldsymbol{\\theta})^{-1} \\mathbf{X})^{-1} \\mathbf{X}' \\mathbf{C}_z(\\boldsymbol{\\theta})^{-1} \\mathbf{Z}\\) (which depends only on \\(\\boldsymbol{\\theta}\\)). Then the profile likelihood is just a function of the unknown parameters \\(\\boldsymbol{\\theta}\\). Using a numerical optimization method (e.g., Newton–Raphson) to obtain \\(\\widehat{\\boldsymbol{\\theta}}_{\\mathrm{mle}}\\), we then obtain \\(\\widehat{\\boldsymbol{\\beta}}_{\\mathrm{mle}} = (\\mathbf{X}' \\mathbf{C}_z(\\widehat{\\boldsymbol{\\theta}}_{\\mathrm{mle}})^{-1} \\mathbf{X})^{-1} \\mathbf{X}' \\mathbf{C}_z(\\widehat{\\boldsymbol{\\theta}}_{\\mathrm{mle}})^{-1} \\mathbf{Z}\\), which is the MLE of \\(\\boldsymbol{\\beta}\\). The parameter estimates \\(\\{\\widehat{\\boldsymbol{\\beta}}_{\\mathrm{mle}}, \\widehat{\\boldsymbol{\\theta}}_{\\mathrm{mle}}\\}\\) are then substituted into the kriging equations above (e.g., Equation 4.8 and Equation 4.10) to obtain the empirical best linear unbiased predictor (EBLUP) and the associated empirical prediction variance.\n\n\n\n\n\n\nTip\n\n\n\nMaximizing the log-likelihood (i.e., the \\(\\log\\) of Equation 4.16) in R can be done in a number of ways. Among the most popular functions in base R are nlm, which implements a Newton-type algorithm, and optim, which contains a number of general-purpose routines, some of which are gradient-based. When a simple covariance function is used, the gradient can be found analytically, and gradient information may then be used to facilitate optimization. Many of the parameters in our models (such as the variance or dependence-scale parameters) need to be positive to ensure positive-definite covariance matrices. This can be easily achieved by finding the MLEs of the log of the parameters, instead of the parameters themselves. Then the MLE of the parameter on the original scale is obtained by exponentiating the MLE on the log scale. In this case, one typically uses the delta method to obtain the variance of the transformed parameter estimates (see, for example, Kendall & Stuart, 1969).\n\n\nAs described in Note 4.2, restricted maximum likelihood (REML) considers the likelihood of a linear transformation of the data vector such that the errors are orthogonal to the \\(\\mathbf{X}\\)s that make up the mean function. Numerical maximization of the associated likelihood, which is only a function of the parameters \\(\\boldsymbol{\\theta}\\) (i.e., not of \\(\\boldsymbol{\\beta}\\)), gives \\(\\widehat{\\boldsymbol{\\theta}}_{\\mathrm{reml}}\\). These estimates are substituted into Equation 4.9, the GLS formula for \\(\\boldsymbol{\\beta}\\), to obtain \\(\\widehat{\\boldsymbol{\\beta}}_{\\mathrm{reml}}\\) as well as the kriging equations Equation 4.8 and Equation 4.10.\nBoth the MLE and REML approaches have the advantage that they are based on the “likelihood principle” and, assuming that the Gaussian distributional assumptions are correct, they have desirable properties such as sufficiency, invariance, consistency, efficiency, and asymptotic normality. In mixed-effects models and in spatial statistics, REML is usually preferred over MLE for estimation of covariance parameters because REML typically has less bias in small samples (see, for example, the overview in Wu et al., 2001).\n\n\n\n\n\n\nNote 4.2: Restricted Maximum Likelihood\n\n\n\nConsider a contrast matrix \\(\\mathbf{K}\\) such that \\(E(\\mathbf{K}\\mathbf{Z}) = \\mathbf{0}\\). For example, let \\(\\mathbf{K}\\) be an \\((m - p ) \\times m\\) matrix orthogonal to the column space of the \\(m \\times p\\) design matrix \\(\\mathbf{X}\\). That is, let \\(\\mathbf{K}\\) correspond to the \\(m - p\\) linearly independent rows of \\((\\mathbf{I}- \\mathbf{X}(\\mathbf{X}' \\mathbf{X})^{-1} \\mathbf{X}')\\). Because \\(\\mathbf{K}\\mathbf{X}= \\mathbf{0}\\), it follows that \\(E(\\mathbf{K}\\mathbf{Z}) = \\mathbf{K}\\mathbf{X}\\boldsymbol{\\beta}= \\mathbf{0}\\), and \\(\\textrm{var}(\\mathbf{K}\\mathbf{Z}) = \\mathbf{K}\\mathbf{C}_z(\\boldsymbol{\\theta}) \\mathbf{K}'\\). In this case, the likelihood based on \\(\\mathbf{K}\\mathbf{Z}\\) is not a function of the mean parameters \\(\\boldsymbol{\\beta}\\) and is given by\n\\[\nL_{\\mathrm{reml}}(\\boldsymbol{\\theta};\\mathbf{Z}) \\propto |\\mathbf{K}\\mathbf{C}_z(\\boldsymbol{\\theta}) \\mathbf{K}'|^{-1/2} \\exp\\left\\{-\\frac{1}{2}(\\mathbf{K}\\mathbf{Z})'(\\mathbf{K}\\mathbf{C}_z(\\boldsymbol{\\theta}) \\mathbf{K}')^{-1}(\\mathbf{K}\\mathbf{Z}) \\right\\}.\n\\tag{4.17}\\]\nThen Equation 4.17 is maximized numerically to obtain \\(\\widehat{\\boldsymbol{\\theta}}_{\\mathrm{reml}}\\). Note that parameter estimation and statistical inference with REML do not depend on the specific choice of \\(\\mathbf{K}\\), so long as it is a contrast matrix that leads to \\(E(\\mathbf{K}\\mathbf{Z}) = \\mathbf{0}\\) (Patterson & Thompson, 1971). One can then use these estimates in a GLS estimate of \\(\\boldsymbol{\\beta}\\): \\(\\widehat{\\boldsymbol{\\beta}}_{\\mathrm{reml}} \\equiv (\\mathbf{X}' \\mathbf{C}_z(\\widehat{\\boldsymbol{\\theta}}_{\\mathrm{reml}})^{-1} \\mathbf{X})^{-1} \\mathbf{X}' \\mathbf{C}_z(\\widehat{\\boldsymbol{\\theta}}_{\\mathrm{reml}})^{-1} \\mathbf{Z}\\).\n\n\n\n\nBayesian Inference\nInstead of treating \\(\\boldsymbol{\\beta}\\) and \\(\\boldsymbol{\\theta}\\) as fixed, unknown, and to be estimated (e.g., from the likelihood), prior distributions \\([\\boldsymbol{\\beta}]\\) and \\([\\boldsymbol{\\theta}]\\) (often assumed independent) could be posited for the mean parameters \\(\\boldsymbol{\\beta}\\) and the covariance parameters \\(\\boldsymbol{\\theta}\\), respectively. Typical choices for \\([\\boldsymbol{\\theta}]\\) do not admit closed-form posterior distributions for \\([Y(\\mathbf{s}_0) | \\mathbf{Z}]\\), which means that the predictor \\(E(Y(\\mathbf{s}_0;t_0) | \\mathbf{Z})\\) and the associated uncertainty, \\(\\textrm{var}(Y(\\mathbf{s}_0;t_0) | \\mathbf{Z})\\), are not available in closed form and must be obtained through numerical evaluation of the posterior distribution (for more details, see Section 4.5.2 below; Cressie & Wikle (2011); Banerjee et al. (2015)).\n\n\nExample: S-T Kriging\nConsider the maximum-temperature observations in the NOAA data set Tmax. The empirical covariogram of these data is shown in the top left panel of Figure 4.4. Consider two spatio-temporal covariance functions fitted to the residuals from a model with a regression component that includes an intercept and latitude as a covariate. The first of these covariance functions is given by an isotropic and stationary separable model of the form\n\\[\nc^{(\\mathrm{sep})}(\\| \\mathbf{h}\\| ; | \\tau|) \\equiv c^{(s)}(\\| \\mathbf{h}\\|) \\cdot c^{(t)}(|\\tau|),\n\\tag{4.18}\\]\nin which we let both covariance functions, \\(c^{(s)}(\\cdot)\\) and \\(c^{(t)}(\\cdot)\\), take the form\n\\[\nc^{(\\cdot)}(h) = b_1\\exp(-\\phi h) + b_2I(h=0),\n\\tag{4.19}\\]\nwhere \\(\\phi\\), \\(b_1\\), and \\(b_2\\) are parameters that are different for \\(c^{(s)}(\\cdot)\\) and \\(c^{(t)}(\\cdot)\\) and need to be estimated; and \\(I(\\cdot)\\) is the indicator function that is used to represent the so-called nugget effect, made up of the measurement-error variance plus the micro-scale variation. The fitted model is shown in the bottom left panel of Figure 4.4.\nThe second model we fit is a non-separable spatio-temporal covariance function, in which the temporal lag is scaled to account for the different nature of space and time. This model is given by\n\\[\nc^{(\\mathrm{st})}(\\| \\mathbf{v}_a \\|) \\equiv b_1\\exp(-\\phi \\| \\mathbf{v}_a \\|) + b_2I(\\| \\mathbf{v}_a \\| = 0),\n\\tag{4.20}\\]\nwhere \\(\\mathbf{v}_a \\equiv (\\mathbf{h}',a \\tau)'\\), and recall that \\(||\\mathbf{v}_a|| = (\\mathbf{h}' \\mathbf{h}+ a^2 \\tau^2)^{1/2}\\). Here, \\(a\\) is the scaling factor used for generating the space-time anisotropy. The fitted model is shown in the bottom right panel of Figure 4.4.\nThe non-separable spatio-temporal covariance function (Equation 4.20) allows for space-time anisotropy, but it is otherwise relatively inflexible. It only contains one parameter (\\(a\\)) to account for the different scaling needed for space and time, one parameter (\\(\\phi\\)) for the length scale, and two parameters to specify the variance (the nugget effect, \\(b_2\\), and the variance of the smooth component, \\(b_1\\)). Thus, Equation 4.20 has a total of four parameters, in contrast to the six parameters in Equation 4.18. This results in a relatively poor fit to the Tmax data from the NOAA data set. In this case, the separable model is able to provide a better reconstruction of the empirical covariance function despite its lack of space-time interaction, which is not surprising given that the fitted separable covariance function (Figure 4.4, top right) is visually similar to the empirical spatio-temporal covariance function (Figure 4.4, top left). We note that although the separable model fits better in this case, it is still a rather unrealistic model for most processes of interest.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Descriptive Spatio-Temporal Statistical Models</span>"
    ]
  },
  {
    "objectID": "Chapter4.html#sec-randomeffects",
    "href": "Chapter4.html#sec-randomeffects",
    "title": "4  Descriptive Spatio-Temporal Statistical Models",
    "section": "4.3 Random-Effects Parameterizations",
    "text": "4.3 Random-Effects Parameterizations\nAs discussed previously, it can be difficult to specify realistic valid spatio-temporal covariance functions and to work with large spatio-temporal covariance matrices (e.g., \\(\\mathbf{C}_z\\)) in situations with large numbers of prediction or observation locations. One way to mitigate these problems is to take advantage of conditional specifications that the hierarchical modeling framework allows.\nWe can consider classical linear mixed models from either a conditional perspective, where we condition the response on the random effects, or from a marginal perspective, where the random effects have been averaged (integrated) out (see Note 4.3), and it is this marginal distribution that is modeled. We digress briefly from the spatio-temporal context to illustrate the conditional versus marginal approach in a simple longitudinal-data-analysis setting. Longitudinal data are collected over time, often in a clinical trial where the response to drug treatments and controls is measured on the same subjects at different follow-up times. Here, one might allow there to be subject-specific intercepts or slopes corresponding to the treatment effect over time.\nFigure 4.5 shows simulated data for a longitudinal study in which 90 individuals are assigned randomly to three treatment groups (control, treatment 1, and treatment 2), 30 per group. Their responses are then plotted through time (20 time points). In each case, the response is generally linear with time, with individual-specific random intercepts and slopes. These responses can be modeled in terms of a linear mixed model, with fixed effects corresponding to the treatment (control, treatment 1, and treatment 2), individual random effects for the slope and intercept, and a random effect for the error. The random effects correspond to a situation where individuals have somewhat different baseline responses (intercept), and their response with time to the treatment is also subject to individual variation (slope).\nFor the simulated data shown in Figure 4.5, we might consider a longitudinal model such as Verbeke & Molenberghs (2009), Section 3.3:\n\\[\nZ_{ij} = \\left\\{\n\\begin{array}{ll}\n(\\beta_0 + \\alpha_{0i}) + (\\beta_1 + \\alpha_{1i}) t_{j} + \\epsilon_{ij}, & \\text{if the subject receives the control}, \\\\\n(\\beta_0 + \\alpha_{0i}) + (\\beta_2 + \\alpha_{1i}) t_{j} + \\epsilon_{ij}, & \\text{if the subject receives treatment 1}, \\\\\n(\\beta_0 + \\alpha_{0i}) + (\\beta_3 + \\alpha_{1i}) t_{j} + \\epsilon_{ij}, & \\text{if the subject receives treatment 2},\n\\end{array}\n\\right.\n\\]\nwhere \\(Z_{ij}\\) is the response for the \\(i\\)th subject (\\(i=1,\\ldots,n=90\\)) at time \\(j=1,\\ldots,T=20\\); \\(\\beta_0\\) is an overall fixed intercept; \\(\\beta_1, \\beta_2, \\beta_3\\) are fixed time-trend effects; and \\(\\alpha_{0i} \\sim \\text{iid} \\, \\text{Gau}(0,\\sigma^2_1)\\) and \\(\\alpha_{1i} \\sim \\text{iid} \\, \\text{Gau}(0,\\sigma^2_2)\\) are individual-specific random intercept and slope effects, respectively. We can write this model in the classical linear mixed-model notation as\n\\[\n\\mathbf{Z}_i = \\mathbf{X}_i \\boldsymbol{\\beta}+ \\boldsymbol{\\Phi}\\boldsymbol{\\alpha}_i + \\boldsymbol{\\varepsilon}_i,\n\\]\nwhere \\(\\mathbf{Z}_i\\) is a \\(20\\)-dimensional vector of responses for the \\(i\\)th individual; \\(\\mathbf{X}_i\\) is a \\(20 \\times 4\\) matrix consisting of a column vector of \\(1\\)s (intercept) and three columns indicating the treatment group of the \\(i\\)th individual; \\(\\boldsymbol{\\beta}\\) is a four-dimensional vector of fixed effects; \\(\\boldsymbol{\\Phi}\\) is a \\(20 \\times 2\\) matrix with a vector of \\(1\\)s in the first column and the second column consists of the vector of times, \\((1,2,\\ldots,20)'\\); the associated random-effect vector is \\(\\boldsymbol{\\alpha}_i \\equiv (\\alpha_{0i},\\alpha_{1i})'  \\sim \\text{Gau}(\\mathbf{0},\\mathbf{C}_\\alpha)\\), where \\(\\mathbf{C}_\\alpha = \\text{diag}(\\sigma^2_1, \\sigma^2_2)\\); and \\(\\boldsymbol{\\varepsilon}_i \\sim \\text{Gau}(\\mathbf{0},\\sigma^2_\\epsilon \\mathbf{I})\\) is a \\(20\\)-dimensional error vector. We assume that the elements of \\(\\{\\boldsymbol{\\alpha}_i\\}\\) and \\(\\{\\boldsymbol{\\varepsilon}_i\\}\\) are all mutually independent.\nBecause the variation in the individuals’ intercepts and slopes is specified by random effects, this formulation allows one to consider inference at the subject (individual) level (e.g., predictions of an individual’s true values). However, if interest is in the fixed treatment effects \\(\\boldsymbol{\\beta}\\), one might consider the marginal distribution of the responses in which these individual random effects have been removed through averaging (integration). Responses that share common random effects exhibit marginal dependence through the marginal covariance matrix, and so the inference on the fixed effects (e.g., via generalized least squares) then accounts for this more complicated marginal dependence. For the example presented here, one can show that the marginal covariance for an individual’s response at time \\(t_j\\) and \\(t_k\\) is given by \\(\\mbox{cov}(Z_{ij},Z_{ik}) = \\sigma^2_1 + t_j  t_k  \\sigma^2_2 + \\sigma^2_\\epsilon I(j = k)\\), which says that the marginal variance is time-varying, whereas the conditional covariance (conditioned on \\(\\boldsymbol{\\alpha}\\)) is simply \\(\\sigma^2_\\epsilon I(j = k)\\).\nIn the context of spatial or spatio-temporal modeling, the same considerations as for the classical linear mixed-effects model apply. That is, we can also write the process of interest conditional on random effects, where the random effects might be spatial, temporal, or spatio-temporal. Why is this important? As we show in the next section, it allows us to build spatio-temporal dependence conditionally, in such a way that the implied marginal spatio-temporal covariance function is always valid, and it provides some computational advantages.\n\n\n\n\n\n\nFigure 4.5: Simulated longitudinal data showing the response of individuals through time. The red lines are the simulated responses for a control group, the green lines are the simulated responses for treatment 1, and the blue lines are the simulated responses for treatment 2.\n\n\n\n\n\n\n\n\n\nNote 4.3: Marginal and Conditional Linear Mixed Models\n\n\n\nConsider the conditional representation of a classic general linear mixed-effects model Laird & Ware (1982) for response vector \\(\\mathbf{Z}\\) and fixed and random effects vectors, \\(\\boldsymbol{\\beta}\\) and \\(\\boldsymbol{\\alpha}\\), respectively. Specifically, consider\n\\[\n\\mathbf{Z}| \\boldsymbol{\\alpha}\\sim Gau(\\mathbf{X}\\boldsymbol{\\beta}+ \\boldsymbol{\\Phi}\\boldsymbol{\\alpha}, \\mathbf{C}_\\epsilon),\n\\tag{4.21}\\]\n\\[\n\\boldsymbol{\\alpha}\\sim Gau({\\mathbf{0}}, \\mathbf{C}_\\alpha),\n\\]\nwhere \\(\\mathbf{X}\\) and \\(\\boldsymbol{\\Phi}\\) are assumed to be known matrices, and \\(\\mathbf{C}_\\epsilon\\) and \\(\\mathbf{C}_\\alpha\\) are known covariance matrices. The marginal distribution of \\(\\mathbf{Z}\\) is then given by integrating out the random effects:\n\\[\n[\\mathbf{Z}] = \\int [\\mathbf{Z}\\; | \\; \\boldsymbol{\\alpha}][\\boldsymbol{\\alpha}] \\textrm{d}\\boldsymbol{\\alpha}.\n\\tag{4.22}\\]\nNote that dependence on \\(\\boldsymbol{\\theta}\\), which recall are the covariance parameters in \\(\\mathbf{C}_z\\) and \\(\\mathbf{C}_\\alpha\\), has been suppressed in Equation 4.22, although the (implicit) presence of \\(\\boldsymbol{\\theta}\\) can be seen in Equation 4.23–Equation 4.26 below. We can obtain this distribution by making use of iterated conditional expectation and variance formulas. In particular, note that we can write the model associated with Equation 4.21 as\n\\[\n\\mathbf{Z}= \\mathbf{X}\\boldsymbol{\\beta}+ \\boldsymbol{\\Phi}\\boldsymbol{\\alpha}+ \\boldsymbol{\\varepsilon}, \\quad\\boldsymbol{\\varepsilon}\\sim Gau({\\mathbf{0}}, \\mathbf{C}_\\epsilon),\n\\tag{4.23}\\]\nand then\n\\[\nE(\\mathbf{Z}) = E_{\\alpha}\\{E(\\mathbf{Z}| \\boldsymbol{\\alpha})\\} = E_{\\alpha}\\{\\mathbf{X}\\boldsymbol{\\beta}+ \\boldsymbol{\\Phi}\\boldsymbol{\\alpha}\\} = \\mathbf{X}\\boldsymbol{\\beta},\n\\tag{4.24}\\]\n\\[\n\\textrm{var}(\\mathbf{Z}) = \\textrm{var}_{\\alpha}\\{E(\\mathbf{Z}| \\boldsymbol{\\alpha})\\} + E_{\\alpha}\\{\\textrm{var}(\\mathbf{Z}| \\boldsymbol{\\alpha})\\}  =  \\boldsymbol{\\Phi}\\mathbf{C}_\\alpha \\boldsymbol{\\Phi}' + \\mathbf{C}_\\epsilon.\n\\tag{4.25}\\]\nThen, since Equation 4.23 shows that \\(\\mathbf{Z}\\) is a linear combination of normally distributed random variables, it is also normally distributed and the marginal distribution is given by\n\\[\n\\mathbf{Z}\\; \\sim \\; Gau(\\mathbf{X}\\boldsymbol{\\beta},  \\boldsymbol{\\Phi}\\mathbf{C}_\\alpha \\boldsymbol{\\Phi}' + \\mathbf{C}_\\epsilon).\n\\tag{4.26}\\]\nThus, we can see that the integration over the common random effects \\(\\boldsymbol{\\alpha}\\) in Equation 4.22 induces a more complicated error covariance structure in the marginal distribution (i.e., compare the marginal covariance matrix, \\(\\boldsymbol{\\Phi}\\mathbf{C}_\\alpha \\boldsymbol{\\Phi}' + \\mathbf{C}_\\epsilon\\), to the conditional covariance matrix, \\(\\mathbf{C}_\\epsilon\\)). This idea of conditioning on random effects and inducing dependence through integration is fundamentally important to hierarchical statistical modeling. That is, it is typically easier to model means than it is to model covariances, and so we put our modeling effort into the conditional mean and then let the integration induce the more complicated marginal dependence rather than specifying it directly.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Descriptive Spatio-Temporal Statistical Models</span>"
    ]
  },
  {
    "objectID": "Chapter4.html#sec-basisfunctions",
    "href": "Chapter4.html#sec-basisfunctions",
    "title": "4  Descriptive Spatio-Temporal Statistical Models",
    "section": "4.4 Basis-Function Representations",
    "text": "4.4 Basis-Function Representations\nBy themselves, the conditional specifications discussed in Section 4.3 are often not enough to help us deal with the problem of specifying realistic spatio-temporal covariance structures and deal with the “curse of dimensionality,” which is endemic in spatio-temporal statistics. We also need to pay particular attention to our choice of \\(\\boldsymbol{\\Phi}\\), and we often do this through basis-function expansions (recall that we introduced basis functions in Chapter 1 and in more detail in Chapter 3).\nBasis functions, like covariates, can be nonlinear functions of \\((\\mathbf{s};t)\\); however, the expansion is a linear function of the basis functions’ coefficients. We assume that these coefficients are the objects of inference in a statistical additive model. If the coefficients are fixed but unknown and to be estimated, then we have a regression model and the basis functions act as covariates (see, for example, Section 3.2). If the coefficients are random, then we have a random-effects model (or, if covariates are also present, a mixed-effects model) and we can perform inference on the moments of those random effects. More importantly, as we have shown in Section 4.3, this framework allows us to build complexity through marginalization. This often simplifies the model specification, particularly if we consider the random effects to be associated with spatial, temporal, or spatio-temporal . In the following subsections, we consider spatio-temporal models that involve these three types of basis functions.\n\n4.4.1 Random Effects with Spatio-Temporal Basis Functions\nAssuming the same data model (Equation 4.3) as above, we rewrite the process model (Equation 4.2) in terms of fixed and random effects, \\(\\boldsymbol{\\beta}\\) and \\(\\{\\alpha_i: i=1,\\ldots,n_\\alpha\\}\\), respectively:\n\\[\nY(\\mathbf{s};t) =  \\mathbf{x}(\\mathbf{s};t)'\\boldsymbol{\\beta}+ \\eta(\\mathbf{s};t)\n\\; = \\;  \\mathbf{x}(\\mathbf{s};t)'\\boldsymbol{\\beta}+ \\sum_{i=1}^{n_\\alpha} \\phi_{i}(\\mathbf{s};t) \\alpha_i + \\nu(\\mathbf{s};t),\n\\tag{4.27}\\]\nwhere \\(\\{\\phi_{i}(\\mathbf{s};t): i=1,\\ldots,n_\\alpha \\}\\) are specified spatio-temporal corresponding to location \\((\\mathbf{s};t)\\), \\(\\{\\alpha_i\\}\\) are random effects, and \\(\\nu(\\mathbf{s};t)\\) is sometimes needed to represent small-scale spatio-temporal random effects not captured by the basis functions. So, in Equation 4.27 we are just decomposing the spatio-temporal random process, \\(\\eta(\\mathbf{s};t)\\), into a linear combination of random effects and a “residual” error term.\nLet \\(\\boldsymbol{\\alpha}\\sim Gau({\\mathbf{0}},\\mathbf{C}_\\alpha)\\), where \\(\\boldsymbol{\\alpha}\\equiv (\\alpha_1,\\ldots,\\alpha_{n_\\alpha})'\\). Suppose we are interested in making inference on the process \\(Y\\) at \\(n_y\\) spatio-temporal locations, which we denote by the \\(n_y\\)-dimensional vector \\(\\mathbf{Y}\\). The process model then becomes\n\\[\n\\mathbf{Y}= \\mathbf{X}\\boldsymbol{\\beta}+ \\boldsymbol{\\Phi}\\boldsymbol{\\alpha}+ \\boldsymbol{\\nu},\n\\tag{4.28}\\]\nwhere the \\(i\\)th column of the \\(n_y \\times n_\\alpha\\) matrix \\(\\boldsymbol{\\Phi}\\) corresponds to the \\(i\\)th basis function, \\(\\phi_{i}(\\cdot;\\cdot)\\), at all of the \\(n_y\\) spatio-temporal locations, and in the same order as that used to construct \\(\\mathbf{Y}\\). The vector \\(\\boldsymbol{\\nu}\\) also corresponds to the spatio-temporal ordering given in \\(\\mathbf{Y}\\), and \\(\\boldsymbol{\\nu}\\sim Gau({\\mathbf{0}},\\mathbf{C}_\\nu)\\). In this case, one can see (Note 4.3) that the marginal distribution of \\(\\mathbf{Y}\\) is given by \\(\\mathbf{Y}\\sim  Gau(\\mathbf{X}\\boldsymbol{\\beta}, \\boldsymbol{\\Phi}\\mathbf{C}_\\alpha \\boldsymbol{\\Phi}' + \\mathbf{C}_\\nu )\\), so that \\(\\mathbf{C}_y = \\boldsymbol{\\Phi}\\mathbf{C}_\\alpha \\boldsymbol{\\Phi}' + \\mathbf{C}_\\nu\\). Now the vector of covariance parameters \\(\\boldsymbol{\\theta}\\) is augmented to include parameters in \\(\\mathbf{C}_\\nu\\). The spatio-temporal dependence is accounted for by the spatio-temporal basis functions, \\(\\boldsymbol{\\Phi}\\), and in general this could accommodate non-separable dependence. A benefit of this approach is that the spatio-temporal modeling effort focuses on the fixed number \\(n_\\alpha\\) of random effects. In this case, note that the random effects \\(\\boldsymbol{\\alpha}\\) are not indexed by space and time, so it should be easier to specify a model for them. For example, we can specify a covariance matrix to describe their dependence, which is easier than specifying a covariance function.\nIn situations where \\(n_\\alpha \\ll n_y\\) (i.e., a low-rank representation), an additional benefit comes from being able to perform matrix inverses in terms of \\(n_\\alpha\\)-dimensional matrices (through well-known matrix-algebra relationships). Specifically, under model Equation 4.28 we note that we can write \\(\\mathbf{C}_z = \\boldsymbol{\\Phi}\\mathbf{C}_\\alpha \\boldsymbol{\\Phi}' + \\mathbf{V}\\), where we define \\(\\mathbf{V}\\equiv \\mathbf{C}_\\nu + \\mathbf{C}_\\epsilon\\). Then, using the well-known Sherman–Morrison–Woodbury matrix identities [e.g., available at https://doi.org/10.1007/978-1-4757-2085-3] (Searle, 1982), we can write\n\\[\n\\mathbf{C}_z^{-1} = \\mathbf{V}^{-1} - \\mathbf{V}^{-1} \\boldsymbol{\\Phi}(\\boldsymbol{\\Phi}' \\mathbf{V}^{-1} \\boldsymbol{\\Phi}+ \\mathbf{C}_\\alpha^{-1})^{-1} \\boldsymbol{\\Phi}' \\mathbf{V}^{-1}.\n\\]\nImportantly, if \\(\\mathbf{V}^{-1}\\) has simple structure (e.g., is sparse or diagonal) and \\(n_\\alpha \\ll n_y\\), then this inverse is easy to calculate because it is a function of a simple high-dimensional matrix \\(\\mathbf{V}^{-1}\\) and a low-dimensional matrix inverse \\(\\mathbf{C}_\\alpha^{-1}\\).\nIt is important to note that even in the full-rank (\\(n_\\alpha = n_y\\)) and over-complete (\\(n_\\alpha &gt; n_y\\)) cases there can still be computational benefits through induced sparsity in \\(\\mathbf{C}_\\alpha\\) and the use of efficient matrix-multiplication routines that use multiresolution algorithms, orthogonality, and/or sparse precision matrices. In addition, basis-function implementations may assume that \\(\\boldsymbol{\\nu}= {\\mathbf{0}}\\) and often that \\(\\boldsymbol{\\Phi}\\) is orthogonal, so that \\(\\boldsymbol{\\Phi}\\boldsymbol{\\Phi}' = \\mathbf{I}\\); in those cases, one can reduce the computational burden significantly. Finally, we note that specific and methodologies are devised to take advantage of other properties of various matrices (e.g., sparse structure on the random-effects covariance matrix, \\(\\mathbf{C}_\\alpha\\), or on the random-effects precision matrix, \\(\\mathbf{C}_\\alpha^{-1}\\)).\n\n\n\n\n\n\nTip\n\n\n\nSparse matrices can be used in R using definitions in the packages Matrix or spam. For both these packages, arithmetic operations, decompositions (e.g., the Cholesky decomposition), back-solves and forward-solves, and other important matrix operations, can be done seamlessly using standard R commands. With Matrix, a sparse matrix can be constructed using the function sparseMatrix, while a sparse diagonal matrix can be constructed using the function Diagonal. With the former, the argument symmetric = TRUE can be used to specify a sparse symmetric matrix.\n\n\nThe definition of “basis function” in our spatio-temporal context is pretty liberal; the matrix \\(\\boldsymbol{\\Phi}\\) in the product \\(\\boldsymbol{\\Phi}\\boldsymbol{\\alpha}\\) is a spatio-temporal basis-function matrix so long as its coefficients \\(\\boldsymbol{\\alpha}\\) are random and the columns of \\(\\boldsymbol{\\Phi}\\) are spatio-temporally referenced. One decision associated with fitting model Equation 4.27 concerns the choice of . For spatial processes, the decisions one makes with regard to the choice of are usually not that critical, as there are multiple types of bases that can accommodate the same spatial variability. However, as one starts considering spatio-temporal processes, the choice of can make a difference, especially for the dynamical formulations presented in Chapter 5.\nIn general, one can use (i) fixed or parameterized basis functions, (ii) local or global , (iii) reduced-rank, complete, or over-complete bases, and (iv) basis functions with expansion coefficients possibly indexed by space, time, or space-time. Further, the choice is affected by the presence and type of residual structure and the distribution of the random effects. Historically, it has been fairly challenging to come up with good spatio-temporal basis functions (for the same reason it has been difficult to come up with truly realistic spatio-temporal covariance functions). One simplification is to consider tensor-product (mentioned in Section 3.2 and Note 4.1), where we define the spatio-temporal basis function as the product of a spatial basis function and a temporal basis function. Note that this does not yield a separable spatio-temporal model, in general. It is also quite common to see spatio-temporal-dependence models for \\(Y\\), where the statistical dependence comes from spatial-only whose coefficients are temporal stochastic processes (Section 4.4.2).\n\nExample: Fixed Rank Kriging\nA widely adopted method for rank reduction is fixed rank kriging (FRK), implemented in R through the package FRK. Lab 4.2 demonstrates how FRK can be applied to the maximum temperature (Tmax) in the NOAA data set using \\(n_\\alpha = 1880\\) space-time tensor-product basis functions (see Note 4.1) at two resolutions for \\(\\{\\phi_i(\\mathbf{s};t):i=1,\\dots,n_\\alpha\\}\\). In particular, bisquare basis functions are used (see Lab 4.2 for details). FRK also considers a fine-scale-variation component \\(\\boldsymbol{\\nu}\\) such that \\(\\mathbf{C}_\\nu\\) is diagonal. The matrix \\(\\mathbf{C}_\\alpha\\) is constructed such that the coefficients \\(\\boldsymbol{\\alpha}\\) at each resolution are independent, and such that the covariances between these coefficients within a resolution decay exponentially with the distance between the centers of the basis functions. Parameters are estimated using an EM algorithm for computing maximum likelihood estimates (see Note 4.4).\nFigure 4.6 shows the predictions and prediction standard errors obtained using FRK; as is typical for kriging, the computations are made with \\(\\widehat{\\boldsymbol{\\theta}}\\) substituted in for the unknown covariance parameters \\(\\boldsymbol{\\theta}\\). Although the uncertainty in \\(\\widehat{\\boldsymbol{\\theta}}\\) is not accounted for in this setting, it is typically thought to be a fairly minor component of the variation in the spatio-temporal prediction. The predictions are similar to those obtained using S-T kriging in Figure 4.2 of Section 4.2, but they are also a bit “noisier” because of the assumed uncorrelated fine-scale variation term; see Equation 4.27. The prediction standard errors show similar patterns to those obtained earlier (Figure 4.2), although there are notable differences upon visual examination. This is commonly observed when using reduced-rank methods, and it is particularly evident with very-low-rank implementations (e.g., with EOFs) accompanied with spatially uncorrelated fine-scale variation. In such cases, the prediction-standard-error maps can have prediction standard errors related more to the shapes of the basis functions and less to the prediction location’s proximity to an observation.\n\n\n\n\n\n\nFigure 4.6: Left: Predictions of Tmax and right: prediction standard errors in degrees Fahrenheit within a square box enclosing the domain of interest for six days (each 5 days apart) spanning the temporal window of the data, 01 July 1993–20 July 2003, using bisquare spatio-temporal and the R package FRK. Data for 14 July 1993 were omitted from the original data set.\n\n\n\n\n\n\n\n\n\nNote 4.4: Basic EM Algorithm\n\n\n\nIn some cases, it can be computationally more efficient to perform maximum likelihood estimation using the expectation-maximization (EM) algorithm rather than through direct optimization of the likelihood function. The basic idea is that one defines complete data to be a combination of actual observations and missing observations. Let \\(W\\) denote these complete data made up of observations (\\(W_{\\mathrm{obs}}\\)) and “missing” observations (\\(W_{\\mathrm{mis}}\\)), and \\(\\theta\\) represents the unknown parameters in the model, so that the complete-data log-likelihood is given by \\(\\log(L(\\theta | W))\\). The basic EM algorithm is given below.\nChoose starting values for the parameter, \\(\\hat{\\theta}^{(0)}\\)\nrepeat \\(i=1,2,\\ldots\\)\n\nE-Step: Obtain \\(Q(\\theta | \\hat{\\theta}^{(i-1)}) = E\\{\\log(L(\\theta \\mid W)) \\mid W_{\\mathrm{obs}}, \\hat{\\theta}^{(i-1)}\\}\\)\nM-Step: Obtain \\(\\hat{\\theta}^{(i)} = \\max_{\\theta} \\{Q(\\theta \\mid \\hat{\\theta}^{(i-1)})\\}\\)\n\nuntil convergence either in \\(\\hat{\\theta}^{(i)}\\) or in \\(\\log(L(\\theta \\mid W))\\)\nIn Section 4.4, \\(W_{\\mathrm{obs}}\\) corresponds to the data \\(\\mathbf{Z}\\), while \\(W_{\\mathrm{mis}}\\) corresponds to the coefficients \\(\\boldsymbol{\\alpha}\\).\n\n\n\n\n\n4.4.2 Random Effects with Spatial Basis Functions\nConsider the case where the of the spatio-temporal process are functions of space only and their random coefficients are indexed by time:\n\\[\nY(\\mathbf{s};t_j) = \\mathbf{x}(\\mathbf{s};t_j)'\\boldsymbol{\\beta}+ \\sum_{i=1}^{n_\\alpha} \\phi_i(\\mathbf{s}) \\alpha_{i}(t_j) + \\nu(\\mathbf{s};t_j),\\quad j=1,\\ldots,T,\n\\tag{4.29}\\]\nwhere \\(\\{\\phi_i(\\mathbf{s}): i=1,\\ldots,n_\\alpha;\\ \\mathbf{s}\\in D_s\\}\\) are known spatial , \\(\\alpha_{i}(t_j)\\) are temporal random processes, and the other model components are defined as above. We can consider a wide variety of spatial for this model, and again these might be of reduced rank, of full rank, or over-complete. For example, we might consider complete global (e.g., Fourier), or reduced-rank empirically defined (e.g., EOFs), or a variety of non-orthogonal bases (e.g., Gaussian functions, wavelets, bisquare functions, or Wendland functions). We illustrate a few of these in one dimension in Figure 4.7 (see also Section 3.2). It is often not important which basis function is used; still, one has to be careful to ensure that the type and number of are flexible and large enough to model the true dependence in \\(Y\\) (and the data \\(\\mathbf{Z}\\)). This requires some experimentation and model diagnostics (see, for example, Chapter 6).\n\n\n\n\n\n\nFigure 4.7: Some spatial that can be employed in spatio-temporal modeling, depicted in one-dimensional space. From left to right: bisquare, cosine, Gaussian, linear element, Mexican-hat wavelet, and first-order Wendland functions.\n\n\n\nAssuming interest in the spatio-temporal dependence at \\(n\\) spatial locations \\(\\{\\mathbf{s}_1,\\ldots,\\mathbf{s}_n\\}\\) and at times \\(\\{t_j: j=1,2,\\ldots,T\\}\\), we can write model Equation 4.29 in vector form as\n\\[\n\\mathbf{Y}_{t_j} = \\mathbf{X}_{t_j} \\boldsymbol{\\beta}+ \\boldsymbol{\\Phi}\\boldsymbol{\\alpha}_{t_j} + \\boldsymbol{\\nu}_{t_j},\n\\tag{4.30}\\]\nwhere \\(\\mathbf{Y}_{t_j}= (Y(\\mathbf{s}_1;t_j),\\ldots,Y(\\mathbf{s}_n;t_j))'\\) is the \\(n\\)-dimensional process vector, \\(\\boldsymbol{\\nu}_{t_j} \\; \\sim \\; Gau({\\mathbf{0}},\\mathbf{C}_\\nu)\\), \\(\\boldsymbol{\\alpha}_{t_j} \\equiv (\\alpha_{1}(t_j),\\ldots,\\alpha_{n_\\alpha}(t_j))'\\), \\(\\boldsymbol{\\Phi}\\equiv (\\boldsymbol{\\phi}(\\mathbf{s}_1),\\ldots,\\boldsymbol{\\phi}(\\mathbf{s}_n))'\\), and \\(\\boldsymbol{\\phi}(\\mathbf{s}_i) \\equiv (\\phi_1(\\mathbf{s}_i),\\ldots,\\phi_{n_\\alpha}(\\mathbf{s}_i))'\\), \\(i=1,\\ldots,n\\). An important question is then what the preferred distribution for \\(\\boldsymbol{\\alpha}_{t_j}\\) is.\nIt can be shown that if \\(\\boldsymbol{\\alpha}_{t_1}, \\boldsymbol{\\alpha}_{t_2},\\ldots\\) are independent in time, where \\(\\boldsymbol{\\alpha}_{t_j} \\sim iid \\; Gau({\\mathbf{0}},\\mathbf{C}_\\alpha)\\), then the marginal distribution of \\(\\mathbf{Y}_{t_j}\\) is \\(Gau(\\mathbf{X}_{t_j} \\boldsymbol{\\beta}, \\boldsymbol{\\Phi}\\mathbf{C}_\\alpha \\boldsymbol{\\Phi}' + \\mathbf{C}_\\nu)\\), and \\(\\mathbf{Y}_{t_1}, \\mathbf{Y}_{t_2},\\ldots\\) are independent. Hence, the \\(nT \\times nT\\) joint spatio-temporal covariance matrix is given by the , \\(\\mathbf{C}_Y = \\mathbf{I}_T \\otimes (\\boldsymbol{\\Phi}\\mathbf{C}_\\alpha \\boldsymbol{\\Phi}' + \\mathbf{C}_\\nu)\\), where \\(\\mathbf{I}_T\\) is the \\(T\\)-dimensional identity matrix (see Note 4.1). So the independence-in-time assumption implies a simple separable spatio-temporal dependence structure. To model a more complex spatio-temporal dependence structure using spatial-only , one must specify the model for the random coefficients such that \\(\\{\\boldsymbol{\\alpha}_{t_j}: j=1,\\ldots,T\\}\\) are dependent in time. This is simplified by assuming conditional temporal dependence (dynamics) as discussed in Chapter 5.\n\n\n4.4.3 Random Effects with Temporal Basis Functions\nWe can also express the spatio-temporal random process in terms of temporal and spatially indexed random effects:\n\\[\nY(\\mathbf{s};t) = \\mathbf{x}(\\mathbf{s};t)' \\boldsymbol{\\beta}+ \\sum_{i=1}^{n_\\alpha} \\phi_{i}(t) \\alpha_i(\\mathbf{s}) + \\nu(\\mathbf{s};t),\n\\tag{4.31}\\]\nwhere \\(\\{\\phi_{i}(t): i=1,\\ldots,n_\\alpha;\\ t \\in D_t\\}\\) are temporal and \\(\\{\\alpha_i(\\mathbf{s})\\}\\) are their spatially indexed random coefficients. In this case, one could model \\(\\{\\alpha_i(\\mathbf{s}): \\mathbf{s}\\in D_s;\\ i=1,\\ldots,n_\\alpha\\}\\) using multivariate geostatistics. The temporal-basis-function representation given in Equation 4.31 is not as common in spatio-temporal statistics as the spatial-basis-function representation given in Equation 4.29. This is probably because most spatio-temporal processes have a scientific interpretation of spatial processes evolving in time. However, this need not be the case, and temporal are increasingly being used to model non-stationary-in-time processes (e.g., complex seasonal or high-frequency time behavior) that vary across space.\n\nExample Using Temporal Basis Functions\nSpatio-temporal modeling and prediction using temporal can be carried out with the package SpatioTemporal (see Lab 4.3). In the top panel of Figure 4.8 we show the three temporal used to model maximum temperature in the NOAA data set. These were obtained following a procedure similar to EOF analysis, which is described in Note 2.2. Note that the basis function \\(\\phi_1(t) = 1\\) is time-invariant.\nOnce \\(\\phi_1(t), \\phi_2(t)\\), and \\(\\phi_3(t)\\) are selected, estimates (e.g., ordinary least squares) of \\(\\alpha_1(\\mathbf{s}), \\alpha_2(\\mathbf{s})\\), and \\(\\alpha_3(\\mathbf{s})\\) can be found and used to indicate how they might be modeled. For example, in Lab 4.3 we see that while both \\(\\alpha_1(\\mathbf{s})\\) and \\(\\alpha_2(\\mathbf{s})\\) have a latitudinal trend, \\(\\alpha_3(\\mathbf{s})\\) does not. Assigning these fields exponential covariance functions, we obtain the models:\n\\[\nE(\\alpha_1(\\mathbf{s})) = \\alpha_{11} + \\alpha_{12}s_2, \\quad \\textrm{cov}(\\alpha_1(\\mathbf{s}), \\alpha_1(\\mathbf{s}+ \\mathbf{h})) = \\sigma^2_1 \\exp(-\\|\\mathbf{h}\\|/r_1),\n\\tag{4.32}\\]\n\\[\nE(\\alpha_2(\\mathbf{s})) = \\alpha_{21} + \\alpha_{22}s_2, \\quad \\textrm{cov}(\\alpha_2(\\mathbf{s}), \\alpha_2(\\mathbf{s}+ \\mathbf{h})) = \\sigma^2_2 \\exp(-\\|\\mathbf{h}\\|/r_2),\n\\tag{4.33}\\]\n\\[\nE(\\alpha_3(\\mathbf{s})) = \\alpha_{31}, \\quad\\qquad~~~~~~~ \\textrm{cov}(\\alpha_3(\\mathbf{s}), \\alpha_3(\\mathbf{s}+ \\mathbf{h})) = \\sigma^2_3 \\exp(-\\|\\mathbf{h}\\|/r_3),\n\\tag{4.34}\\]\nwhere \\(s_2\\) denotes the latitude coordinate at \\(\\mathbf{s}= (s_1, s_2)'\\), \\(r_1, r_2\\), and \\(r_3\\) are scale parameters, and \\(\\sigma^2_1, \\sigma^2_2\\), and \\(\\sigma^2_3\\) are stationary variances. We further assume that \\(\\textrm{cov}(\\alpha_k(\\mathbf{s}),\\alpha_{\\ell}(\\mathbf{s}')) = 0\\) for \\(k \\neq \\ell\\), which is a strong assumption.\nUsing maximum likelihood to estimate all unknown parameters and “plugging” the estimates in, the resulting prediction is the spatio-temporal smoothed map, \\(E(Y(\\cdot;\\cdot)\\mid \\mathbf{Z})\\), obtained from maps of \\(E(\\alpha_1(\\cdot)\\mid \\mathbf{Z}), E(\\alpha_2(\\cdot)\\mid \\mathbf{Z})\\), and \\(E(\\alpha_3(\\cdot)\\mid  \\mathbf{Z})\\), which can all be written in closed form. We show the first three basis-function times series in the top panel of Figure 4.8 and the predicted spatial maps (i.e., the basis-function coefficients), corresponding to these three basis functions in the bottom panel. Note how \\(E(\\alpha_1(\\cdot) \\mid  \\mathbf{Z})\\) picks up the latitude component evident in the NOAA maximum-temperature data. On the other hand, the fields \\(E(\\alpha_2(\\cdot) \\mid  \\mathbf{Z})\\) and \\(E(\\alpha_3(\\cdot)\\mid  \\mathbf{Z})\\) appear to capture oblique and longitudinal trends that have not been considered up to now, but with much smaller magnitudes. Although not shown here, these predictions of the basis-function coefficients \\(\\alpha_1(\\cdot)\\), \\(\\alpha_2(\\cdot)\\), and \\(\\alpha_3(\\cdot)\\) have associated uncertainties and those can be plotted as prediction standard-error maps as well.\n\n\n\n\n\n\n\nFigure 4.8: Top: Basis functions \\(\\phi_1(t), \\phi_2(t)\\), and \\(\\phi_3(t)\\), where the latter two were obtained from the left-singular vectors following a singular value decomposition of the data matrix. Bottom: \\(E(\\alpha_1(\\mathbf{s}) \\mid \\mathbf{Z})\\), \\(E(\\alpha_2(\\mathbf{s}) \\mid \\mathbf{Z})\\), and \\(E(\\alpha_3(\\mathbf{s}) \\mid \\mathbf{Z})\\).\n\n\n\n\n\n\n4.4.4 Confounding of Fixed Effects and Random Effects\nConsider the general mixed-effects representation given in Equation 4.28:\n\\[\n\\mathbf{Y}= \\mathbf{X}\\boldsymbol{\\beta}+ \\boldsymbol{\\Phi}\\boldsymbol{\\alpha}+ \\boldsymbol{\\nu},\\quad \\boldsymbol{\\nu}\\; \\sim \\; Gau({\\mathbf{0}}, \\mathbf{C}_\\nu),\n\\]\nand recall that \\(\\mathbf{Z}= \\mathbf{Y}+ \\boldsymbol{\\varepsilon}\\). Although the columns of \\(\\boldsymbol{\\Phi}\\) are , they are indexed in space and time in the same way that the columns of \\(\\mathbf{X}\\) are. Then, depending on the structure of the columns in these two matrices, it is quite possible that the random effects can be confounded with the fixed effects, similarly to the way extreme can affect the estimation of fixed effects in traditional regression (recall Section 3.2.2). This suggests that if primary interest is in inference on the fixed-effect parameters (\\(\\boldsymbol{\\beta}\\)), then one should mitigate potential associated with the random effects. As with , if the columns of \\(\\boldsymbol{\\Phi}\\) and \\(\\mathbf{X}\\) are linearly independent, then there is no concern about . This has led to mitigation strategies that tend to restrict the random effects by selecting in \\(\\boldsymbol{\\Phi}\\) that are orthogonal to the column space of \\(\\mathbf{X}\\) (or approximately so). If prediction of the hidden process \\(\\mathbf{Y}\\) is the primary goal, one is typically much less concerned about potential confounding.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Descriptive Spatio-Temporal Statistical Models</span>"
    ]
  },
  {
    "objectID": "Chapter4.html#sec-nonGaussian",
    "href": "Chapter4.html#sec-nonGaussian",
    "title": "4  Descriptive Spatio-Temporal Statistical Models",
    "section": "4.5 Non-Gaussian Data Models with Latent Gaussian Processes",
    "text": "4.5 Non-Gaussian Data Models with Latent Gaussian Processes\nThere is only one way to be Gaussian, but an infinite number of ways to be non-Gaussian! This is a challenge that we address in this section through the use of hierarchical statistical models. The modeling paradigm that we follow is to find a Gaussian process, possibly deep in the hierarchy, that describes the spatio-temporal behavior of a hidden process or of parameters that vary with space and time. The marginal distribution of the data is then non-Gaussian, but somewhere there is a Gaussian process that results in spatio-temporal dependence in the data through marginalization.\nThe examples presented thus far in this chapter have all assumed additive Gaussian error and random-effects distributions. Many spatio-temporal problems of interest deal with distinctly non-Gaussian data (e.g., counts, binary responses, extreme values). One of the most useful aspects of the hierarchical-modeling paradigm is that it allows one to accommodate fairly easily non-Gaussian data models, so long as the observations are conditionally independent, conditional on latent dependent Gaussian processes. This is the spatio-temporal manifestation of traditional generalized linear mixed models (GLMMs) and generalized additive mixed models (GAMMs) in statistics. That is, the likelihood assumes that the observations are conditionally independent given a spatio-temporal mean response that is some transformation of an additive mixed model. Our situation is a bit more flexible than the GLMM and GAMM in that our data model does not necessarily have to be from the exponential family, so long as we can allow conditional independence in the observations conditioned on spatio-temporal structure in the hidden process (and/or the associated process parameters).\nAs an example, consider a data model from the exponential family as in Section 3.4.1, such that\n\\[\nZ(\\mathbf{s};t) \\mid  Y(\\mathbf{s};t), \\gamma \\; \\sim \\; \\text{indep.} \\; EF(Y(\\mathbf{s};t), \\gamma), \\quad \\mathbf{s}\\in D_s,~t \\in D_t,\n\\tag{4.35}\\]\nwhere \\(EF\\) corresponds to a distribution from the exponential family with scale parameter \\(\\gamma\\) and mean \\(Y(\\mathbf{s};t)\\). In Section 3.4.1, we modeled a transformation of the mean response in terms of additive fixed effects (e.g., a linear combination of covariates). Here, we extend that and model the transformed mean response in terms of additive fixed effects and random effects,\n\\[\ng(Y(\\mathbf{s};t)) = \\mathbf{x}(\\mathbf{s};t)' \\boldsymbol{\\beta}+  \\eta(\\mathbf{s};t),\\quad \\mathbf{s}\\in D_s, t \\in D_t,\n\\]\nwhere \\(g(\\cdot)\\) is a specified monotonic link function, \\(\\mathbf{x}(\\mathbf{s};t)\\) is a \\(p\\)-dimensional vector of covariates for spatial location \\(\\mathbf{s}\\) and time \\(t\\), and \\(\\eta(\\mathbf{s};t)\\) is a spatio-temporal Gaussian random process that can be modeled either in terms of spatio-temporal covariances (as in Section 4.2), a special case of which uses a basis-function expansion (Section 4.3), or as a dynamic spatio-temporal process (Chapter 5). The same modeling issues associated with this latent Gaussian spatio-temporal process are present here as with the Gaussian-data case, but estimation of parameters and prediction of \\(Y(\\mathbf{s}_0;t_0)\\) are typically more involved given the non-Gaussian data model.\nAs an illustration, a simple model involving spatio-temporal count data could be represented by\n\\[\n\\mathbf{Z}_t \\mid  \\mathbf{Y}_t \\; \\sim \\; \\text{indep.} \\; \\text{Poi}(\\mathbf{Y}_t),\n\\]\n\\[\n\\log(\\mathbf{Y}_t) =  \\mathbf{X}_t \\boldsymbol{\\beta}+ \\boldsymbol{\\Phi}_t \\boldsymbol{\\alpha}_t + \\boldsymbol{\\nu}_t,\n\\]\nwhere \\(\\mathbf{Z}_t\\) is an \\(m_t\\)-dimensional data vector of counts at \\(m_t\\) spatial locations, \\(\\mathbf{Y}_t\\) represents the latent spatio-temporal mean process at \\(m_t\\) locations, \\(\\boldsymbol{\\Phi}_t\\) is an \\(m_t \\times n_\\alpha\\) matrix of \\(n_\\alpha\\) spatial , and the associated random coefficients are modeled as \\(\\boldsymbol{\\alpha}_t \\; \\sim \\; \\text{Gau}({\\mathbf{0}},\\mathbf{C}_\\alpha)\\), independent in time, with micro-scale error term \\(\\boldsymbol{\\nu}_t \\; \\sim \\; \\text{indep.} \\; \\text{Gau}({\\mathbf{0}},\\sigma^2_\\nu \\mathbf{I})\\); that is, \\(\\mathbf{C}_\\nu = \\sigma^2_\\nu \\mathbf{I}\\). As discussed in Section 4.4.2, it is often more realistic to consider temporal dependence through a dynamic model on \\(\\{\\boldsymbol{\\alpha}_t\\}\\), which will be explored in Chapter 5. As was the case for the Gaussian data models in Section 4.1–Section 4.4, the parameters \\(\\boldsymbol{\\beta}\\) and \\(\\boldsymbol{\\theta}\\) (in \\(\\mathbf{C}_\\alpha\\) and \\(\\mathbf{C}_\\nu\\)) could be estimated or a prior distribution could be put on them.\n\n4.5.1 Generalized Additive Models (GAMs)\nWe often seek more flexible models that can accommodate nonlinear structure in the mean function. Recall from Section 3.4.1 that one successful approach to this problem has been through the use of GAMs. In general, these models consider a transformation of the mean response to have an additive form in which the additive components are smooth functions (e.g., splines) of the covariates, where generally the functions themselves are expressed as basis-function expansions. In practical applications, the basis coefficients are treated as random coefficients in the estimation procedure. However, just as one can add random effects to generalized linear models (GLMs) to get generalized linear mixed models (GLMMs), one can also add (additional) random effects to GAMs to get generalized additive mixed models (GAMMs).\nFor example, consider data model (Equation 4.35). Similarly to Equation 3.12, we can write the transformed mean response additively as\n\\[\ng(Y(\\mathbf{s};t)) = \\mathbf{x}(\\mathbf{s};t)' \\boldsymbol{\\beta}+ \\sum_{i=1}^{n_f} f_i(\\mathbf{x}(\\mathbf{s};t);\\mathbf{s};t) + \\nu(\\mathbf{s};t),\n\\tag{4.36}\\]\nwhere again \\(g(\\cdot)\\) is a specified monotonic ; \\(\\mathbf{x}(\\mathbf{s};t)\\) is a \\(p\\)-dimensional vector of covariates for spatial location \\(\\mathbf{s}\\) and time \\(t\\); \\(f_i(\\cdot)\\) are functions of the covariates, the spatial locations, and the time index; and \\(\\nu(\\mathbf{s};t)\\) is a spatio-temporal random effect. Typically, the functions \\(f_i(\\cdot)\\) are modeled in terms of a truncated basis-function expansion; for example, \\(f_i(x_1(\\mathbf{s};t);\\mathbf{s};t) = \\sum_{k=1}^{q_i} \\phi_k(x_1(\\mathbf{s};t);\\mathbf{s};t) \\alpha_{ik}\\). Thus, we can see that the basis-function expansions with random coefficients given in Equation 4.27, Equation 4.29, and Equation 4.31 are essentially GAMMs. But, whereas in those models the smooth functions are typically only a function of spatio-temporal location, spatial location, or time, respectively, it is more common in the GAM/GAMM setting to allow the to also depend nonlinearly on covariates. On the other hand, GAM/GAMMs typically assume that the are smooth functions, whereas there is no such requirement for spatio-temporal-basis-function models. GAM/GAMMs can easily be implemented in R (e.g., we provide an example with the mgcv package in Lab 4.4).\n\n\n4.5.2 Inference for Spatio-Temporal Hierarchical Models\nImplicit in the estimation associated with the linear Gaussian spatio-temporal model discussed in Section 4.2.3 is that the covariance and fixed-effects parameters can be estimated more easily when we marginalize (integrate) out the latent Gaussian spatio-temporal process. In general, the likelihood is\n\\[\n[\\mathbf{Z}\\mid  \\boldsymbol{\\theta}, \\boldsymbol{\\beta}] = \\int [\\mathbf{Z}\\mid  \\mathbf{Y}, \\boldsymbol{\\theta}][\\mathbf{Y}\\mid  \\boldsymbol{\\theta}, \\boldsymbol{\\beta}] \\textrm{d}\\mathbf{Y},\n\\tag{4.37}\\]\nviewed as a function of \\(\\boldsymbol{\\theta}\\) and \\(\\boldsymbol{\\beta}\\). For linear mixed models (Section 4.1–Section 4.4), we assumed that the two distributions inside the integral in Equation 4.37 were Gaussian with linear relationships; this implied that the marginal likelihood function was in the form of a Gaussian density (e.g., Note 4.3), and thus can be written in closed form. More generally, we can relax the Gaussian assumption for the data model and, in the models presented here, the latent Gaussian spatio-temporal process \\(\\mathbf{Y}\\) is transformed through a nonlinear link function. This lack of Gaussianity and the presence of nonlinearity complicates the analysis, as generally the likelihood Equation 4.37 cannot be obtained in closed form.\nThe integral in Equation 4.37 can in principle be evaluated numerically, from which one can estimate the relatively few fixed effects and covariance parameters \\(\\{\\boldsymbol{\\beta}, \\boldsymbol{\\theta}\\}\\) through numerical optimization. In spatio-temporal models this is complicated by the high dimensionality of the integral; recall that \\(\\mathbf{Y}\\) is a \\((\\sum_{t=1}^T m_t)\\)-dimensional vector. Traditional approaches to this problem are facilitated by the usual conditional-independence assumption in the data model and by exploiting the latent Gaussian nature of the random effects. These approaches include methods such as Laplace approximation, quasi-likelihood, generalized estimating equations, pseudo-likelihood, and penalized quasi-likelihood. For example, recent advances in automatic differentiation have led to very efficient Laplace approximation approaches for performing inference with such likelihoods, even when there are a very large number of random effects (see, for example, the Template Model Builder (TMB) R package). Although these methods are increasingly being used successfully in the spatial context, there has tended to be more focus on Bayesian estimation approaches for spatio-temporal models in the literature. Either way, some type of approximation is needed (approximating the integrals, approximating the models using linearization, or approximating the posterior distribution through various Bayesian computational methods).\n\nBayesian Hierarchical Modeling\nThe hierarchical model (BHM) paradigm provides the estimation and inferential framework for many complex spatio-temporal models in the literature. Recall from Note 1.1 that we can decompose an arbitrary joint distribution in terms of a hierarchical sequence of conditional distributions and a marginal distribution; for example,\n\\[\n[A,B,C] = [A \\mid  B, C] [B \\mid  C][C].\n\\]\nIn the context of our general spatio-temporal model given in Section 4.2,\n\\[\n[\\mathbf{Z}, \\mathbf{Y},\\boldsymbol{\\beta},\\boldsymbol{\\theta}] = [\\mathbf{Z}\\mid  \\mathbf{Y}, \\boldsymbol{\\beta},\\boldsymbol{\\theta}][\\mathbf{Y}\\mid  \\boldsymbol{\\beta}, \\boldsymbol{\\theta}][\\boldsymbol{\\beta}\\mid  \\boldsymbol{\\theta}][\\boldsymbol{\\theta}]\n=  [\\mathbf{Z}\\mid  \\mathbf{Y}, \\boldsymbol{\\theta}_\\epsilon][\\mathbf{Y}\\mid  \\boldsymbol{\\beta}, \\boldsymbol{\\theta}_y][\\boldsymbol{\\theta}_\\epsilon][\\boldsymbol{\\theta}_y][\\boldsymbol{\\beta}],\n\\]\nwhere \\(\\boldsymbol{\\theta}\\) contains all of the variance and covariance parameters from the data model and the process model. Note that the first equality is based on the probability decomposition and the second equality is based on writing \\(\\boldsymbol{\\theta}=\\{\\boldsymbol{\\theta}_\\epsilon, \\boldsymbol{\\theta}_y\\}\\) and assuming that \\(\\boldsymbol{\\beta}\\), \\(\\boldsymbol{\\theta}_\\epsilon\\), and \\(\\boldsymbol{\\theta}_y\\) are independent a priori. Now, Bayes’ Rule implies that\n\\[\n[\\mathbf{Y}, \\boldsymbol{\\beta}, \\boldsymbol{\\theta}\\mid  \\mathbf{Z}] \\propto [\\mathbf{Z}\\mid  \\mathbf{Y}, \\boldsymbol{\\theta}_\\epsilon][\\mathbf{Y}\\mid  \\boldsymbol{\\beta}, \\boldsymbol{\\theta}_y][\\boldsymbol{\\beta}][\\boldsymbol{\\theta}_\\epsilon][\\boldsymbol{\\theta}_y].\n\\tag{4.38}\\]\nFor example, in the linear Gaussian case, \\([\\mathbf{Z}\\mid  \\mathbf{Y}, \\boldsymbol{\\theta}_\\epsilon]\\) is given by Equation 4.3 and \\([\\mathbf{Y}\\mid  \\boldsymbol{\\beta}, \\boldsymbol{\\theta}_y]\\) is given by Equation 4.4. The prior distributions \\([\\boldsymbol{\\beta}]\\), \\([\\boldsymbol{\\theta}_\\epsilon]\\), and \\([\\boldsymbol{\\theta}_y]\\) are then specified according to the particular modeling choices made.\nIf we are interested in inference on the parameters, then we focus on the posterior distribution, \\([\\boldsymbol{\\beta}, \\boldsymbol{\\theta}\\mid \\mathbf{Z}]\\); if our interest is in prediction, we focus on the predictive distribution, \\([\\mathbf{Y}\\mid \\mathbf{Z}]\\). In principle, we can obtain these posterior distributions if we can evaluate the normalizing constant in Equation 4.38, which is a function of the data \\(\\mathbf{Z}\\), specifically, the marginal distribution \\([\\mathbf{Z}]\\). However, in the general spatio-temporal case (and in most hierarchical models) there is no analytical form for this normalizing constant, and one must use numerical approximations. A common and useful approach is to use Markov chain Monte Carlo (MCMC) techniques to obtain (Markov dependent) Monte Carlo (MC) samples from the posterior distribution and then to perform inference on the parameters and prediction of the hidden process by summarizing these MC samples (see Note 4.5 for a basic Gibbs sampler MCMC algorithm). The advantage of the BHM approach is that parameter uncertainty is accounted for directly. But, there is no “free lunch,” and this usually comes at a cost of greater computational complexity.\nIn cases where the BHM computational complexity is formidable, one can sometimes find approximations that help simplify the computational burden. For example, just as penalized-quasi-likelihood methods use Laplace approximations to deal with the integral in Equation 4.37, the integrated nested Laplace approximation (INLA) approach is sometimes well suited for latent Gaussian spatial and spatio-temporal processes. The method exploits the Laplace approximation in Bayesian latent-Gaussian models and does not require generating samples from the posterior distribution. Hence, it can often be used for quite large data sets at reasonable computational expense. We use INLA to fit a latent separable spatio-temporal model in Lab 4.5.\nAnother way to mitigate the computational burden of a BHM is to obtain estimates \\(\\widehat{\\boldsymbol{\\theta}}\\) of the parameters \\({\\boldsymbol{\\theta}}\\) outside of the fully Bayesian model as in empirical Bayesian estimation (e.g., Carlin & Louis, 2010). As mentioned in Chapter 1, Cressie & Wikle (2011) (pp. 23–24) call this approach empirical hierarchical modeling in the spatio-temporal context. In this case, one focuses on the “empirical predictive distribution,” \\([\\mathbf{Y}\\mid \\mathbf{Z}, \\widehat{\\boldsymbol{\\theta}}]\\). The primary example of this in spatio-temporal statistics is S-T kriging as discussed in Section 4.2. That is, rather than assigning prior distributions to the parameters, they are estimated and the estimates are “plugged in” to the closed-form kriging formulas. This typically has the advantage of substantially less computational burden but at a cost of overly liberal uncertainty quantification. Ideally, one should take additional steps to account for the uncertainty associated with using these plug-in estimates (e.g., via the bootstrap).\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote 4.5: Basic Gibbs Sampler MCMC Algorithm\n\n\n\nConsider the joint posterior distribution of \\(K\\) random variables, \\(w_1, \\ldots, w_K\\), given data, \\(\\mathbf{Z}\\), which we denote as \\([w_1, \\ldots, w_K \\; | \\; \\mathbf{Z}]\\). As is typical, assume that we do not know the normalizing constant for this posterior distribution. Markov chain Monte Carlo (MCMC) approaches can be used to obtain samples from such distributions indirectly. Specifically, rather than compute the posterior distribution directly, one computes successive simulations from a Markov chain constructed so that samples from the stationary distribution of this chain are equivalent to samples from the target posterior distribution. That is, after some “burn-in” time, samples of the chain are viewed as samples simulated from the posterior distribution. Note that these samples are statistically dependent. The posterior distribution can be explored by various Monte Carlo summaries of the MCMC samples.\nOne of the simplest MCMC algorithms is the Gibbs sampler, which is most appropriate when the distributions of each of the random variables conditioned on all of the others and the data (the “full-conditional” distributions) are available in closed form. For a basic overview, see Gelman et al. (2014). A generic Gibbs sampler algorithm is given below.\nAn initial step in the Gibbs sampler algorithm is to derive all of the full conditional distributions in closed form. That is, derive\n\\[\n[w_1 | w_2,\\ldots,w_K, \\mathbf{Z}], \\; [w_2 | w_1, w_3,\\ldots,w_K, \\mathbf{Z}], \\ldots,  [w_K | w_1, w_2,\\ldots,w_{K-1}, \\mathbf{Z}].\n\\]\n\nObtain starting values: \\(\\{w_1^{(0)}, \\ldots, w_K^{(0)}\\}\\)\nfor \\(i=1,2,\\ldots, N_{\\mathrm{gibbs}}\\) do\n\n\nSample \\(w_1^{(i)} \\sim [w_1 | w_2^{(i-1)}, \\ldots, w_K^{(i-1)}, \\mathbf{Z}]\\)\n\n\nSample \\(w_2^{(i)} \\sim [w_2 | w_1^{(i)}, w_3^{(i-1)}, \\ldots, w_K^{(i-1)}, \\mathbf{Z}]\\)\n\n…\nK. Sample \\(w_K^{(i)} \\sim [w_K | w_1^{(i)}, \\ldots, w_{K-1}^{(i)}, \\mathbf{Z}]\\)\n\nend for\nDiscard the first \\(b\\) “burn-in” samples and use the remaining \\(b+1,\\ldots,N_{\\mathrm{gibbs}}\\) samples as though they are coming from the posterior distribution \\([w_1,\\ldots,w_K | \\mathbf{Z}]\\).\n\nNote that this is one of the most basic MCMC algorithms. Many modifications exist to improve efficiency and deal with the common case where the full conditional distributions are not available in closed form (see, for example, Gelman et al., 2014).",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Descriptive Spatio-Temporal Statistical Models</span>"
    ]
  },
  {
    "objectID": "Chapter4.html#sec-Chap4wrapup",
    "href": "Chapter4.html#sec-Chap4wrapup",
    "title": "4  Descriptive Spatio-Temporal Statistical Models",
    "section": "4.6 Chapter 4 Wrap-Up",
    "text": "4.6 Chapter 4 Wrap-Up\nTime marches forward, but it can be valuable to look back at a changing landscape over a period of time. We can describe how space and time interact using spatio-temporal mean and covariance functions, without having to commit to a mechanistic model that expresses the interaction dynamically. Hence, in this chapter we considered spatio-temporal modeling using what we have called the “descriptive” approach. Importantly, we made a clear distinction between the data and the underlying that represents the real-world process upon which measurements were taken. That is, we need to think conditionally! Thus, we considered a data model where the conditional distribution was Gaussian and where the conditional distribution was non-Gaussian. In both cases, we conditioned on a latent Gaussian spatio-temporal process.\nWe also considered the latent spatio-temporal Gaussian process by specifying the first-order (mean) structure in terms of exogenous covariates (including functions of locations of space or time) and the second-order dependence in terms of spatio-temporal covariance functions. We discussed various assumptions for such models related to stationarity, separability, and full symmetry. These sorts of representations are ideally suited for problems where there are not too many observations or locations in time and space at which one wants to predict, and where either we feel comfortable that we know the dependence structure (and can represent it by covariance functions), or we just want to account for dependence and do not care so much that the model is not all that realistic. In situations with large data sets and/or large numbers of prediction locations, it is often more efficient computationally to consider random-effects representations of the second-order structure using basis-function expansions. The basis-function construction also frees the modeler from having to develop valid spatio-temporal covariance functions, as our conditional basis-function random effects induce a valid marginal covariance function. We considered this from the perspective of that are defined in space and time, in space only, and in time only. The descriptive-modeling framework is similar for each. In addition, we briefly showed how these spatio-temporal mixed models using are related to GAM/GAMMs, depending on the choice of and the estimation approach. An overview of GAMs can be found in Wood (2017).\nA potential issue with performing parameter inference in descriptive models with spatial or spatio-temporal random effects is the problem of . Traditionally, this has not been as big a concern in spatial and spatio-temporal statistics because the focus has been on prediction. But, as these methods have increasingly been used to account for dependence when interpreting fixed effects, has received much more attention Hodges & Reich (2010); Hughes & Haran (2013); Hanks et al. (2015).\nAn overview of computation for spatial and spatio-temporal descriptive models is presented in Diggle & Ribeiro Jr. (2007) and Banerjee et al. (2015). The INLA approximate- methodology is discussed in Rue et al. (2009), Lindgren et al. (2011), Blangiardo & Cameletti (2015), and Krainski et al. (2019). Descriptive models that can be formulated using simple dynamic equations in a Bayesian framework can also be implemented using spTimer Bakar & Sahu (2015) and the function spDynLM in spBayes Finley et al. (2007). Computational methods for non-Bayesian approaches to non-Gaussian spatial data can be found in Schabenberger & Gotway (2005). An overview on using R to perform some exploratory and geostatistical modeling for spatio-temporal data can be found in RESSTE Network et al. (2017).\nThere are a number of informative books on spatio-temporal statistical methodology. These include Le & Zidek (2006), Cressie & Wikle (2011), Sherman (2011), Blangiardo & Cameletti (2015), Diggle (2013), Mateu & Müller (2013), Baddeley et al. (2015), Banerjee et al. (2015), Montero et al. (2015), Shaddick & Zidek (2015), and Christakos (2017).\nOne of the most challenging aspects of characterizing the spatio-temporal dependence structure, from either the marginal-covariance-model perspective or the conditional-basis-function perspective, is the ability to model real-world interactions that occur across time and space. In that case, the underlying processes are often best described by spatial fields that evolve through time according to “rules” that govern the spatio-temporal variability. That is, they represent a dynamical system. As we shall see in Chapter 5, spatio-temporal models that explicitly account for these dynamics offer the benefit of providing more realistic models in general, and they can simplify model construction and estimation through conditioning.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Descriptive Spatio-Temporal Statistical Models</span>"
    ]
  },
  {
    "objectID": "Chapter4.html#lab-4.1-spatio-temporal-kriging-with-gstat",
    "href": "Chapter4.html#lab-4.1-spatio-temporal-kriging-with-gstat",
    "title": "4  Descriptive Spatio-Temporal Statistical Models",
    "section": "Lab 4.1: Spatio-Temporal Kriging with gstat",
    "text": "Lab 4.1: Spatio-Temporal Kriging with gstat\nIn this Lab we go through the process of carrying out spatio-temporal universal kriging using the semivariogram with the package gstat. We focus on the maximum temperature data in the NOAA data set (Tmax) in July 1993. In addition to the packages used in Chapter 2 for data wrangling, we need RColorBrewer to color some of the surfaces that will be produced.\n\nlibrary(\"sp\")\nlibrary(\"spacetime\")\nlibrary(\"ggplot2\")\nlibrary(\"dplyr\")\nlibrary(\"gstat\")\nlibrary(\"RColorBrewer\")\nlibrary(\"STRbook\")\nlibrary(\"tidyr\")\n\nFor S-T kriging of the maximum-temperature data set in July 1993, we need to fit a parametric function to the empirical semivariogram vv computed in Lab 2.3. The code is reproduced below for completeness.\n\ndata(\"STObj3\", package = \"STRbook\")\nSTObj4 &lt;- STObj3[, \"1993-07-01::1993-07-31\"]\nvv &lt;- variogram(object = z ~ 1 + lat, # fixed effect component\n                data = STObj4,      # July data\n                width = 80,         # spatial bin (80 km)\n                cutoff = 1000,      # consider pts &lt; 1000 km apart\n                tlags = 0.01:6.01)  # 0 days to 6 days\n\nA number of covariance-function models are available with the package gstat; see the gstat vignette “spatio-temporal-kriging” for details by typing\n\nvignette(\"spatio-temporal-kriging\")\n\nThe first semivariogram we consider here corresponds to the spatio-temporal separable covariance function in Equation 4.18 and Equation 4.19. Observe from the vignette that a separable covariance function Equation 4.18 corresponds to a semivariogram of the form\n\\[\n\\gamma^{\\mathrm{sep}}(\\mathbf{h}; \\tau) = \\mathrm{sill}\\cdot\\left(\\bar\\gamma^{(s)}(\\| \\mathbf{h}\\|) + \\bar\\gamma^{(t)}(|\\tau|) - \\bar\\gamma^{(s)}(\\| \\mathbf{h}\\|)\\bar\\gamma^{(t)}(|\\tau|)\\right),\n\\]\nwhere the “standardized” semivariograms \\(\\bar\\gamma^{(s)}\\) and \\(\\bar\\gamma^{(t)}\\) have separate nugget effects and sills equal to 1.\nA spatio-temporal semivariogram is constructed with gstat using the function vgmST. The argument stModel = \"separable\" is used to define a separable model, while the function vgm is used to construct the individual semivariograms (one for space and one for time). Several arguments can be passed to vgm. The first four, which we use below, correspond to the partial sill, the model type, the range, and the nugget, respectively. The argument sill that is supplied to vgmST defines the joint spatio-temporal sill. The numbers used in their definition are initial values supplied to the optimization routine used for fitting in the function fit.StVariogram, which fits sepVgm to vv. These initial values should be reasonable – for example, the length scale \\(\\phi\\) can be set to a value that spans 10% of the spatial/temporal domain, and the variances/sills can be set such that they have similar orders of magnitude to the total variance of the measurements.\n\nsepVgm &lt;- vgmST(stModel = \"separable\",\n                space = vgm(10, \"Exp\", 400, nugget = 0.1),\n                time = vgm(10, \"Exp\", 1, nugget = 0.1),\n                sill = 20)\nsepVgm &lt;- fit.StVariogram(vv, sepVgm)\n\nThe second model we fit has the covariance function given in Equation 4.20. For this model, the function vgmST takes the joint semivariogram as an argument, as well as the sill (sill) and the scaling factor (stAni), denoted by \\(a\\) in \\(\\mathbf{v}\\), defined just below Equation 4.20. This parameter can be initially set by considering orders of magnitudes – if the spatial field is evolving on scales of the order of hundreds of kilometers and the temporal evolution has a scale on the order of days, then an initial value of stAni = 100 is reasonable.\n\nmetricVgm &lt;- vgmST(stModel = \"metric\",\n                   joint = vgm(100, \"Exp\", 400, nugget = 0.1),\n                   sill = 10,\n                   stAni = 100)\nmetricVgm &lt;- fit.StVariogram(vv, metricVgm)\n\nWe can compare the fits of the two semivariograms by checking the mean squared error of the fits. These can be found by directly accessing the final function value of the optimizer used by fit.StVariogram.\n\nmetricMSE &lt;- attr(metricVgm, \"optim\")$value\nsepMSE &lt;- attr(sepVgm, \"optim\")$value\n\nHere the variable metricMSE is 2.1 while sepMSE is 1.4, indicating that the separable semivariogram gives a better fit to the empirical semivariogram in this case. The fitted semivariograms can be plotted using the standard plot function.\n\nplot(vv, list(sepVgm, metricVgm), main = \"Semi-variance\")\n\nContour plots of the fitted variograms are shown in the bottom panels of Figure 4.4. The corresponding stationary S-T covariance function is obtained from Equation 4.15.\nNext, we use the fitted S-T covariance models for prediction using S-T kriging, in this case universal S-T kriging since we are treating the latitude coordinate as a covariate. First, we need to create a space-time prediction grid. For our spatial grid, we consider 20 spatial locations between 100°W and 80°W, and 20 spatial locations between 32°N and 46°N. In the code below, when converting to SpatialPoints, we ensure that the coordinate reference system (CRS) of the prediction grid is the same as that of the observations.\n\nspat_pred_grid &lt;- expand.grid(\n                      lon = seq(-100, -80, length = 20),\n                      lat = seq(32, 46, length = 20)) %&gt;%\n            SpatialPoints(proj4string = CRS(proj4string(STObj3)))\ngridded(spat_pred_grid) &lt;- TRUE\n\nFor our temporal grid, we consider six equally spaced days in July 1993.\n\ntemp_pred_grid &lt;- as.Date(\"1993-07-01\") + seq(3, 28, length = 6)\n\nWe can then combine spat_pred_grid and temp_pred_grid to construct an STF object for our space-time prediction grid.\n\nDE_pred &lt;- STF(sp = spat_pred_grid,    # spatial part\n               time = temp_pred_grid)  # temporal part\n\nSince there are missing observations in STObj4, we first need to cast STObj4 into either an STSDF or an STIDF, and remove the data recording missing observations. For simplicity here, we consider the STIDF (considering STSDF would be around twice as fast). Also, in order to show the capability of S-T kriging to predict across time, we omitted data on 14 July 1993 from the data set.\n\nSTObj5 &lt;- as(STObj4[, -14], \"STIDF\")         # convert to STIDF\nSTObj5 &lt;- subset(STObj5, !is.na(STObj5$z))   # remove missing data\n\nNow we can call krigeST using STObj5 as our data.\n\npred_kriged &lt;- krigeST(z ~ 1 + lat,         # latitude trend\n                       data = STObj5,       # data set w/o 14 July\n                       newdata = DE_pred,   # prediction grid\n                       modelList = sepVgm,  # semivariogram\n                       computeVar = TRUE)   # compute variances\n\nTo plot the predictions and accompanying prediction standard errors, it is straightforward to use the function stplot. First, we define our color palette using the function brewer.pal and the function colorRampPalette (see help files for details on what these functions do).\n\ncolor_pal &lt;- rev(colorRampPalette(brewer.pal(11, \"Spectral\"))(16))\n\nSecond, we call the stplot function with the object containing the results.\n\nstplot(pred_kriged,\n       main = \"Predictions (degrees Fahrenheit)\",\n       layout = c(3, 2),\n       col.regions = color_pal)\n\nThe prediction (kriging) standard errors can be plotted in a similar way.\n\npred_kriged$se &lt;- sqrt(pred_kriged$var1.var)\nstplot(pred_kriged[, , \"se\"],\n       main = \"Prediction std. errors (degrees Fahrenheit)\",\n       layout = c(3, 2),\n       col.regions = color_pal)\n\nSpatio-temporal kriging as shown in this Lab is relatively quick and easy to implement for small data sets, but it starts to become prohibitive as data sets grow in size, unless some approximation is used. For example, the function krigeST allows one to use the argument nmax to determine the maximum number of observations to use when doing prediction. The predictor is no longer optimal, but it is close enough to the optimal predictor in many cases of practical interest.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Descriptive Spatio-Temporal Statistical Models</span>"
    ]
  },
  {
    "objectID": "Chapter4.html#lab-4.2-spatio-temporal-basis-functions-with-frk",
    "href": "Chapter4.html#lab-4.2-spatio-temporal-basis-functions-with-frk",
    "title": "4  Descriptive Spatio-Temporal Statistical Models",
    "section": "Lab 4.2: Spatio-Temporal Basis Functions with FRK",
    "text": "Lab 4.2: Spatio-Temporal Basis Functions with FRK\nIn this Lab we shall focus on modeling the maximum temperature in July 1993 from data in the NOAA data set using spatio-temporal basis functions. The packages we need are the following:\n\nlibrary(\"dplyr\")\nlibrary(\"FRK\")\nlibrary(\"ggplot2\")\nlibrary(\"gstat\")\nlibrary(\"RColorBrewer\")\nlibrary(\"sp\")\nlibrary(\"spacetime\")\nlibrary(\"STRbook\")\nlibrary(\"tidyr\")\n\nThe package FRK implements a low-rank approach to spatial and spatio-temporal modeling known as fixed rank kriging (FRK). FRK considers the random-effects model Equation 4.27, sometimes known as the spatio-temporal random-effects model (Cressie et al., 2010), and provides functionality to the user for choosing the basis functions \\(\\{\\phi_i(\\mathbf{s};t) : i = 1,\\dots,n_\\alpha\\}\\) from the data.\nA key difference between FRK and other geostatistical packages is that, in FRK, modeling and prediction are carried out on a fine, regular discretization of the spatio-temporal domain. The small grid cells are known as basic areal units (BAUs), and their primary utility is to account for problems of change of support (varying measurement footprint), which we do not consider in this Lab. The package is loaded by typing in the console\n\nlibrary(\"FRK\")\n\nFor spatio-temporal modeling and prediction, FRK requires the user to provide the point-level data as objects of class STIDF. Hence, for this exercise, we use STObj5 from Lab 3.1, which we reconstruct below (for completeness) from STObj3.\n\ndata(\"STObj3\", package = \"STRbook\")          # load STObj3\nSTObj4 &lt;- STObj3[, \"1993-07-01::1993-07-31\"] # subset time\nSTObj5 &lt;- as(STObj4[, -14], \"STIDF\")         # omit t = 14\nSTObj5 &lt;- subset(STObj5, !is.na(STObj5$z))   # remove NAs\nproj4string(STObj5) &lt;- CRS(\"+proj=longlat +ellps=WGS84 +no_defs\")\n\nThe spatio-temporal BAUs are constructed using the function auto_BAUs which takes several arguments, as shown below and detailed using the in-line comments. For more details see help(auto_BAUs). Note that as cellsize we chose c(1, 0.75, 1) which indicates a BAU size of 1 degree longitude \\(\\times\\) 0.75 degrees latitude \\(\\times\\) 1 day – this choice ensures that the BAUs are similar to the prediction grid used in Lab 3.1. The argument convex is an “extension radius” used in domain construction via the package INLA. See the help file of inla.nonconvex.hull for details.\n\nBAUs &lt;- auto_BAUs(manifold = STplane(),   # ST field on the plane\n                  type = \"grid\",          # gridded (not \"hex\")\n                  data = STObj5,          # data\n                  cellsize = c(1, 0.75, 1), # BAU cell size\n                  convex = -0.12,           # hull extension\n                  tunit = \"days\")           # time unit is \"days\"\n\nThe BAUs are of class STFDF since they are three-dimensional pixels arranged regularly in both space and in time. To plot the spatial BAUs overlaid with the data locations, we run\n\nplot(as(BAUs[, 1], \"SpatialPixels\"))    # plot pixel BAUs\nplot(SpatialPoints(STObj5),\n     add = TRUE, col = \"red\")           # plot data points\n\nThis generates the left panel of Figure 4.9. The BAUs, which we will also use as our prediction grid, overlap all the data points. The user has other options in BAU construction; for example, the following code generates hexagonal BAUs using a convex hull for a boundary.\n\nBAUs_hex &lt;- auto_BAUs(manifold = STplane(), # model on the plane\n                  type = \"hex\",             # hex (not \"grid\")\n                  data = STObj5,            # data\n                  cellsize = c(1, 0.75, 1), # BAU cell size\n                  nonconvex_hull = FALSE,   # convex hull\n                  tunit = \"days\")           # time unit is \"days\"\n\nPlotting proceeds in a similar fashion, except that the first line in the code chunk above now becomes\n\nplot(as(BAUs_hex[, 1], \"SpatialPolygons\"))\n\nThis allows for the fact that the BAUs are now (hexagonal) polygons and not rectangular pixels. The resulting plot is shown in the right panel of Figure 4.9.\n\n\n\n\n\n\nFigure 4.9: BAUs constructed for modeling and predicting maximum temperature from data in the NOAA data set. Left: Gridded BAUs arranged within a non-convex hull enclosing the data. Right: Hexagonal BAUs arranged within a convex hull enclosing the data.\n\n\n\nNext, we construct the basis functions \\(\\{\\phi_i(\\mathbf{s};t) : i = 1,\\dots,n_\\alpha\\}\\). In FRK, these are constructed by taking the tensor product of spatial basis functions with temporal basis functions. Specifically, consider a set of \\(r_s\\) spatial basis functions \\(\\{\\phi_{p}(\\mathbf{s}): p = 1,\\dots,r_s\\}\\), and a set of \\(r_t\\) temporal basis functions \\(\\{\\psi_{q}(t): q = 1,\\dots,r_t\\}\\). Then we construct the set of spatio-temporal basis functions as \\(\\{\\phi_{st,u}(s,t) : u = 1,\\dots,r_sr_t\\} = \\{\\phi_{p}(\\mathbf{s})\\psi_{q}(t) : p = 1,\\dots,r_s;\\ q = 1,\\dots,r_t\\}\\).\nThe generic basis function that FRK uses by default is the bisquare function (see Figure 4.7) given by\n\\[\nb(\\mathbf{s},\\mathbf{v}) \\equiv \\left\\{\\begin{array}{ll} \\{1 - (\\|\\mathbf{v}- \\mathbf{s}\\|/r)^2\\}^2, &\\| \\mathbf{v}-\\mathbf{s}\\| \\le r, \\\\\n0, & \\textrm{otherwise}, \\end{array} \\right.\n\\]\nwhere \\(r\\) is the aperture parameter. Basis functions can be either regularly placed or irregularly placed, and they are often multiresolutional. We choose two resolutions below, yielding \\(r_s = 94\\) spatial basis functions in total, and place them irregularly in the domain. (Note that \\(r_s\\) and the bisquare apertures are determined automatically by auto_basis.)\n\nG_spatial &lt;- auto_basis(manifold = plane(),      # fns on plane\n                        data = as(STObj5, \"Spatial\"), # project\n                        nres = 2,                     # 2 res.\n                        type = \"bisquare\",            # bisquare.\n                        regular = 0)                  # irregular\n\nTemporal basis functions also need to be defined. We use the function local_basis below to construct a regular sequence of \\(r_t = 20\\) bisquare basis functions between day 1 and day 31 of the month. Each of these bisquare basis functions is assigned an aperture of 2 days; that is, the support of each bisquare function is 4 days. The temporal grid is defined through\n\nt_grid &lt;- matrix(seq(1, 31, length = 20))\n\nThe basis functions are constructed using the following commands.\n\nG_temporal &lt;- local_basis(manifold = real_line(),  # fns on R1\n                          type = \"bisquare\",       # bisquare\n                          loc = t_grid,            # centroids\n                          scale = rep(2, 20))      # aperture par.\n\nFinally, we construct the \\(r_sr_t = 1880\\) spatio-temporal basis functions by taking the tensor product of the spatial and the temporal ones, using the function TensorP.\n\nG &lt;- TensorP(G_spatial, G_temporal)      # take the tensor product\n\nThe basis functions G_spatial and G_temporal can be visualized using the plotting function show_basis; see Figure 4.10. While the basis functions are of tensor-product form, the resulting S-T covariance function obtained from the spatio-temporal random effects model is not separable in space and time.\n\n\n\n\n\n\nFigure 4.10: Spatial and temporal basis functions used to construct the spatio-temporal basis functions. Left: Locations of spatial basis functions (circles denote spatial support). Right: Temporal basis functions.\n\n\n\nIn FRK, the fine-scale variation term at the BAU level, Equation 4.28, is assumed to be Gaussian with covariance matrix proportional to \\(\\textrm{diag}(\\{\\sigma^2_{\\nu,i}\\})\\), where \\(\\{\\sigma^2_{\\nu,i}: i = 1,\\dots,n_y\\}\\) are pre-specified at the BAU level (the constant of proportionality is then estimated by FRK). Typically, these are related to some geographically related quantity such as surface roughness. In our case, we simply set \\(\\sigma^2_{\\nu,i} = 1\\) for all \\(i\\).\n\nBAUs$fs = 1\n\nThe fine-scale variance at the BAU level is confounded with the measurement-error variance. In some cases, the measurement-error variance is known; when it is not (as in this case), one can carry out a simple analysis to estimate the value of the semivariogram at the origin. In this case, we simply assume that the nugget effect estimated when fitting the separable covariance function in Lab 4.1 is the measurement-error variance – any residual nugget component is then assumed to be the fine-scale variance introduced as a consequence of the low-rank approximation to the process. The measurement-error variance is specified in the std field in the data ST object.\n\nSTObj5$std &lt;- sqrt(0.049)\n\nThe response variable and covariates are identified through a standard R formula. In this case, we use latitude as a covariate and set\n\nf &lt;- z ~ lat + 1\n\nWe are now ready to call the main function FRK, which estimates all the unknown parameters in the models, including the covariance matrix of the basis-function coefficients and the fine-scale variance. We need to supply the formula, the data, the basis functions, the BAUs, and any other parameters configuring the expectation-maximization (EM) algorithm used for finding the maximum likelihood estimates. To reduce processing time, we have set the number of EM-algorithm steps to 3. Convergence of the EM algorithm can be assessed visually by setting print_lik = TRUE below.\n\nS &lt;- FRK(f = f,               # formula\n         data = list(STObj5), # (list of) data\n         basis = G,           # basis functions\n         BAUs = BAUs,         # BAUs\n         n_EM = 3,            # max. no. of EM iterations\n         tol = 0.01)          # tol. on change in log-likelihood\n\nOnce the model is fitted, prediction proceeds via the function predict. If the argument newdata is not specified, then prediction is done at all the BAUs.\n\ngrid_BAUs &lt;- predict(S)\n\nThe resulting object, grid_BAUs, is also of class STFDF, and plotting proceeds as per Lab 4.1 using the stplot function. The resulting predictions and prediction standard errors are illustrated in Figure 4.6.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Descriptive Spatio-Temporal Statistical Models</span>"
    ]
  },
  {
    "objectID": "Chapter4.html#lab-4.3-temporal-basis-functions-with-spatiotemporal",
    "href": "Chapter4.html#lab-4.3-temporal-basis-functions-with-spatiotemporal",
    "title": "4  Descriptive Spatio-Temporal Statistical Models",
    "section": "Lab 4.3: Temporal Basis Functions with SpatioTemporal",
    "text": "Lab 4.3: Temporal Basis Functions with SpatioTemporal\nIn this Lab we model the maximum temperature in the NOAA data set (Tmax) using temporal basis functions and spatial random fields. Specifically, we use the model\n\\[\nY(\\mathbf{s};t) = \\mathbf{x}(\\mathbf{s};t)' \\boldsymbol{\\beta}+ \\sum_{i=1}^{n_\\alpha} \\phi_{i}(t) \\alpha_i(\\mathbf{s}) + \\nu(\\mathbf{s};t),\n\\tag{4.39}\\]\nwhere \\(\\mathbf{x}(\\mathbf{s};t)\\) are the covariates; \\(\\boldsymbol{\\beta}\\) are the regression coefficients; \\(\\{\\phi_i(t)\\}\\) are the temporal basis functions; \\(\\{\\alpha_i(\\mathbf{s})\\}\\) are coefficients of the temporal basis functions, modeled as multivariate (spatial) random fields; and \\(\\nu(\\mathbf{s};t)\\) is a spatially correlated, but temporally independent, random process.\nSpatio-temporal modeling using temporal basis functions can be carried out using the package SpatioTemporal. For this Lab we require the following packages.\n\nlibrary(\"dplyr\")\nlibrary(\"ggplot2\")\nlibrary(\"gstat\")\nlibrary(\"RColorBrewer\")\nlibrary(\"sp\")\nlibrary(\"spacetime\")\nlibrary(\"SpatioTemporal\")\nlibrary(\"STRbook\")\nlibrary(\"tidyr\")\n\nThe space-time object used by SpatioTemporal is of class STdata and is created using the function createSTdata. This function takes the data either as a space-wide matrix with the row names containing the date and the column names the station ID, or as a data frame in long form. Here we use the latter. This data frame needs to have the station ID as characters in the field ID, the data in the field obs, and the date in the field date. A new data frame of this form can be easily created using the function transmute from the package dplyr.\n\ndata(\"NOAA_df_1990\", package = \"STRbook\")   # load NOAA data\nNOAA_sub &lt;- filter(NOAA_df_1990,       # filter data to only\n                   year == 1993 &      # contain July 1993\n                   month == 7 &\n                   proc == \"Tmax\")     # and just max. temp.\n\nNOAA_sub_for_STdata &lt;- NOAA_sub %&gt;%\n                       transmute(ID = as.character(id),\n                                 obs = z,\n                                 date = date)\n\nThe covariates that will be used to model the spatially varying effects also need to be supplied as a data frame. In our case we only consider the station coordinates as covariates. The station coordinates are extracted from the maximum temperature data as follows.\n\ncovars &lt;-  dplyr::select(NOAA_sub, id, lat, lon) %&gt;%\n           unique() %&gt;%\n           dplyr::rename(ID = id)      # createSTdata expects \"ID\"\n\nNow we can construct the STdata object by calling the function createSTdata.\n\nSTdata &lt;- createSTdata(NOAA_sub_for_STdata, covars = covars)\n\nThe model used in SpatioTemporal assumes that \\(\\nu(\\mathbf{s};t)\\) is temporally uncorrelated. Consequently, all temporal variability needs to be captured through the covariates or the basis functions. To check whether the data exhibit temporal autocorrelation (before adding any temporal basis functions), one can use the plot function. For example, we plot the estimated autocorrelation function for station 3812 in the left panel of Figure 4.11 (after the mean is removed from the data). The plot suggests that the data are correlated (the estimated lag-1 autocorrelation coefficient is larger than would be expected by chance at the 5% level of significance).\n\nplot(STdata, \"acf\", ID = \"3812\")\n\nThe role of the temporal basis functions is to adequately capture temporal modes of variation. When modeling data over a time interval that spans years, one of these is typically a seasonal component. As another example, when modeling trace-gas emissions, one basis function to use would be one that captures weekday/weekend cycles typically found in gaseous pollutants (e.g., due to vehicular traffic). The package SpatioTemporal allows for user-defined basis functions (see the example at the end of this Lab) or data-driven basis functions (which we consider now). In both cases, the first temporal basis function, \\(\\phi_1(t)\\), is a constant; that is, \\(\\phi_1(t) = 1\\).\nThe basis functions extracted from the data are smoothed, left singular vectors (i.e., smoothed temporal EOFs) of the matrix \\(\\widetilde{\\mathbf{Z}}\\), described in Note 2.2. These make up the remaining \\(n_\\alpha - 1\\) basis functions, upon which smoothing is carried out using splines. In SpatioTemporal, these basis functions are found (or set) using the function updateTrend.\n\nSTdata &lt;- updateTrend(STdata, n.basis = 2)\n\nWe can see that the lag-1 autocorrelation coefficient is no longer significant (at the 5% level) after adding in these basis functions; see the right panel of Figure 4.11. In practice, one should add basis functions until temporal autocorrelation in the data (at most stations) is considerably reduced. In this case study, it can be shown that 69% of stations record maximum temperature data that have lag-1 autocorrelation coefficients that are significant at the 5% level. On the other hand, with n.basis = 2 (i.e., with two temporal basis functions for capturing temporal variation), the proportion of stations with residuals exhibiting a significant lag-1 autocorrelation coefficient is 26%.\n\nplot(STdata, \"acf\", ID = \"3812\")\n\n\n\n\n\n\n\nFigure 4.11: Left: Estimated autocorrelation function for the time series of maximum temperature Tmax at Station 3812. Right: Same as left panel, but with the data first detrended using an intercept and the two temporal basis functions shown in the top panel of Figure 4.8.\n\n\n\nThe basis functions, available in STdata$trend, are shown in the top panel of Figure 4.8.\nIn SpatioTemporal, the spatial quantities \\(\\{\\alpha_i(\\mathbf{s})\\}\\) are themselves modeled as spatial fields. Once the \\(\\{\\phi_i(t)\\}\\) are declared, empirical estimates of \\(\\{\\alpha_i(\\mathbf{s})\\}\\) can be found using the function estimateBetaFields. Note that we use the Greek letter “alpha” to denote these fields, which differs from the name “Beta” inside the command. The following and all subsequent references to “Beta” and “beta” should be interpreted as representing spatial fields \\(\\{\\alpha_i(\\mathbf{s})\\}\\).\n\nbeta.lm &lt;- estimateBetaFields(STdata)\n\nThe resulting object, beta.lm, contains two fields; beta (estimated coefficients) and beta.sd (standard error of the estimates) with row names equal to the station ID, and three columns corresponding to estimates of \\(\\alpha_1(\\mathbf{s})\\), \\(\\alpha_2(\\mathbf{s})\\), and \\(\\alpha_3(\\mathbf{s})\\), respectively. We are interested in seeing whether the empirical estimates are correlated with our covariate, latitude. To this end, the authors of SpatioTemporal suggest using the package plotrix, and the function plotCI, to plot the estimates and covariance intervals against a covariate of choice. When plotting using plotCI, care should be taken that the ordering of the stations in beta and beta.sd is the same as that if the covariate data frame. For example, consider\n\nhead(row.names(beta.lm$beta))\n\n[1] \"13865\" \"13866\" \"13871\" \"13873\" \"13874\" \"13876\"\n\nhead(covars$ID)\n\n[1] 3804 3810 3811 3812 3813 3816\n\n\nThis illustrates a discrepancy, since the ordering of strings is not necessarily that of the ordered integers. For this reason, we recommend employing best practice and always merging (e.g., using left_join) on a column variable; in this case, we choose the integer version of the field ID. In the following commands, we first convert the beta and beta.sd objects into data frames, add the column ID, join into a data frame BETA, and then combine with covars containing the latitude data.\n\nbeta.lm$beta &lt;- data.frame(beta.lm$beta)\nbeta.lm$beta.sd &lt;- data.frame(beta.lm$beta.sd)\nbeta.lm$beta$ID &lt;- as.integer(row.names(beta.lm$beta))\nBETA &lt;- cbind(beta.lm$beta, beta.lm$beta.sd)\ncolnames(BETA) &lt;- c(\"alpha1\", \"alpha2\", \"alpha3\", \"ID\",\n                    \"alpha1_CI\", \"alpha2_CI\", \"alpha3_CI\")\nBETA &lt;- left_join(BETA, covars, by = \"ID\")\n\nOnce BETA is constructed, the empirical estimates can be plotted using ggplot, with geom_errorbar to also plot error bars, as follows.\n\nggplot(BETA) + geom_point(aes(x = lat, y = alpha1)) +\n    geom_errorbar(aes(x = lat,\n                      ymin = alpha1 - 1.96*alpha1_CI,\n                      ymax = alpha1 + 1.96*alpha1_CI)) +\n    ylab(expression(alpha[1](s))) +\n    xlab(\"lat (deg)\") + theme_bw()\n\nThe three empirical estimates, plotted as a function of latitude, are shown in Figure 4.12. The function \\(\\alpha_1(\\mathbf{s})\\) exhibits a strong latitudinal trend, as expected; \\(\\alpha_2(\\mathbf{s})\\) shows a weak latitudinal trend; and \\(\\alpha_3(\\mathbf{s})\\) exhibits no trend. For this reason, we model the expectations of these fields as Equation 4.32–Equation 4.34, where \\(s_2\\) denotes the latitude coordinate at \\(\\mathbf{s}= (s_1, s_2)'\\). Note that in this model we do not consider any spatio-temporal covariates, and hence the term \\(\\mathbf{x}(\\mathbf{s};t)'\\boldsymbol{\\beta}= 0\\) in Equation 4.39. This does not mean that we do not have an intercept in our model: although it is random, the spatial field \\(\\alpha_1(\\mathbf{s})\\) acts as a temporally invariant spatial covariate and includes a global space-time mean (\\(\\alpha_{11}\\) in Equation 4.32), which is estimated.\n\n\n\n\n\n\nFigure 4.12: Empirical estimates of \\(\\alpha_1(\\mathbf{s})\\), \\(\\alpha_2(\\mathbf{s})\\), and \\(\\alpha_3(\\mathbf{s})\\) at each station, with 95% confidence intervals, plotted as a function of latitude.\n\n\n\nWe let the covariance functions \\(\\textrm{cov}(\\alpha_i(\\mathbf{s}),\\alpha_i(\\mathbf{s}+ \\mathbf{h}))\\), \\(i = 1,2,3\\), be exponential covariance functions without a nugget-effect term. In SpatioTemporal these are declared as follows.\n\ncov.beta &lt;- list(covf = \"exp\", nugget = FALSE)\n\nAll that remains for constructing the spatio-temporal model is to define the spatial covariance function of the zero-mean, temporally independent, residual process \\(\\nu(\\mathbf{s};t)\\); see Equation 4.39. We choose this to be an exponential covariance function with a nugget effect to account for measurement error. The argument random.effect = FALSE is used to indicate that there is no random mean offset for the field at each time point.\n\ncov.nu &lt;- list(covf = \"exp\",\n               nugget = ~1,\n               random.effect = FALSE) # No random mean\n                                      # for each nu\n\nThe function to create the spatio-temporal model is createSTmodel. This takes as data the object STdata, the covariates for the \\(\\alpha\\)-fields (an intercept and latitude for \\(\\alpha_1(\\mathbf{s})\\) and \\(\\alpha_2(\\mathbf{s})\\), and just an intercept for \\(\\alpha_3(\\mathbf{s})\\); see Equation 4.32–Equation 4.34), the covariance functions of the \\(\\alpha\\)-fields and the \\(\\nu\\)-field, and a list containing the names of station coordinate fields (lon and lat).\n\nlocations &lt;- list(coords = c(\"lon\", \"lat\"))\nLUR &lt;- list(~lat, ~lat, ~1)  # lat trend for phi1 and phi2 only\nSTmodel &lt;- createSTmodel(STdata,              # data\n                         LUR = LUR,           # spatial covariates\n                         cov.beta = cov.beta, # cov. of alphas\n                         cov.nu = cov.nu,     # cov. of nu\n                         locations = locations) # coord. names\n\nIn order to fit the spatio-temporal model to the data, we need to provide initial values of the parameter estimates. The required parameter names can be extracted using the function loglikeSTnames and, for our model, are as follows.\n\nparnames &lt;- loglikeSTnames(STmodel, all = FALSE)\nprint(parnames)\n\n[1] \"log.range.const.exp\"           \"log.sill.const.exp\"           \n[3] \"log.range.V1.exp\"              \"log.sill.V1.exp\"              \n[5] \"log.range.V2.exp\"              \"log.sill.V2.exp\"              \n[7] \"nu.log.range.exp\"              \"nu.log.sill.exp\"              \n[9] \"nu.log.nugget.(Intercept).exp\"\n\n\nNoting that all parameters are log-transforms of the quantities of interest, we let all of the initial values be equal to 3 (so that all initial ranges and sills are \\(e^3 \\approx 20\\)). This seems reasonable when the temperature is varying on the order of several degrees Fahrenheit, and where the domain also spans several degrees (in latitude and longitude).\nWe use the function estimate below to fit the spatio-temporal model to the data. This may take several minutes on a standard desktop computer. In this instance, the resulting object SpatioTemporalfit1 has been pre-computed and can be loaded directly from STRbook by typing data(\"SpatioTemporalfit1\", package = \"STRbook\").\n\nx.init &lt;- matrix(3, 9, 1)\nrownames(x.init) &lt;- loglikeSTnames(STmodel, all = FALSE)\nSpatioTemporalfit1 &lt;- estimate(STmodel, x.init)\n\nThe fitted coefficients for the parameters described by parnames above can be extracted from the fitted object using the function coef.\n\nx.final &lt;- coef(SpatioTemporalfit1, pars = \"cov\")$par\n\nHaving fitted the model, we now predict at unobserved locations. First, we establish the spatial and temporal grid upon which to predict; this proceeds by first initializing an STdata object on a grid. We construct the grid following a very similar approach to what was done in Lab 4.1.\n\n## Define space-time grid\nspat_pred_grid &lt;- expand.grid(lon = seq(-100, -80, length = 20),\n                      lat = seq(32, 46, length = 20))\nspat_pred_grid$id &lt;- 1:nrow(spat_pred_grid)\ntemp_pred_grid &lt;- as.Date(\"1993-07-01\") + seq(3, 28, length = 6)\n\n## Initialize data matrix\nobs_pred_wide &lt;- matrix(0, nrow = 6, ncol = 400)\n\n## Set row names and column names\nrownames(obs_pred_wide) &lt;- as.character(temp_pred_grid)\ncolnames(obs_pred_wide) &lt;- spat_pred_grid$id\n\ncovars_pred &lt;- spat_pred_grid                     # covariates\nSTdata_pred &lt;- createSTdata(obs = obs_pred_wide,  # ST object\n                            covars = covars_pred)\n\nNow prediction proceeds using the function predict, which requires as arguments the model, the fitted model parameters, and the data matrix STdata_pred.\n\nE &lt;- predict(STmodel, x.final, STdata = STdata_pred)\n\nThe returned object E contains both the \\(\\alpha\\)-fields predictions as well as the \\(Y\\)-field prediction at the unobserved locations. For example, E$beta$EX contains the conditional expectations of \\(\\alpha_1(\\mathbf{s}),\\alpha_2(\\mathbf{s})\\), and \\(\\alpha_3(\\mathbf{s})\\) given the data. For conciseness, we do not illustrate the plotting commands here. In the bottom panels of Figure 4.8, we show the conditional expectations, while in Figure 4.13 and Figure 4.14 we show the predictions and prediction standard errors of maximum temperature over six days of interest in July 1993.\n\n\n\n\n\n\nFigure 4.13: Predictions of Tmax in degrees Fahrenheit within a square lat-lon box defining the spatial domain of interest, for six days in July 1993, using temporal basis functions. Data for 14 July 1993 were deliberately omitted from the original data set.\n\n\n\n\n\n\n\n\n\nFigure 4.14: Prediction standard errors of Tmax in degrees Fahrenheit within a square lat-lon box enclosing the spatial domain of interest, for six days in July 1993, using temporal basis functions. Data for 14 July 1993 were deliberately omitted from the original data set.\n\n\n\n\nUsing SpatioTemporal for Modeling Spatial Effects of Temporal Covariates\nIn the first part of this Lab, we extracted the temporal basis functions from the data. However, SpatioTemporal can also be used to model the spatially varying effect of exogenous temporal covariates. This can be done by manually setting the STdata$trend data frame. When modeling temperature, interesting covariates may include a periodic signal with period equal to one year, or an index such as the El Niño Southern Oscillation (ENSO) Index.\nTo use a pre-existing covariate, we need to use the fnc argument in updateTrend to define a function that takes a Date object as an input and returns the covariate at these dates. The easiest way to do this in this example is to specify a look-up table in the function containing the covariate for each date, but an interpolant can also be used when the covariate has missing information for one or more dates.\nAs an exercise, repeat the Lab above, but this time use a single linear temporal trend as a temporal covariate. The look-up table we need is just a two-column data frame containing the date in the first column, and V1 (first covariate) in the second column. This can be set up as follows.\n\nall_dates &lt;- NOAA_sub$date %&gt;% unique()     # dates\nlookup &lt;- data.frame(date = all_dates,      # covariate (linear)\n                     V1 = scale(as.numeric(all_dates)))\n\nType plot(lookup) to see the temporal covariate that we have just created. Now we need to create the function that takes a Date object as input and returns the required covariate values. This can be done using left_join.\n\n## Function that returns the covariates in a data frame\n## at the required dates\nfnc &lt;- function(dates) {\n  left_join(data.frame(date = dates),\n            lookup, by = \"date\") %&gt;%\n  select(-date)\n}\n\nNow we can call updateTrend with our covariate function as argument.\n\nSTdata &lt;- updateTrend(STdata, fnc = fnc)\n\nThe rest of the code remains largely similar, except that now we are considering only two temporal basis functions and not three (the first basis function is constant in time, and the second one is linear in time). Changing the required parts of the code is left as an exercise.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Descriptive Spatio-Temporal Statistical Models</span>"
    ]
  },
  {
    "objectID": "Chapter4.html#lab-4.4-non-gaussian-spatio-temporal-gams-with-mgcv",
    "href": "Chapter4.html#lab-4.4-non-gaussian-spatio-temporal-gams-with-mgcv",
    "title": "4  Descriptive Spatio-Temporal Statistical Models",
    "section": "Lab 4.4: Non-Gaussian Spatio-Temporal GAMs with mgcv",
    "text": "Lab 4.4: Non-Gaussian Spatio-Temporal GAMs with mgcv\nGeneralized additive models (GAMs) and generalized additive mixed models (GAMMs) can be implemented quickly and efficiently with the package mgcv and the functions gam and gamm, respectively. For a comprehensive treatment of GAMs and GAMMs and their implementation through mgcv, see Wood (2017).\nIn this Lab, we aim to predict the expected counts at arbitrary spatio-temporal locations from the vector of observed counts \\(\\mathbf{Z}\\). The data we use are the Carolina wren counts in the BBS data set described in Section 2.1. We require the package mgcv as well as dplyr, tidyr, ggplot2, and STRbook.\n\nlibrary(\"dplyr\")\nlibrary(\"ggplot2\")\nlibrary(\"mgcv\")\nlibrary(\"STRbook\")\nlibrary(\"tidyr\")\ndata(\"MOcarolinawren_long\", package = \"STRbook\")\n\nGAMs and GAMMs rely on constructing smooth functions of the covariates, and in a spatio-temporal context, these will inevitably include space and time. In this Lab, we consider the following simple GAM (see Equation 4.36):\n\\[\ng(Y(\\mathbf{s};t)) = \\beta + f(\\mathbf{s};t) + \\nu(\\mathbf{s};t),\n\\tag{4.40}\\]\nwhere \\(g(\\cdot)\\) is a link function, \\(\\beta\\) is an intercept, the function \\(f(\\mathbf{s};t)\\) is a random smooth function of space and time, and \\(\\nu(\\mathbf{s};t)\\) is a spatio-temporal white-noise error process.\nIn mgcv, the random function \\(f(\\mathbf{s};t)\\) is generally decomposed using a separable spline basis. Now, there are several basis functions that can be used to reconstruct \\(f(\\mathbf{s};t)\\), some of which are knot-based (e.g., B-splines). For the purpose of this Lab, it is sufficient to know that splines, of whatever order, are decomposed into a set of basis functions. Thus, \\(f(\\mathbf{s};t)\\) is decomposed as \\(\\sum_{i=1}^{r_1}\\phi_{1i}(\\mathbf{s};t)\\alpha_{1i}\\), where the \\(\\{\\alpha_{1i}\\}\\) are unknown random effects that need to be predicted, and the \\(\\{\\phi_{1i}\\}\\) are given below.\nThere are a number of basis functions that can be chosen. Those derived from thin-plate regression splines are convenient, as they are easily amenable to multiple covariates (e.g., functions of \\((\\mathbf{s};t) \\equiv (s_1,s_2; t)\\)). Thin-plate splines are isotropic and invariant to rotation but not invariant to covariate scaling. Hence, the use of thin-plate splines for fitting a curve over space and time is not recommended, since units in time are different from those in space.\nTo combine interacting covariates with different units, such as space and time, mgcv implements a tensor-product structure, whereby the basis functions smoothing the individual covariates are combined productwise. That is,\n\\[\nf(\\mathbf{s};t) = \\sum_{i=1}^{r_1}\\sum_{j=1}^{r_2}\\phi_{1i}(\\mathbf{s})\\phi_{2j}(t)\\alpha_{ij} \\equiv \\boldsymbol{\\phi}(\\mathbf{s};t)' \\boldsymbol{\\alpha}.\n\\]\nThe function te forms the product from the marginals; for example, in our case this can be achieved by using te(lon,lat,t). Other arguments can be passed to te for added functionality; for example, the basis-function class is specified through bs, the number of basis functions through k, and the dimension of each spline through d. In this case, we employ a thin-plate spline basis over longitude and latitude (\"tp\") and a cubic regression spline over time (\"cr\"). A GAM formula for Equation 4.40 is implemented as follows:\n\nf &lt;- cnt ~ te(lon, lat, t,         # inputs over which to smooth\n              bs = c(\"tp\", \"cr\"),  # types of bases\n              k = c(50, 10),       # knot count in each dimension\n              d = c(2, 1))         # (s,t) basis dimension\n\nWe chose \\(r_1 = 50\\) basis functions for the spatial component and \\(r_2 = 10\\) for the temporal component. These values were chosen after some trial and error. The number of knots could have been set using cross-validation; see Chapter 3. In general, the estimated degrees of freedom should be considerably lower than the total number of knots; if this is not the case, probably the number of knots should be increased.\nIn Lab 3.4 we saw that the Carolina wren counts are over-dispersed. To account for this, we use the negative-binomial distribution to model the response in Equation 4.35 (a quasi-Poisson model would also be suitable). The gam function is called in the code below, where we specify the negative-binomial family and a log link (the function \\(g(\\cdot)\\) in Equation 4.40):\n\ncnts &lt;- gam(f, family = nb(link = \"log\"),\n             data = MOcarolinawren_long)\n\nThe returned object is a gam object, which extends glm and lm objects (i.e., functions that can be applied to glm and lm objects, such as residuals, can also be applied to gam objects). The negative-binomial distribution handles over-dispersion in the data through a size parameter \\(r\\), such that, for a fixed mean, the negative-binomial distribution approaches the Poisson distribution as \\(r \\rightarrow \\infty\\). In this case, the estimated value for \\(r\\) (named Theta in mgcv) is\n\ncnts$family$getTheta(trans = 1)\n\n[1] 5.178305\n\n\nwhich is not large and suggestive of over-dispersion. Several graphical diagnostics relating to the fit can be explored using the gam.check function.\nTo predict the field at unobserved locations using the hierarchical model, we first construct a space-time grid upon which to predict.\n\nMOlon &lt;- MOcarolinawren_long$lon\nMOlat &lt;- MOcarolinawren_long$lat\n\n## Construct space-time grid {.unnumbered}\ngrid_locs &lt;- expand.grid(lon = seq(min(MOlon) - 0.2,\n                                   max(MOlon) + 0.2,\n                                   length.out = 80),\n                         lat = seq(min(MOlat) - 0.2,\n                                   max(MOlat) + 0.2,\n                                   length.out = 80),\n                         t = 1:max(MOcarolinawren_long$t))\n\nThen we call the function predict which, when se.fit = TRUE, returns a list containing the predictions and their associated prediction standard errors.\n\nX &lt;- predict(cnts, grid_locs, se.fit = TRUE)\n\nSpecifically, the predictions and prediction standard errors are available in X$fit and X$se.fit, respectively. These can be plotted using ggplot2 as follows.\n\n## Put data to plot into data frame\ngrid_locs$pred &lt;- X$fit\ngrid_locs$se &lt;- X$se.fit\n\n## Plot predictions and overlay observations\ng1 &lt;- ggplot() +\n    geom_raster(data = grid_locs,\n                aes(lon, lat, fill = pmin(pmax(pred, -1), 5))) +\n    facet_wrap(~t, nrow = 3, ncol = 7) +\n    geom_point(data = filter(MOcarolinawren_long, !is.na(cnt)),\n               aes(lon, lat),\n               colour = \"black\", size = 3) +\n    geom_point(data=filter(MOcarolinawren_long, !is.na(cnt)),\n               aes(lon, lat, colour = log(cnt)),\n               size = 2) +\n    fill_scale(limits = c(-1, 5),\n               name = expression(log(Y[t]))) +\n    col_scale(name = \"log(cnt)\", limits=c(-1, 5)) +\n    theme_bw()\n\n## Plot prediction standard errors\ng2 &lt;- ggplot() +\n    geom_raster(data = grid_locs,\n                aes(lon, lat, fill = pmin(se, 2.5))) +\n    facet_wrap(~t, nrow = 3, ncol = 7) +\n    fill_scale(palette = \"BrBG\",\n               limits = c(0, 2.5),\n               name = expression(s.e.)) +\n    theme_bw()\n\nThe plots are shown in Figure 4.15 and Figure 4.16, respectively. One may also use the plot.gam function on cnts to quickly generate plots of the tensor products.\n\n\n\n\n\n\nFigure 4.15: Posterior mean of \\(\\log(Y(\\cdot))\\) on a grid for \\(t=1\\) (the year 1994) to \\(t=21\\) (the year 2014), based on a negative-binomial data model using the package mgcv. The log of the observed count is shown in circles using the same color scale.\n\n\n\n\n\n\n\n\n\nFigure 4.16: Posterior standard deviation (i.e., prediction standard error) of \\(\\log(Y(\\cdot))\\) on a grid for \\(t=1\\) (the year 1994) to \\(t=21\\) (the year 2014), based on a negative-binomial data model using the package mgcv.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Descriptive Spatio-Temporal Statistical Models</span>"
    ]
  },
  {
    "objectID": "Chapter4.html#lab-4.5-non-gaussian-spatio-temporal-models-with-inla",
    "href": "Chapter4.html#lab-4.5-non-gaussian-spatio-temporal-models-with-inla",
    "title": "4  Descriptive Spatio-Temporal Statistical Models",
    "section": "Lab 4.5: Non-Gaussian Spatio-Temporal Models with INLA",
    "text": "Lab 4.5: Non-Gaussian Spatio-Temporal Models with INLA\nIntegrated Nested Laplace Approximation (INLA) is a Bayesian method that provides approximate marginal (posterior) distributions over all states and parameters. The package INLA allows for a variety of modeling approaches, and the reader is referred to the book by Blangiardo & Cameletti (2015) for an extensive treatment. Other useful resources are Lindgren & Rue (2015) and Krainski et al. (2019).\nIn this Lab we shall predict expected counts at arbitrary space-time locations from the vector of observed counts \\(\\mathbf{Z}\\). The data we use are the Carolina wren counts in the BBS data set described in Section 2.1. For this Lab, we require the package INLA as well as dplyr, tidyr, ggplot2, and STRbook.\n\nlibrary(\"INLA\")\nlibrary(\"dplyr\")\nlibrary(\"tidyr\")\nlibrary(\"ggplot2\")\nlibrary(\"STRbook\")\ndata(\"MOcarolinawren_long\", package = \"STRbook\")\n\nConsider the data model,\n\\[\n\\mathbf{Z}_t | \\mathbf{Y}_t \\; \\sim \\; ind.~NB(\\mathbf{Y}_t, r),\n\\tag{4.41}\\]\nand the process model,\n\\[\n\\log(\\mathbf{Y}_t) =  \\mathbf{X}_t \\boldsymbol{\\beta}+ \\boldsymbol{\\Phi}_t \\boldsymbol{\\alpha}_t.\n\\tag{4.42}\\]\nIn Equation 4.41 and Equation 4.42, \\(\\mathbf{Z}_t\\) is an \\(m_t\\)-dimensional data vector of counts at \\(m_t\\) spatial locations, \\(E(\\mathbf{Z}_t | \\mathbf{Y}_t) = \\mathbf{Y}_t\\), \\(\\mathbf{Y}_t\\) represents the latent spatio-temporal mean process at \\(m_t\\) locations, \\(\\boldsymbol{\\Phi}_t\\) is an \\(m_t \\times n_\\alpha\\) matrix of spatial basis functions, \\(r\\) is the size parameter, and the associated random coefficients are modeled as \\(\\boldsymbol{\\alpha}_t \\; \\sim \\; \\text{Gau}({\\mathbf{0}},\\mathbf{C}_\\alpha)\\).\nIn order to fit this hierarchical model, we need to generate the basis functions with which to construct the matrices \\(\\{\\boldsymbol{\\Phi}_t: t = 1,\\dots,T\\}\\). In INLA, the basis functions used are typically “tent” (finite element) functions constructed over a triangulation of the domain. To establish a “boundary” for the domain, we can use the function inla.nonconvex.hull, as follows.\n\ncoords &lt;- unique(MOcarolinawren_long[c(\"loc.ID\", \"lon\", \"lat\")])\nboundary &lt;- inla.nonconvex.hull(as.matrix(coords[, 2:3]))\n\nThe triangulation of the domain is then carried out using the function inla.mesh.2d. This function takes several arguments (see its help file for details). Two of the most important arguments are max.edge and cutoff. When the former is supplied with a vector of length \\(2\\), the first element is the maximum edge length in the interior of the domain, and the second element is the maximum edge length in the exterior of the domain (obtained from a small buffer that is automatically created to reduce boundary effects). The second argument, cutoff, establishes the minimum edge length. Below we choose a maximum edge length of 0.8 in the domain interior. This is probably too large for the problem at hand, but reducing this considerably increases the computational burden when fitting the model.\n\nMOmesh &lt;- inla.mesh.2d(boundary = boundary,\n                       max.edge = c(0.8, 1.2), # max. edge length\n                       cutoff = 0.1)           # min. edge length\n\nThe mesh and the data locations are plotted using the following commands.\n\nplot(MOmesh, asp = 1, main = \"\")\nlines(coords[c(\"lon\", \"lat\")], col = \"red\", type = \"p\")\n\nThese are shown in Figure 4.17. Note that the triangulation is irregular and contains an extension with triangles that are larger than those in the interior of the domain.\n\n\n\n\n\n\nFigure 4.17: Triangulation for the Carolina wren data locations over which the “tent” functions are constructed (black), and the observation locations (red circles) are superimposed. The blue line denotes the interior non-convex domain of interest that includes all the data points.\n\n\n\nAs in the standard Gaussian case, the modeling effort lies in establishing the covariance matrix of \\(\\boldsymbol{\\alpha}\\equiv (\\boldsymbol{\\alpha}_1',\\dots,\\boldsymbol{\\alpha}_T')'\\). When using INLA, typically the covariance matrix of \\(\\boldsymbol{\\alpha}\\) is chosen to be separable and of the form \\(\\boldsymbol{\\Sigma}_t(\\rho) \\otimes \\boldsymbol{\\Sigma}_s(\\tau,\\kappa,\\nu)\\) in such a way that its inverse (i.e., the precision matrix) is sparse. The matrix \\(\\boldsymbol{\\Sigma}_t\\) is constructed assuming an AR(1) process, and thus it is parameterized using a single AR parameter, \\(\\rho\\). This parameter dictates the correlation of \\(\\boldsymbol{\\alpha}\\) across time; the closer \\(\\rho\\) is to 1, the higher the temporal correlation. The matrix \\(\\boldsymbol{\\Sigma}_s\\) is parameterized using three parameters, and it reflects the spatial covariance required such that the reconstructed field is, approximately, a solution to the stochastic partial differential equation (SPDE)\n\\[\n(\\kappa^2 - \\Delta)^{\\alpha/2}(\\tau Y(\\cdot)) = \\epsilon(\\cdot),\n\\]\nwhere \\(\\Delta\\) is the Laplacian, \\(\\epsilon(\\cdot)\\) is a white-noise process, and \\(\\tau\\) controls the variance. The resulting field has a Matérn covariance function. The parameter \\(\\kappa\\) is a scaling parameter that translates to a “practical” spatial correlation length (i.e., the spatial separation at which the correlation is 0.1) \\(l = (\\sqrt{8\\nu})/\\kappa\\), while \\(\\alpha = \\nu + d/2\\) is a smoothness parameter and \\(d\\) is the spatial dimension. Here we fix \\(\\nu = 1\\) (\\(\\alpha = 2\\)); this parameter is notoriously difficult to estimate and frequently set using cross-validation. Note that there are other “practical” length scales used to characterize the range of a correlation function (e.g., “effective range” when the correlation is 0.05); our choice here is motivated by the INLA package that readily provides a marginal distribution over the parameter \\(l\\) as defined here.\nThe SPDE can be constructed on the mesh using the function inla.spde2.pcmatern. The pc in pcmatern is short for “penalized complexity,” and it is used to refer to prior distributions over the hyperparameters that are both interpretable and that have interesting theoretical properties Simpson et al. (2017). We define prior distributions below over the range parameter \\(l\\) such that \\(P(l &lt; 1) = 0.01\\), and over the marginal standard deviation such that \\(P(\\sigma &gt; 4) = 0.01\\). We elicited these distributions by looking at the count data – it is highly unlikely that the spatial correlation length is less than 1 degree and that the expected counts are of the order of 50 or more (we will use a log link, and \\(e^4 \\approx 55\\)).\n\nspde &lt;- inla.spde2.pcmatern(mesh = MOmesh,\n                            alpha = 2,\n                            prior.range = c(1, 0.01),\n                            prior.sigma = c(4, 0.01))\n\nWith the discretization shown in Figure 4.17, \\(\\alpha_{t,i}\\) can be viewed as the weight of the \\(i\\)th basis function at time \\(t\\). The observation matrix \\(\\boldsymbol{\\Phi}_t\\) then maps the observations to the finite-element space at time \\(t\\); if the observation lies exactly on a vertex, then the associated row in \\(\\boldsymbol{\\Phi}_t\\) will be 0 everywhere except for a 1 in the column corresponding to the vertex. Otherwise, the row has three non-zero elements, with each representing the proportion being assigned to each vertex. For point predictions or areal averages, all rows in \\(\\boldsymbol{\\Phi}_t\\) sum to 1. Finally, for this example, we choose each element in \\(\\mathbf{X}_t\\) to be equal to 1. The coefficient \\(\\beta_0\\) is then the intercept.\nThe package INLA requires space and time to be “blocked up” with an ordering of the variables in which space runs faster than time (i.e., the first few variables are spatial nodes at the first time point, the next few are at the second time point, and so on). Hence we have the block-matrix structure\n\\[\n\\log\\left(\\begin{bmatrix} \\mathbf{Y}_1 \\\\ \\vdots \\\\ \\mathbf{Y}_T \\end{bmatrix}\\right) = \\begin{bmatrix} \\mathbf{X}_1 \\\\ \\vdots \\\\ \\mathbf{X}_T \\end{bmatrix}\\boldsymbol{\\beta}+ \\begin{bmatrix} \\boldsymbol{\\Phi}_1 & \\mathbf{0}& \\dots \\\\ \\vdots & \\ddots & \\vdots \\\\ \\mathbf{0}& \\dots & \\boldsymbol{\\Phi}_T \\end{bmatrix} \\begin{bmatrix} \\boldsymbol{\\alpha}_1 \\\\ \\vdots \\\\ \\boldsymbol{\\alpha}_T \\end{bmatrix},\n\\tag{4.43}\\]\nwhere \\(\\log(\\cdot)\\) corresponds to a vector of elementwise logarithms. This can be further simplified to\n\\[\n\\log(\\mathbf{Y}) = \\mathbf{X}\\boldsymbol{\\beta}+ \\boldsymbol{\\Phi}\\boldsymbol{\\alpha},\n\\tag{4.44}\\]\nwhere \\(\\mathbf{Y}= (\\mathbf{Y}_1',\\dots,\\mathbf{Y}_T')'\\), \\(\\mathbf{X}= (\\mathbf{X}_1',\\dots,\\mathbf{X}_T')'\\), \\(\\boldsymbol{\\Phi}\\equiv \\textrm{bdiag}(\\{\\boldsymbol{\\Phi}_t : t = 1,\\dots,T\\})\\), \\(\\textrm{bdiag}(\\cdot)\\) constructs a block-diagonal matrix from its arguments, and \\(\\boldsymbol{\\alpha}\\equiv (\\boldsymbol{\\alpha}_1',\\dots,\\boldsymbol{\\alpha}_T')'\\).\nA space-time index needs to be constructed for this representation. This index is a double index that identifies both the spatial location and the associated time point. In Lab 2.2 we saw how the function expand.grid can be used to generate such indices from a set of spatial locations and time points. In INLA, we instead use the function inla.spde.make.index. It takes as arguments the index name, the number of spatial points in the mesh, and the number of time points.\n\nn_years &lt;- length(unique(MOcarolinawren_long$t))\nn_spatial &lt;- MOmesh$n\ns_index &lt;- inla.spde.make.index(name = \"spatial.field\",\n                                n.spde = n_spatial,\n                                n.group = n_years)\n\nThe list s_index contains two important items, the spatial.field index, which runs from 1 to n_spatial for n_years times, and spatial.field.group, which runs from 1 to n_years, with each element replicated n_spatial times. Note how this is similar to what one would obtain from expand.grid.\nThe matrix \\(\\boldsymbol{\\Phi}\\) in Equation 4.44 is found using the inla.spde.make.A function. This function takes as arguments the mesh, the measurement locations loc, the measurement group (in our case the year of observation) and the number of groups.\n\ncoords.allyear &lt;- MOcarolinawren_long[c(\"lon\", \"lat\")] %&gt;%\n                  as.matrix()\nPHI &lt;- inla.spde.make.A(mesh = MOmesh,\n                        loc = coords.allyear,\n                        group = MOcarolinawren_long$t,\n                        n.group = n_years)\n\nNote that\n\ndim(PHI)\n\n[1] 1575 5439\n\n\nThis is a matrix equal in dimension to (number of observations) \\(\\times\\) (number of indices) of our basis functions in space and time.\n\nnrow(MOcarolinawren_long)\n\n[1] 1575\n\nlength(s_index$spatial.field)\n\n[1] 5439\n\n\nThe latent Gaussian model is constructed in INLA through a stack. Stacks are handy as they allow one to define data, effects, and observation matrices in groups (e.g., one accounting for the measurement locations and another accounting for the prediction locations), which can then be stacked together into one bigger stack. In order to build a stack we need to further block up Equation 4.43 into a representation amenable to the inla function (called later on) as follows:\n\\[\n\\log(\\mathbf{Y}) = \\boldsymbol{\\Pi}\\boldsymbol{\\gamma},\n\\]\nwhere \\(\\boldsymbol{\\Pi}= (\\boldsymbol{\\Phi}, \\mathbf{1})\\) and \\(\\boldsymbol{\\gamma}= (\\boldsymbol{\\alpha}',\\beta_0)'\\).\nA stack containing the data and covariates at the measurement locations is constructed by supplying the data (argument data), the matrix \\(\\boldsymbol{\\Pi}\\) (argument A), and information on the vector \\(\\boldsymbol{\\gamma}\\). The stack is then tagged with the label \"est\".\n\n## First stack: Estimation\nn_data &lt;- nrow(MOcarolinawren_long)\nstack_est &lt;- inla.stack(\n                 data = list(cnt = MOcarolinawren_long$cnt),\n                 A = list(PHI, 1),\n                 effects = list(s_index,\n                                list(Intercept = rep(1, n_data))),\n                 tag = \"est\")\n\nWe next construct a stack containing the matrices and vectors defining the model at the prediction locations. In this case, we choose the triangulation vertices as the prediction locations; then \\(\\boldsymbol{\\Phi}\\) is simply the identity matrix, and \\(\\mathbf{X}\\) is a vector of ones. We store the information on the prediction locations in df_pred and that for \\(\\boldsymbol{\\Phi}\\) in PHI_pred.\n\ndf_pred &lt;- data.frame(lon = rep(MOmesh$loc[,1], n_years),\n                      lat = rep(MOmesh$loc[,2], n_years),\n                      t = rep(1:n_years, each = MOmesh$n))\nn_pred &lt;- nrow(df_pred)\nPHI_pred &lt;- Diagonal(n = n_pred)\n\nThe prediction stack is constructed in a very similar way to the estimation stack, but this time we set the data values to NA to indicate that prediction should be carried out at these locations.\n\n## Second stack: Prediction\nstack_pred &lt;- inla.stack(\n                 data = list(cnt = NA),\n                 A = list(PHI_pred, 1),\n                 effects = list(s_index,\n                                list(Intercept = rep(1, n_pred))),\n                 tag = \"pred\")\n\nThe estimation stack and prediction stack are combined using the inla.stack function.\n\nstack &lt;- inla.stack(stack_est, stack_pred)\n\nAll inla.stack does is block-concatenate the matrices and vectors in the individual stacks. Denote the log-expected counts at the prediction locations as \\(\\mathbf{Y}^*\\), the covariates as \\(\\mathbf{X}^*\\), and the basis functions evaluated at the prediction locations as \\(\\boldsymbol{\\Phi}^*\\). Then\n\\[\n\\begin{bmatrix} \\log(\\mathbf{Y})  \\\\ \\log(\\mathbf{Y}^*) \\end{bmatrix} = \\begin{bmatrix} \\boldsymbol{\\Pi}\\\\ \\boldsymbol{\\Pi}^* \\end{bmatrix}   \\boldsymbol{\\gamma},\n\\]\nrecalling that \\(\\boldsymbol{\\gamma}= (\\boldsymbol{\\alpha}',\\beta_0)'\\). Note that, internally, some columns of \\(\\boldsymbol{\\Pi}\\) and \\(\\boldsymbol{\\Pi}^*\\) corresponding to unobserved states are not stored. For example \\(\\boldsymbol{\\Phi}\\), internally, has dimension\n\ndim(stack_est$A)\n\n[1] 1575 1702\n\n\nThe number of rows corresponds to the number of data points, while the number of columns corresponds to the number of observed states (sum(colSums(PHI) &gt; 0)) plus one for the intercept term.\nAll that remains before fitting the model is for us to define the formula, which is a combination of a standard R formula for the fixed effects and an INLA formula for the spatio-temporal residual component. For the latter, we need to specify the name of the index we created as the first argument (in this case spatial.field), the model (in this case spde), the name of the grouping/time index (in this case spatial.field.group) and, finally, the model to be constructed across groups (in this case an AR(1) model). The latter modeling choice implies that \\(E(\\boldsymbol{\\alpha}_{t+1}\\mid \\boldsymbol{\\alpha}_t) = \\rho \\boldsymbol{\\alpha}_t\\), \\(t = 1,\\dots,T-1\\). Our choice for the prior on the AR(1) coefficient, \\(\\rho\\), is a penalized complexity prior, such that \\(P(\\rho &gt; 0) = 0.9\\) to reflect the prior belief that we highly doubt a negative temporal correlation.\n\n## PC prior on rho\nrho_hyper &lt;- list(theta=list(prior = 'pccor1',\n                             param = c(0, 0.9)))\n\n## Formula\nformula &lt;- cnt ~ -1 + Intercept +\n                 f(spatial.field,\n                   model = spde,\n                   group = spatial.field.group,\n                   control.group = list(model = \"ar1\",\n                                        hyper = rho_hyper))\n\nNow we have everything in place to run the main function for fitting the model, inla. This needs the data from the stack (extracted through inla.stack.data) and the exponential family (in this case negative-binomial). The remaining options indicate the desired outputs. In the command given below, we instruct inla to fit the model and also to compute the predictions at the required locations.\n\noutput &lt;- inla(formula,\n               data = inla.stack.data(stack, spde = spde),\n               family = \"nbinomial\",\n               control.predictor = list(A = inla.stack.A(stack),\n                                        compute = TRUE))\n\nThis operation takes a long time. In STRbook we provide the important components of this object, which can be loaded through\n\ndata(\"INLA_output\", package = \"STRbook\")\n\nINLA provides approximate marginal posterior distributions for each \\(\\boldsymbol{\\alpha}_t\\) in \\(\\boldsymbol{\\alpha}\\) and \\(\\{\\boldsymbol{\\beta},\\rho,\\tau\\,\\kappa\\}\\). The returned object, output, contains all the results as well as summaries of these results for quick analysis. From the posterior distributions over the precision parameter \\(\\tau\\) and scale parameter \\(\\kappa\\), we can readily obtain marginal posterior distributions over the more interpretable variance parameter \\(\\sigma^2\\) and practical range parameter \\(l\\). Posterior distributions of some of the parameters are shown in Figure 4.18, where we can see that the AR(1) coefficient of the latent field, \\(\\rho\\), is large (most of the mass of the posterior distribution is close to \\(1\\)), and the practical range parameter, \\(l\\), is of the order of 2 degrees (\\(\\approx 200\\),km). The posterior distribution of the marginal variance of the latent field is largest between 2 and 4. These values suggest that there are strong spatial and temporal dependencies in the data. We give code below for plotting the posterior marginal distributions shown in Figure 4.18.\n\n\n\n\n\n\nFigure 4.18: Marginal posterior distributions of \\(\\beta_0\\), the temporal correlation \\(\\rho\\), the variance \\(\\sigma^2\\), and the range parameter \\(l\\).\n\n\n\n\noutput.field &lt;- inla.spde2.result(inla = output,\n                                  name = \"spatial.field\",\n                                  spde = spde,\n                                  do.transf = TRUE)\n## plot p(beta0 | Z)\nplot(output$marginals.fix$Intercept,\n     type='l',\n     xlab=expression(beta[0]),\n     ylab=expression(p(beta[0]*\"|\"*Z)))\n\n## plot p(rho | Z)\nplot(output$marginals.hyperpar$`GroupRho for spatial.field`,\n     type='l',\n     xlab=expression(rho),\n     ylab=expression(p(rho*\"|\"*Z)))\n\n## plot p(sigma^2 | Z)\nplot(output.field$marginals.variance.nominal[[1]],\n     type='l',\n     xlab=expression(sigma^2),\n     ylab=expression(p(sigma^2*\"|\"*Z)))\n\n## plot p(range | Z)\nplot(output.field$marginals.range.nominal[[1]],\n     type='l',\n     xlab=expression(l),\n     ylab=expression(p(l*\"|\"*Z)))\n\nWe provide the prediction (posterior mean) and prediction standard error (posterior standard deviation) for \\(\\log(Y(\\cdot))\\) in Figures Figure 4.19 and Figure 4.20, respectively. These figures were generated by linearly interpolating the posterior mean and posterior standard deviation of \\(\\log(\\mathbf{Y}^*)\\) on a fine grid. Note how a high observed count at a certain location in one year affects the predictions at the same location in neighboring years, even if unobserved.\n\n\n\n\n\n\nFigure 4.19: Posterior mean of \\(\\log(Y(\\cdot))\\) on a grid for \\(t=1\\) (the year 1994) to \\(t=21\\) (the year 2014), based on a negative-binomial data model using the package INLA. The log of the observed count is shown in circles using the same color scale.\n\n\n\n\n\n\n\n\n\nFigure 4.20: Posterior standard deviation (i.e., prediction standard error) of \\(\\log(Y(\\cdot))\\) on a grid for \\(t=1\\) (the year 1994) to \\(t=21\\) (the year 2014), based on a negative-binomial data model using the package INLA.\n\n\n\nPlotting spatial fields, such as those shown in Figures Figure 4.19 and Figure 4.20, from the INLA output can be a bit involved since each prediction and prediction standard error of \\(\\boldsymbol{\\alpha}_t\\) for each \\(t\\) needs to be projected spatially. First, we extract the predictions and prediction standard errors of \\(\\boldsymbol{\\alpha}= (\\boldsymbol{\\alpha}_1',\\dots,\\boldsymbol{\\alpha}_T')'\\) as follows.\n\nindex_pred &lt;- inla.stack.index(stack, \"pred\")$data\nlp_mean &lt;- output$summary.fitted.values$mean[index_pred]\nlp_sd &lt;- output$summary.fitted.values$sd[index_pred]\n\nNext, we need to create a spatial grid upon which we map the predictions and their associated prediction standard errors. This can be constructed using the function expand.grid. We construct an 80 \\(\\times\\) 80 grid below.\n\ngrid_locs &lt;- expand.grid(\n                 lon = seq(min(MOcarolinawren_long$lon) - 0.2,\n                           max(MOcarolinawren_long$lon) + 0.2,\n                           length.out = 80),\n                 lat = seq(min(MOcarolinawren_long$lat) - 0.2,\n                           max(MOcarolinawren_long$lat) + 0.2,\n                           length.out = 80))\n\nThe function inla.mesh.projector provides all the information required, based on the created spatial grid, to carry out the mapping.\n\nproj.grid &lt;- inla.mesh.projector(MOmesh,\n                    xlim = c(min(MOcarolinawren_long$lon) - 0.2,\n                             max(MOcarolinawren_long$lon) + 0.2),\n                    ylim = c(min(MOcarolinawren_long$lat) - 0.2,\n                             max(MOcarolinawren_long$lat) + 0.2),\n                    dims = c(80, 80))\n\nNow we have everything in place to map each \\(\\boldsymbol{\\alpha}_t\\) on our spatial grid. We iterate through \\(t\\), and for each \\(t = 1,\\dots,T\\) we map both the prediction and prediction standard errors of \\(\\boldsymbol{\\alpha}_t\\) on the spatial grid as follows.\n\npred &lt;- sd &lt;-  NULL\nfor(i in 1:n_years) {\n    ii &lt;- (i-1)*MOmesh$n + 1\n    jj &lt;- i*MOmesh$n\n    pred[[i]] &lt;- cbind(grid_locs,\n                       z = c(inla.mesh.project(proj.grid,\n                                             lp_mean[ii:jj])),\n                       t = i)\n    sd[[i]] &lt;- cbind(grid_locs,\n                     z = c(inla.mesh.project(proj.grid,\n                                             lp_sd[ii:jj])),\n                     t = i)\n}\n\nThe last thing we need to do is compile all the data (which are in lists) into one data frame for plotting with ggplot2. We concatenate all the list elements rowwise and remove those elements that are NA because they fall outside of the support of any basis function.\n\npred &lt;- do.call(\"rbind\", pred) %&gt;% filter(!is.na(z))\nsd &lt;- do.call(\"rbind\", sd) %&gt;% filter(!is.na(z))\n\nThe data frames pred and sd now contain the spatio-temporal predictions and spatio-temporal prediction standard errors. Plotting of these fields using ggplot2 is left as an exercise for the reader.\n\n\n\n\nBaddeley, A., Rubak, E., & Turner, R. (2015). Spatial point patterns: Methodology and applications with r. Chapman & Hall/CRC.\n\n\nBakar, K. S., & Sahu, S. K. (2015). spTimer: Spatio-temporal bayesian modelling using r. Journal of Statistical Software, 63(15), 1–32.\n\n\nBanerjee, S., Carlin, B. P., & Gelfand, A. E. (2015). Hierarchical modeling and analysis for spatial data (second). Chapman & Hall/CRC.\n\n\nBlangiardo, M., & Cameletti, M. (2015). Spatial and spatio-temporal bayesian models with R-INLA. John Wiley & Sons.\n\n\nCarlin, B. P., & Louis, T. A. (2010). Bayes and empirical bayes methods for data analysis. Chapman & Hall/CRC.\n\n\nChristakos, G. (2017). Spatiotemporal random fields: Theory and applications (second). Elsevier.\n\n\nCressie, N. (1993). Statistics for spatial data (revised). John Wiley & Sons.\n\n\nCressie, N., & Huang, H.-C. (1999). Classes of nonseparable, spatio-temporal stationary covariance functions. Journal of the American Statistical Association, 94(448), 1330–1339.\n\n\nCressie, N., Shi, T., & Kang, E. L. (2010). Fixed rank filtering for spatio-temporal data. Journal of Computational and Graphical Statistics, 19(3), 724–745.\n\n\nCressie, N., & Wikle, C. K. (2011). Statistics for spatio-temporal data. John Wiley & Sons.\n\n\nCrujeiras, R. M., Fernández-Casal, R., & González-Manteiga, W. (2010). Nonparametric test for separability of spatio-temporal processes. Environmetrics, 21(3-4), 382–399.\n\n\nDiggle, P. J. (2013). Statistical analysis of spatial and spatio-temporal point patterns. Chapman & Hall/CRC.\n\n\nDiggle, P. J., & Ribeiro Jr., P. J. (2007). Model-based geostatistics. Springer.\n\n\nFinley, A. O., Banerjee, S., & Carlin, B. P. (2007). spBayes: An r package for univariate and multivariate hierarchical point-referenced spatial models. Journal of Statistical Software, 19(4), 1–24.\n\n\nGelman, A., Carlin, J. B., Stern, H. S., Dunson, D. B., Vehtari, A., & Rubin, D. B. (2014). Bayesian data analysis (3rd ed.). Chapman & Hall/CRC.\n\n\nHanks, E. M., Schliep, E. M., Hooten, M. B., & Hoeting, J. A. (2015). Restricted spatial regression in practice: Geostatistical models, confounding, and robustness under model misspecification. Environmetrics, 26(4), 243–254.\n\n\nHodges, J. S., & Reich, B. J. (2010). Adding spatially-correlated errors can mess up the fixed effect you love. American Statistician, 64(4), 325–334.\n\n\nHughes, J., & Haran, M. (2013). Dimension reduction and alleviation of confounding for spatial generalized linear mixed models. Journal of the Royal Statistical Society, Series B, 75(1), 139–159.\n\n\nJohnson, R. A., & Wichern, D. W. (1992). Applied multivariate statistical analysis. In Prentice Hall (3rd ed.). Prentice Hall.\n\n\nKendall, M. G., & Stuart, A. (1969). The advanced theory of statistics (3rd ed., Vol. 1). Hafner.\n\n\nKrainski, E. T., Gómez-Rubio, V., Bakka, H., Lenzi, A., Castro-Camilo, D., Simpson, D., Lindgren, F., & Rue, H. (2019). Advanced spatial modeling with stochastic partial differential equations using r and INLA. Chapman; Hall/CRC.\n\n\nLaird, N. M., & Ware, J. H. (1982). Random-effects models for longitudinal data. Biometrics, 963–974.\n\n\nLe, N. D., & Zidek, J. V. (2006). Statistical analysis of environmental space-time processes. Springer.\n\n\nLindgren, F., & Rue, H. (2015). Bayesian spatial modelling with R-INLA. Journal of Statistical Software, 63(19), 1–25. http://www.jstatsoft.org/v63/i19/\n\n\nLindgren, F., Rue, H., & Lindström, J. (2011). An explicit link between gaussian fields and gaussian markov random fields: The stochastic partial differential equation approach. Journal of the Royal Statistical Society, Series B, 73(4), 423–498.\n\n\nMateu, J., & Müller, W. G. (2013). Spatio-temporal design: Advances in efficient data acquisition. John Wiley & Sons.\n\n\nMcCulloch, C. E., & Searle, S. R. (2001). Generalized, linear, and mixed models. John Wiley & Sons.\n\n\nMontero, J.-M., Fernández-Avilés, G., & Mateu, J. (2015). Spatial and spatio-temporal geostatistical modeling and kriging. John Wiley & Sons.\n\n\nPatterson, H. D., & Thompson, R. (1971). Recovery of inter-block information when block sizes are unequal. Biometrika, 58(3), 545–554.\n\n\nRasmussen, C. E., & Williams, C. K. I. (2006). Gaussian processes for machine learning. MIT Press.\n\n\nRESSTE Network et al. (2017). Analyzing spatio-temporal data with r: Everything you always wanted to know – but were afraid to ask. Journal de La Société Française de Statistique, 158, 124–158.\n\n\nRue, H., Martino, S., & Chopin, N. (2009). Approximate bayesian inference for latent gaussian models by using integrated nested laplace approximations. Journal of the Royal Statistical Society, Series B, 71(2), 319–392.\n\n\nSchabenberger, O., & Gotway, C. A. (2005). Statistical methods for spatial data analysis. Chapman & Hall/CRC.\n\n\nSearle, S. R. (1982). Matrix algebra useful for statistics. John Wiley & Sons.\n\n\nShaddick, G., & Zidek, J. V. (2015). Spatio-temporal methods in environmental epidemiology. Chapman & Hall/CRC.\n\n\nSherman, M. (2011). Spatial statistics and spatio-temporal data: Covariance functions and directional properties. John Wiley & Sons.\n\n\nSimpson, D., Rue, H., Riebler, A., Martins, T. G., & Sørbye, S. H. (2017). Penalising model component complexity: A principled, practical approach to constructing priors. Statistical Science, 32(1), 1–28.\n\n\nVerbeke, G., & Molenberghs, G. (2009). Linear mixed models for longitudinal data. Springer.\n\n\nWood, S. N. (2017). Generalized additive models: An introduction with r (2nd ed.). Chapman & Hall/CRC.\n\n\nWu, C.-T., Gumpertz, M. L., & Boos, D. D. (2001). Comparison of GEE, MINQUE, ML, and REML estimating equations for normally distributed data. American Statistician, 55(2), 125–130.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Descriptive Spatio-Temporal Statistical Models</span>"
    ]
  },
  {
    "objectID": "Chapter5.html",
    "href": "Chapter5.html",
    "title": "5  Dynamic Spatio-Temporal Models",
    "section": "",
    "text": "5.1 General Dynamic Spatio-Temporal Models\nChapter 4 presented the “descriptive” approach to incorporating spatio-temporal statistical dependence into models. This chapter discusses the “dynamic” approach, closer to that holy grail of causation that scientists talk and theorize about and that often drives their experiments. In contrast to descriptive models, which fit means and covariances to spatio-temporal data, dynamic models can more easily use scientific knowledge and probability distributions to capture the evolution of current and future spatial fields from past fields.\nTo convince yourself that this dynamic approach has merit, just look around. If you have ever been mesmerized by waves breaking on the beach, storm clouds building on the horizon, or huge flocks of birds flying collectively in formation, you have witnessed spatio-temporal dynamics in action. What these processes (and many others) have in common is the spatial arrangement of the objects or fields changing, or evolving, from one moment to the next. This is how nature works at the macro scale - the current state of nature evolves from past states. Why does this matter if you are interested in simply modeling data that are indexed in space and time? Don’t the descriptive models presented in Chapter 4 represent nature as well?\nThe short answer to the second question is “yes,” but less directly. As we discuss in this chapter, it is difficult to describe all the joint and marginal dependence structures that exist in nature and respect this natural dynamic evolution - which answers the first question. While there are important differences, common to both Chapter 4 and this chapter is a statistical modeling approach where we always attempt to account for uncertainty, both in our understanding of the process of interest and in the data we observe.\nThe primary focus of this chapter is on linear dynamic spatio-temporal models (DSTMs) in the univariate context. Although it is reasonable, and often quite useful, to consider such processes to be continuous in time, for the sake of brevity we focus here on the more practical case where time has been discretized. However, we note that many science-oriented mechanistic models are specified from a continuous-time perspective (e.g., stochastic differential equations), and these are used to motivate the dynamic portion of the DSTM. It is beyond the scope of this book to take a continuous-time perspective, although it does fit into the DSTM framework.\nFor readers who have more of a time-series background, the DSTM could be thought of as a time series of spatial processes. We could consider an alternative perspective where the spatio-temporal process is a spatial process of time series, but the former perspective describes more naturally the dynamic evolutional aspect of the type of real-world phenomena discussed above. In particular, such a framework allows one not only to make predictions of spatial processes into the future, but also to make inference on parameters of models that correspond to mechanistic (e.g., physical or biological or economic …) processes. This gives DSTMs a powerful insight into causative mechanisms.\nWe start the chapter with a general hierarchical DSTM formulation in Section 5.1, followed by a more complete discussion of the special case of the linear Gaussian DSTM in Section 5.2. This includes brief discussion of data models, process models, and parameter models. Section 5.3 considers approaches for dealing with the curse of dimensionality in spatial processes and parameter spaces that is often present in DSTM settings. Section 5.4 gives a brief discussion of nonlinear DSTMs. More details on the technical aspects are given in a number of appendices: we present some standard estimation and prediction algorithms in Appendix C and examples of parameter reduction and process motivation through mechanistic models in Appendix D. Finally, Appendix E and Appendix F present case studies on mechanistically motivated prediction of Mediterranean winds and a machine-learning-motivated nonlinear DSTM for forecasting tropical Pacific SSTs, respectively.\nAs discussed in Chapter 1, we like to consider statistical models from a hierarchical modeling (HM) perspective. In the context of DSTMs, this means that at a minimum we must specify: a “data model” that gives a model for the data, conditioned on the true process of interest and some parameters; a “process model” that specifies the dynamic evolution of the spatio-temporal process, given some parameters; and either models for the parameters from the previous two stages (Bayesian hierarchical model, BHM), or “plug-in” estimates of the parameters (empirical hierarchical model, EHM). In this section we give a general overview of hierarchical modeling in the context of a DSTM.\nRecall from our preamble that we are considering discrete time here with temporal domain \\(D_t = \\{0,1,2,\\ldots\\}\\), where we assume a constant time increment \\(\\Delta_t = 1\\) (without loss of generality). We shall consider spatial locations for our observations and our latent process to be in some spatial domain \\(D_s\\) (which we may consider continuous or discrete and finite, depending on the context). The data can potentially come from anywhere and any time in the spatial and temporal domains; we denote data and potential data by \\(\\{Z_t(\\mathbf{s}): \\mathbf{s}\\in D_s;\\ t=0,1,\\ldots\\}\\), although only a subset is actually observed. The latent process is denoted by \\(\\{Y_t(\\mathbf{s}): \\mathbf{s}\\in D_s;\\ t=0,1,\\ldots\\}\\), and we may make inference on \\(Y_{t_0}(\\mathbf{s}_0)\\), even though there is no datum \\(Z_{t_0}(\\mathbf{s}_0)\\). Note that, unlike the models presented in Chapter 4, we change notation slightly and use a subscript \\(t\\) to represent time here, as is customary for discrete-time processes with \\(D_t = \\{0,1,2,\\ldots\\}\\).",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Dynamic Spatio-Temporal Models</span>"
    ]
  },
  {
    "objectID": "Chapter5.html#sec-GDSTM",
    "href": "Chapter5.html#sec-GDSTM",
    "title": "5  Dynamic Spatio-Temporal Models",
    "section": "",
    "text": "5.1.1 Data Model\nWe begin with a data model that describes the relationship between the observations and the latent spatio-temporal process. Generally, we could write the data model in a DSTM as\n\\[\nZ_t(\\cdot)  =  {\\cal{H}}_t({Y}_t(\\cdot), {\\boldsymbol{\\theta}}_{d,t}, {\\epsilon}_t(\\cdot)),\\quad t=1,\\ldots,T,\n\\]\nwhere \\(Z_t(\\cdot)\\) corresponds to the data at time \\(t\\) (and we use \\((\\cdot)\\) to represent arbitrary spatial locations), \\({Y}_t(\\cdot)\\) is the latent spatio-temporal process of interest, with a linear or nonlinear mapping, \\({\\cal{H}}_t\\), that connects the data to the latent process. The data-model error, which is typically measurement error and sometimes small-scale spatio-temporal variability, is given by \\({ \\epsilon}_t(\\cdot)\\). Finally, data-model parameters, which themselves may vary spatially and/or temporally, are represented by the vector \\({\\boldsymbol{\\theta}}_{d,t}\\). An important assumption here, and in many hierarchical representations of DSTMs, is that the data \\(Z_t(\\cdot)\\) are independent (in time) when conditioned on the true process \\({Y}_t(\\cdot)\\) and parameters \\({\\boldsymbol{\\theta}}_{d,t}\\). Under this conditional-independence assumption, the joint distribution of the data conditioned on the true process and parameters can be represented in product form,\n\\[\n[ \\{Z_t(\\cdot)\\}_{t=1}^T \\; | \\{Y_t(\\cdot)\\}_{t=1}^T , \\{{\\boldsymbol{\\theta}}_{d,t}\\}_{t=1}^T ]\n  \\; = \\; \\prod_{t=1}^T[Z_t(\\cdot) \\; | \\; Y_{t}(\\cdot),{\\boldsymbol{\\theta}}_{d,t}].\n\\tag{5.1}\\]\nThis is one of two key independence/dependence assumptions in DSTMs (the other is discussed in Section 5.1.2 below). Most applications consider the component distributions on the right-hand side of Equation 5.1 to be Gaussian, but it is not uncommon to consider other members of the exponential family of distributions (see Section 5.2.2 below). Indeed, a broader class of data models than the familiar Gaussian model is fairly easy to consider so long as the observations are conditionally independent given the true process. We consider specific examples of data models in Section 5.2.1.\n\n\n5.1.2 Process Model\nPerhaps the most important part of a DSTM is the decomposition of the joint distribution of the process in terms of conditional distributions that respect the time evolution of the spatial process. With \\(Y_t(\\cdot)\\) corresponding to the spatial process at time \\(t\\), we can always factor the joint distribution using the chain rule of conditional probabilities:\n\\[\n\\begin{aligned}\n\\left[Y_0(\\cdot),\\ldots,Y_T(\\cdot)\\right] &= [Y_T(\\cdot) \\mid Y_{T-1}(\\cdot),\\ldots,Y_0(\\cdot)] \\\\\n  &\\quad\\times [Y_{T-1}(\\cdot) \\mid Y_{T-2}(\\cdot),\\ldots,Y_0(\\cdot)] \\times \\dots \\\\\n  &\\quad\\times [Y_2(\\cdot) \\mid Y_1(\\cdot),Y_0(\\cdot)]\\\\\n  &\\quad\\times [Y_1(\\cdot)\\mid Y_0(\\cdot)] \\\\\n  &\\quad\\times [Y_0(\\cdot)],\n\\end{aligned}\n\\]\nwhere, for notational simplicity, the dependence of these distributions on parameters has been suppressed. By itself, this decomposition is not all that useful because it requires a separate conditional model for \\(Y_t(\\cdot)\\) at each \\(t\\). However, if we make a modeling assumption that utilizes conditional independence, then such a hierarchical decomposition can be quite useful. For example, we could make a Markov assumption; that is, conditioned on the past, only the recent past is important to explain the present. Under the first-order Markov assumption that the process at time \\(t\\) conditioned on all of the past is only dependent on the most recent past (and an additional modeling assumption that this process only depends on the current parameters), we get a very useful simplification,\n\\[\n[Y_t(\\cdot) | Y_{t-1}(\\cdot),\\ldots,Y_0(\\cdot), \\{{\\boldsymbol{\\theta}}_{p,t}\\}_{t=0}^{T}] = [Y_t(\\cdot) | Y_{t-1}(\\cdot),{\\boldsymbol{\\theta}}_{p,t}],\n\\tag{5.2}\\]\nfor \\(t=1,2,\\ldots,\\) so that\n\\[\n[Y_0(\\cdot),Y_1(\\cdot),\\ldots,Y_T(\\cdot) | \\{{\\boldsymbol{\\theta}}_{p,t}\\}_{t=0}^T] =  \\left( \\prod_{t=1}^T[Y_t(\\cdot) | Y_{t-1}(\\cdot), {\\boldsymbol{\\theta}}_{p,t}] \\right) [Y_0(\\cdot) | {\\boldsymbol{\\theta}}_{p,0}].\n\\tag{5.3}\\]\nThis is the second of the key assumptions that is usually made for DSTMs (the first was discussed above in Section 5.1.1).\nThis first-order-Markov assumption, which is simple but powerful in spatio-temporal statistics, holds when \\(\\{Y_t(\\cdot)\\}\\) follows a dynamic model of the form\n\\[\n{Y}_t(\\cdot)  =  {\\cal M}({Y}_{t-1}(\\cdot), {\\boldsymbol{\\theta}}_{p,t}, { \\eta}_t(\\cdot)),\\quad t=1,2,\\ldots,\n\\tag{5.4}\\]\nwhere \\({\\boldsymbol{\\theta}}_{p,t}\\) are parameters (possibly with spatial or temporal dependence) that control the process evolution described by the evolution operator \\({\\cal M}\\), and \\(\\eta_t(\\cdot)\\) is a spatial noise (error) process that is independent in time (i.e., \\(\\eta_t(\\cdot)\\) and \\(\\eta_r(\\cdot)\\) are independent for \\(r \\neq t\\)). In general, this model can be linear or nonlinear and the associated conditional distribution, \\([Y_t(\\cdot) \\; | Y_{t-1}(\\cdot)]\\), can be Gaussian or non-Gaussian. As in autoregressive modeling in time series, one can make higher-order Markov assumptions in this case as well, which requires additional time lags of the spatial process to be included on the right-hand side of the conditioning symbol “\\(\\; | \\;\\)” in the component distributions of Equation 5.3. We focus primarily on the first-order-Markov case here, which is usually assumed; however, note that one can always reformulate a higher-order Markov model as a first-order model, albeit increasing the dimensionality of the process, so the first-order representation in terms of probability distributions is actually quite general. One also needs to specify a distribution for the initial state, \\([{Y}_0(\\cdot) | {\\boldsymbol{\\theta}}_{p,0}]\\) or condition on it. We consider specific examples of DSTM process models in Section 5.2.3.\n\n\n5.1.3 Parameters\nA BHM requires distributions to be assigned to the parameters defined in the data model and the process model, namely \\(\\{{\\boldsymbol{\\theta}}_{d,t}, {\\boldsymbol{\\theta}}_{p,t}\\}.\\) Specific distributional forms for the parameters (e.g., spatially or temporally varying dependence on auxiliary covariate information) depend strongly on the problem of interest. Indeed, as mentioned in Chapter 1, one of the most important aspects of “deep” hierarchical modeling is the specification of these distributions, especially when one must deal with the curse of dimensionality. In that case, the primary modeling challenge in DSTMs is to come up with ways to effectively reduce the parameter space. This is illustrated in Section 5.3.1 with regard to linear DSTMs.\nDespite the power of the BHM, in many cases it is possible and sufficient to simply estimate the parameters in an EHM context. This is commonly done in state-space models in time series and often utilizes the expectation-maximization (EM) algorithm or, as is done in the engineering literature, “state augmentation,” where the process “vector” is augmented by the parameters. Again, the choice of the estimation approach is very problem-specific. We give a general EM algorithm for linear DSTMs in Section C.2.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Dynamic Spatio-Temporal Models</span>"
    ]
  },
  {
    "objectID": "Chapter5.html#sec-LGDSTM",
    "href": "Chapter5.html#sec-LGDSTM",
    "title": "5  Dynamic Spatio-Temporal Models",
    "section": "5.2 Latent Linear Gaussian DSTMs",
    "text": "5.2 Latent Linear Gaussian DSTMs\nFor illustration, we consider in this section the simplest (yet, most widely used) DSTM – where the process models in Equation 5.2 are assumed to have additive Gaussian error distributions, and the evolution operator \\({\\cal M}\\) in Equation 5.4 is assumed to be linear. Let us suppose that we are interested in a latent process \\(\\{Y_t(\\mathbf{s}_i)\\}\\) at a set of locations given by \\(\\{\\mathbf{s}_i: i=1,\\ldots,n\\}\\), and that we have data at locations \\(\\{\\mathbf{r}_{j}: j=1,\\ldots,m_t; \\ t=0,1,\\ldots,T\\}\\) (i.e., there could be a different number of data locations for each observation time, but we assume there is a finite set of \\(m\\) possible data locations to be considered; so \\(m_t \\leq m\\)).\nFor simplicity, unless noted otherwise, we assume that the “locations” of interest can have either point or areal support (and, possibly different supports for the prediction locations and data locations).\n\n5.2.1 Linear Data Model with Additive Gaussian Error\nConsider the \\(m_t\\)-dimensional data vector, \\(\\mathbf{Z}_t \\equiv (Z_t(\\mathbf{r}_{1}),\\ldots,Z_t(\\mathbf{r}_{m_t}))'\\), and the \\(n\\)-dimensional latent-process vector, \\(\\mathbf{Y}_t \\equiv (Y_t(\\mathbf{s}_1),\\ldots,Y_t(\\mathbf{s}_n))'\\), that we wish to infer. For the \\(j\\)th observation at time \\(t\\), the linear data model with additive Gaussian error is written as\n\\[\nZ_t(\\mathbf{r}_{j}) = b_t(\\mathbf{r}_{j}) + \\sum_{i=1}^n h_{t,ji} Y_t(\\mathbf{s}_i) + \\epsilon_t(\\mathbf{r}_{j}),\n\\tag{5.5}\\]\nfor \\(t=1,\\ldots,T\\), where \\(b_t(\\mathbf{r}_{j})\\) is an additive offset term for the \\(j\\)th observation at time \\(t\\), \\(\\{h_{t,ji}\\}_{i=1}^n \\equiv \\mathbf{h}_{t,j}'\\) are coefficients that map the latent process to the \\(j\\)th observation at time \\(t\\), and the error term \\(\\epsilon_t(\\cdot)\\) is independent of \\(Y_t(\\cdot)\\). Since \\(j=1,\\ldots,m_t\\), the data model can be written in vector–matrix form as\n\\[\n\\mathbf{Z}_t = \\mathbf{b}_t + \\mathbf{H}_t \\mathbf{Y}_t + \\boldsymbol{\\varepsilon}_t, \\quad \\boldsymbol{\\varepsilon}_t \\sim \\; Gau(\\mathbf{0},\\mathbf{C}_{\\epsilon,t}),\n\\tag{5.6}\\]\nwhere \\(\\mathbf{b}_t\\) is the \\(m_t\\)-dimensional offset term, \\(\\mathbf{H}_t\\) is the \\(m_t \\times n\\) mapping matrix (note that \\(\\mathbf{h}'_{t,j}\\) corresponds to the \\(j\\)th row of \\(\\mathbf{H}_t\\)), and \\(\\mathbf{C}_{\\epsilon,t}\\) is an \\(m_t \\times m_t\\) error covariance matrix, typically taken to be diagonal. Each of the data-model components is described briefly below.\n\nLatent Spatio-Temporal Dynamic Process\nThe latent dynamic spatio-temporal process is represented by \\(\\mathbf{Y}_t\\). This is where most of the modeling effort is focused in the latent linear DSTM framework. It is convenient in many situations to assume that \\(\\mathbf{Y}_t\\) has mean zero; however, we present an alternative perspective in Section 5.3 below. As mentioned previously, we shall focus on first-order Markov models to describe the evolution of \\(\\mathbf{Y}_t\\).\n\n\nAdditive Offset Term\nThere are instances where there are potential biases between the observations and the process of interest, or where one would like to be able to model \\(\\{Y_t(\\cdot)\\}\\) as a mean-zero process. That is, the additive offset term, \\(\\mathbf{b}_t\\), accounts for non-dynamic spatio-temporal structure in the data vector, \\(\\mathbf{Z}_t\\), that allows us to consider \\(\\mathbf{Y}_t\\) to have mean zero. One might still be interested scientifically in predicting the sum \\(\\mathbf{b}_t + \\mathbf{H}_t \\mathbf{Y}_t\\) in Equation 5.6. We may assume that the additive offset term \\(b_t(\\mathbf{r}_{j})\\) is fixed through time, space, or constant across space and time (e.g., \\(b_t(\\mathbf{r}_{j}) \\equiv b(\\mathbf{r}_{j})\\), \\(b_t(\\mathbf{r}_{j}) \\equiv b_t\\), or \\(b_t(\\mathbf{r}_{j}) \\equiv b\\), respectively), or we may define it in terms of covariates (e.g., \\(b_t(\\mathbf{r}_{j}) \\equiv \\mathbf{x}'_{t,j} \\boldsymbol{\\beta}\\), or \\(b_t(\\mathbf{r}_{j}) \\equiv \\mathbf{x}'_{t} \\boldsymbol{\\beta}_j\\), where \\(\\mathbf{x}_{t,j}\\) and \\(\\mathbf{x}_{t}\\) are \\(q\\)-dimensional vectors of covariates and \\(\\boldsymbol{\\beta}\\) and \\(\\boldsymbol{\\beta}_j\\) are \\(q\\)-dimensional parameter vectors). Alternatively, we may consider the offset parameters to be either spatial or temporal random processes with distributions assigned at the next level of the model hierarchy (e.g., \\(\\mathbf{b}_t \\sim \\; Gau(\\mathbf{X}_t \\boldsymbol{\\beta},\\mathbf{C}_b)\\), where \\(\\mathbf{C}_b\\) is a positive-definite matrix constructed using the methods described in Chapter 4).\n\n\nObservation Mapping Function (or Matrix)\nThe observation mapping matrix \\(\\mathbf{H}_t\\) has elements \\(\\{h_{t,ji}\\}\\) that are typically assumed known. They can be any linear basis that relates the process at the prediction locations to the observations. For example, it is often quite useful to let \\(\\mathbf{h}_{t,j}\\) correspond to a simple incidence vector (i.e., a vector of ones and zeros), so that each data location is associated with one or more of the process locations. The incidence vector can easily accommodate missing data or can serve as an “interpolator” such that each observation is related to some weighted combination of the process values.\nIn this simple illustration, consider the single observation in Equation 5.5 where \\(n = 3\\). If \\(\\mathbf{h}_{t,j}' = (0,0,1)\\), it indicates that the observation \\(Z_t(\\mathbf{r}_{j})\\) corresponds to the process value, \\(Y_t(\\mathbf{s}_3)\\), at time \\(t\\). This is especially useful if the locations of the prediction grid are very close to (or a subset of) the observation locations and consequently are considered coincident. If \\(\\mathbf{h}_{t,j}' = (0.1,0.4,0)\\), then the observation at location \\(\\mathbf{r}_{j}\\) corresponds to a weighted sum of the process at locations \\(\\mathbf{s}_1\\) and \\(\\mathbf{s}_2\\), with more weight being given to location \\(\\mathbf{s}_2\\). More generally, these weights can provide a simple way to deal with different spatial supports and orientations of the observations and the process. For example, the weights can correspond to the area of overlap between observation supports and process supports (see Chapter 7 of Cressie & Wikle, 2011 for details).\n\n\n\n\n\n\nTip\n\n\n\nRecall from Section 2.2 that finding the intersections (areas or points of overlap) across spatial or spatio-temporal polygons, points, and grids can be done in a straightforward manner using the function over from the packages sp and spacetime. This function can hence be used to construct the mapping matrices \\(\\{\\mathbf{H}_t\\}\\) in Equation 5.6.\n\n\nFinally, in the situation where the observation locations are a subset of the process locations and \\(m_t &lt; n\\), one has missing data, and this is easily accommodated via the mapping matrix. For example, if \\(m_t = 2\\) and \\(n=3\\), then \\(\\mathbf{Z}_t \\equiv (Z_t(\\mathbf{r}_{t}), Z_t(\\mathbf{r}_{t}))'\\), \\(\\mathbf{Y}_t \\equiv (Y_t(\\mathbf{s}_1), Y_t(\\mathbf{s}_2), Y_t(\\mathbf{s}_3))'\\), and \\(\\boldsymbol{\\varepsilon}_t \\equiv (\\epsilon_t(\\mathbf{r}_{t}), \\epsilon_t(\\mathbf{r}_{t}))'\\). If \\(\\mathbf{r}_{t} = \\mathbf{s}_2\\) and \\(\\mathbf{r}_{t} = \\mathbf{s}_3\\), the mapping matrix in Equation 5.6 is given by the incidence matrix\n\\[\n\\mathbf{H}_t = \\left( \\begin{array}{ccc}\n0 & 1 & 0 \\\\\n0 & 0 & 1 \\end{array}\\right),\n\\tag{5.7}\\]\nwhich indicates that observation \\(Z_t(\\mathbf{r}_{1})\\) corresponds to process value \\(Y_t(\\mathbf{s}_2)\\), observation \\(Z_t(\\mathbf{r}_{2})\\) corresponds to process value \\(Y_t(\\mathbf{s}_3)\\), and process value \\(Y_t(\\mathbf{s}_1)\\) does not have a corresponding observation at time \\(t\\). This way to accommodate missing information is very useful for HMs, because it allows one to focus the modeling effort on the latent process \\(\\{Y_t(\\cdot)\\}\\), and the process is oblivious to which data are missing. Some would argue that a downside of this is the need to pre-specify the locations at which one is interested in modeling the process, but, with a sufficiently fine grid, this could effectively be everywhere in the spatial domain, \\(D_s\\).\nAlthough it is possible in principle to parameterize the mapping matrix and/or estimate it directly in some cases, we shall typically assume that it is known. Otherwise, one would have to be careful when specifying and estimating the process-model parameters to mitigate identifiability problems.\n\n\n\n\n\n\nTip\n\n\n\nRecall from Section 4.4.1 that matrices such as that given in Equation 5.7 tend to have many zeros and hence are sparse. Two R packages that cater to sparse matrices are Matrix and spam. Sparse matrices are stored and operated on differently than the standard dense R matrices, and they have favorable memory and computational properties. However, sparse matrix objects should only be used when the number of non-zero entries is small, generally on the order of 5% or less of the total number of matrix elements.\n\n\n\n\nError Covariance Matrix\nIn the linear Gaussian DSTM, the additive error process \\(\\{\\epsilon_t(\\mathbf{r}_{j})\\}\\) is assumed to have mean zero, is Gaussian, and can generally include dependence in space or time (although we will typically assume that the errors are independent in time, as is customary). So, when considering \\(\\epsilon_t(\\cdot)\\) at a finite set of \\(m_t\\) observation locations, namely \\(\\boldsymbol{\\varepsilon}_t \\equiv (\\epsilon_t(\\mathbf{r}_{1}), \\ldots, \\epsilon_t(\\mathbf{r}_{m_t}))'\\), we need to specify time-varying covariance matrices \\(\\{\\mathbf{C}_{\\epsilon,t}\\}\\). In practice, given that most of the interesting dependence structure in the observations is contained in the process, and recalling that in the data model we are conditioning on that process, the structure of \\(\\mathbf{C}_{\\epsilon,t}\\) should be pretty simple. Indeed, there is often an assumption that these data-model errors are independent with constant (in time and space) variance, so that \\(\\mathbf{C}_{\\epsilon,t} = \\sigma^2_\\epsilon \\mathbf{I}_{m_t}\\), where \\(\\sigma^2_\\epsilon\\) represents the measurement-error variance. If this assumption is not reasonable, then in situations where the data-model error covariance matrix is assumed constant over time (e.g., \\(\\mathbf{C}_{\\epsilon,t} = \\mathbf{C}_{\\epsilon}\\)) and \\(m_t = m\\), one might estimate the \\(m \\times m\\) covariance matrix \\(\\mathbf{C}_{\\epsilon}\\) directly if \\(m\\) is not too large (see Section C.2), or one might parameterize \\(\\mathbf{C}_{\\epsilon}\\). For example, in the case where the data-model error variances are heteroskedastic in space, one might model \\(\\mathbf{C}_{\\epsilon} = \\textrm{diag}(\\mathbf{v}_{\\epsilon})\\), and estimate the elements of \\(\\mathbf{v}_{\\epsilon}\\). Alternatively, if there is spatial dependence one might parameterize \\(\\mathbf{C}_{\\epsilon}\\) in terms of some valid spatial covariance functions (e.g., the Matérn class). The specific choice is very problem-dependent. It is important to recall the central principle of hierarchical modeling discussed in Chapter 1, in Section 3.5, and in greater detail in Section 4.3, which is that we attempt to place as much of the dependence structure as possible in the conditional mean, which simplifies the conditional-covariance specification dramatically.\n\n\n\n5.2.2 Non-Gaussian and Nonlinear Data Model\nRecall the general data model from Section 4.5, rewritten here to correspond to the discrete-time case. For \\(t=1,2,\\ldots,\\) let\n\\[\nZ_t(\\mathbf{s}) | Y_t(\\mathbf{s}), \\gamma \\; \\sim \\; EF(Y_t(\\mathbf{s}), \\gamma),\n\\]\nwhere \\(EF\\) corresponds to a distribution from the with scale parameter \\(\\gamma\\) and mean \\(Y_t(\\mathbf{s})\\). Now, consider a transformation of the mean response \\(g(Y_t(\\mathbf{s})) \\equiv \\tilde{Y}_t(\\mathbf{s})\\) using a specified monotonic link function \\(g(\\cdot)\\). Using a standard GLMM framework, we can model the transformed process \\(\\tilde{Y}_t(\\mathbf{s})\\) as a latent Gaussian DSTM (Section 5.2.3) with or without the use of process/parameter reduction methods (Section 5.3). Note that we can also include a “mapping matrix” to this non-Gaussian data model as we did with the Gaussian data model in Section 5.2.1. That is, in a matrix formulation we could consider\n\\[\n\\mathbf{Z}_t | \\mathbf{Y}_t, \\gamma \\; \\sim \\; EF(\\mathbf{H}_t \\mathbf{Y}_t, \\gamma),\n\\]\nwhere the distribution \\(EF\\) is taken elementwise, and \\(\\mathbf{H}_t\\) is an incidence matrix or change-of-support matrix as described in Section 5.2.1.\nIt is sometimes useful to consider a nonlinear transformation of the latent process \\(\\{Y_t(\\cdot)\\}\\) in a data model even if the error term is Gaussian. For example, analogous to equation (7.39) in Cressie & Wikle (2011), one can modify Equation 5.6 above to accommodate a transformation of the elements of the process vector:\n\\[\n\\mathbf{Z}_t = \\mathbf{b}_t + \\mathbf{H}_t \\mathbf{Y}_t^{a} + \\boldsymbol{\\varepsilon}_t, \\quad \\boldsymbol{\\varepsilon}_t \\sim \\; Gau(\\mathbf{0},\\mathbf{C}_{\\epsilon,t}),\n\\tag{5.8}\\]\nwhere the coefficient \\(\\{-\\infty &lt; a &lt; \\infty\\}\\) corresponds to a (applied to each element of \\(\\mathbf{Y}_t\\)), which is one of the simplest ways to accommodate nonlinear or non-Gaussian processes in the data model. In general, \\(\\{\\mathbf{Y}_t^a\\}\\) may not generate a linear Gaussian model, but the additivity of the errors \\(\\{\\boldsymbol{\\varepsilon}_t\\}\\) is an important part of Equation 5.8. As an example, if \\(\\{Y_t(\\cdot)\\}\\) is positive valued, then this is analogous to the famed Box–Cox transformation. In some applications it is reasonable to assume that the transformation power \\(a\\) in Equation 5.8 may vary with space or time, and may depend on covariates.\nAs with non-dynamic spatio-temporal models with non-Gaussian errors (Chapter 4), computation for estimation and prediction is more problematic when one considers non-Gaussian or nonlinear data models.\n\n\n5.2.3 Process Model\nLinear Markovian spatio-temporal process models generally assume that the value of the process at a given location at the present time is made up of a weighted combination (or is a “smoothed version”) of the process throughout the spatial domain at previous times, plus an additive, Gaussian, spatially coherent “innovation” (see the schematic in Figure 5.1). This is perhaps best represented in a continuous-spatial context through an integro-difference equation (IDE). Specifically, a first-order spatio-temporal IDE process model is given by\n\\[\nY_t(\\mathbf{s}) = \\int_{D_s} m(\\mathbf{s},\\mathbf{x};\\boldsymbol{\\theta}_p) Y_{t-1}(\\mathbf{x}) \\, \\textrm{d}\\mathbf{x}+ \\eta_t(\\mathbf{s}), \\quad \\mathbf{s},\\mathbf{x}\\in D_s,\n\\tag{5.9}\\]\nfor \\(t=1,2,\\ldots,\\) where \\(m(\\mathbf{s},\\mathbf{x};\\boldsymbol{\\theta}_p)\\) is a transition kernel, depending on parameters \\(\\boldsymbol{\\theta}_p\\) that specify “redistribution weights” for the process at the previous time over the spatial domain, \\(D_s\\), and \\(\\eta_t(\\cdot)\\) is a time-varying (but statistically independent in time) continuous mean-zero Gaussian spatial process independent of \\(Y_{t-1}(\\cdot)\\). Generally, one of the parameters of \\(\\boldsymbol{\\theta}_p\\) is just a multiplicative scalar that controls the temporal stability; see Equation 5.23 in Lab 5.2. Note that we assume here, as one often does, that the parameter vector \\(\\boldsymbol{\\theta}_p\\) does not vary with time, but it could do so in general. So, from Equation 5.9, the process at location \\(\\mathbf{s}\\) and time \\(t\\) is given by the weighted average (integral) of the process throughout the domain at the past time, where the weights are given by the transition kernel, \\(m(\\cdot,\\cdot)\\). The innovation given by \\(\\eta_t(\\cdot)\\), which is independent of \\(Y_{t-1}(\\cdot)\\), has spatial dependence, is typically Gaussian, and accounts for spatial dependencies in \\(Y_t(\\cdot)\\) that are not captured by this weighted average. Another way to think about \\(\\eta_t(\\cdot)\\) is that it adds back smaller-scale dependence that is removed in the inherent smoothing that occurs when \\(\\{Y_{t-1}(\\cdot)\\}\\) is averaged over space. In general, \\(\\int_{D_s} m(\\mathbf{s},\\mathbf{x};\\boldsymbol{\\theta}_p) \\,\\textrm{d}\\mathbf{x}&lt; 1\\) is needed for the process to be stable (non-explosive) in time. Note that the model in Equation 5.9 implicitly assumes that the process \\(Y_t(\\cdot)\\) has mean zero. In some cases it may be appropriate to model a non-zero mean directly in the process, as is shown generally in Equation 5.16 below and specifically for the IDE in Lab 5.2.\n\n\n\n\n\n\nFigure 5.1: Cartoon illustration of a linear DSTM. The process at spatial location \\(\\mathbf{s}\\) and time \\(t\\), \\(Y_t(\\mathbf{s})\\), is constructed from a linear combination of the process values at the previous time, \\(Y_{t-1}(\\cdot)\\), plus an “instantaneous” random spatial error process, \\(\\eta_t(\\cdot)\\). The thick arrows indicate the passage from past to present to future.\n\n\n\nIn the case where one has a finite set of prediction spatial locations (or regions) \\(D_s = \\{\\mathbf{s}_1,\\mathbf{s}_2,\\ldots,\\mathbf{s}_n\\}\\) of interest (e.g., an irregular lattice or a regular grid), the first-order IDE evolution process model Equation 5.9 can be discretized and written as a stochastic difference equation,\n\\[\nY_t(\\mathbf{s}_i) = \\sum_{j=1}^n m_{ij}(\\boldsymbol{\\theta}_p) \\;Y_{t-1}(\\mathbf{s}_j) + \\eta_t(\\mathbf{s}_i),\n\\tag{5.10}\\]\nfor \\(t=1,2,\\ldots,\\) with transition (redistribution) weights \\(m_{ij}(\\boldsymbol{\\theta}_p)\\) that depend on parameters \\(\\boldsymbol{\\theta}_p\\). In this case, the process at \\(Y_t(\\mathbf{s}_i)\\) considers a weighted combination of the values of the process at time \\(t-1\\) and at a discrete set of spatial locations.\nNow, denoting the process vector \\(\\mathbf{Y}_t \\equiv (Y_t(\\mathbf{s}_1),\\ldots,Y_t(\\mathbf{s}_n))'\\), Equation 5.10 can be written in vector–matrix form as a linear first-order vector autoregression DSTM,\n\\[\n\\mathbf{Y}_t = \\mathbf{M}\\mathbf{Y}_{t-1} + \\boldsymbol{\\eta}_t,\n\\tag{5.11}\\]\nwhere the \\(n \\times n\\) transition matrix is given by \\(\\mathbf{M}\\) with elements \\(\\{m_{ij}\\}\\), and the additive spatial error process \\(\\boldsymbol{\\eta}_t \\equiv (\\eta_t(\\mathbf{s}_1),\\ldots,\\eta_t(\\mathbf{s}_n))'\\) is independent of \\(\\mathbf{Y}_{t-1}\\) and is specified to be mean-zero and Gaussian with spatial covariance matrix \\(\\mathbf{C}_{\\eta}\\). The stability (non-explosive) condition in this case requires that the maximum modulus of the eigenvalues of \\(\\mathbf{M}\\) (which may be complex-valued) be less than 1 (see Note 5.1).\nWe have assumed in our discussion of the process model that the \\(\\{Y_t(\\mathbf{s}_i)\\}\\) have mean zero. Although it is possible to include an offset term in the Markovian process model at this stage, in this section we consider such an offset only in the data model as described above for Equation 5.6. However, as we discuss below in Section 5.3, it is reasonable to consider the offset as part of this “process” decomposition, typically including covariate effects and/or seasonality.\nUsually, \\(\\mathbf{M}\\) and \\(\\mathbf{C}_{\\eta}\\) are assumed to depend on parameters \\(\\boldsymbol{\\theta}_p\\) and \\(\\boldsymbol{\\theta}_\\eta\\), respectively, to mitigate the curse of dimensionality (here, the exponential increase in the number of parameters) that often occurs in spatio-temporal modeling. As discussed below in Section 5.3, the parameterization of these matrices (particularly \\(\\mathbf{M}\\)) is one of the greatest challenges in DSTMs, and it is facilitated by using parameter models in a BHM. However, in relatively simple applications of fairly low dimensionality and large sample sizes (e.g., when \\(n\\) is small and \\(T \\gg n\\)), one can estimate \\(n \\times n\\) matrices \\(\\mathbf{M}\\) and \\(\\mathbf{C}_{\\eta}\\) directly in an EHM, as is commonly done in state-space models of time series (see Section C.2).\n\n\n\n\n\n\nNote 5.1: Eigenvalues of the Transition Matrix\n\n\n\nConsider the first-order vector autoregressive model,\n\\[\n\\mathbf{Y}_t = \\mathbf{M}\\mathbf{Y}_{t-1} + \\boldsymbol{\\eta}_t,\n\\]\nwhere \\(\\mathbf{Y}_t\\) is an \\(n\\)-dimensional vector, and \\(\\mathbf{M}\\) is an \\(n \\times n\\) real-valued transition matrix. The characteristic equation obtained from the determinant,\n\\[\n\\text{det}(\\mathbf{M}- \\lambda \\mathbf{I}) = 0,\n\\]\nhas \\(n\\) eigenvalues (latent roots), \\(\\{\\lambda_i: i=1,\\ldots,n\\}\\), some of which may be complex numbers. Each eigenvalue has a modulus and is associated with an eigenvector (taken together, an eigenvalue–eigenvector pair is sometimes referred to as an eigenmode) that describes the behavior associated with that eigenvalue. As discussed in Cressie and Wikle (2011, Section 3.2.1), the eigenvalues and eigenvectors can tell us quite a bit about the dynamical properties of the model. First, assume in general that \\(\\lambda_i = a_i + b_i \\sqrt{-1}\\) (where \\(b_i = 0\\) if \\(\\lambda_i\\) is real-valued), and define the complex magnitude (or “modulus”) to be \\(|\\lambda_i| = \\sqrt{a_i^2 + b_i^2}\\). We note that if \\(\\text{max}\\{|\\lambda_i|: i=1,\\ldots,n\\} \\ge 1\\), then the eigenmode, and hence the model, is unstable, and \\(\\mathbf{Y}_t\\) will grow without bound as \\(t\\) increases. Conversely, if the maximum modulus of all the eigenvalues is less than 1, then the model is stable. Since \\(\\mathbf{M}\\) is real-valued, complex eigenvalues come in complex conjugate pairs, and their eigenmodes are associated with oscillatory behavior in the dynamics (either damped or exponentially growing sinusoids, depending on whether the modulus of the corresponding eigenvalue is less than 1 or greater than or equal to 1, respectively). In contrast, real-valued eigenvalues correspond to non-oscillatory dynamics.\n\n\n\nIntuition for Linear Dynamics\nParameterizations of realistic dynamics should respect the fact that spatio-temporal interactions are crucial for dynamic propagation. For example, in the linear IDE model (Equation 5.9), the asymmetry and rate of decay of the transition kernel \\(m(\\mathbf{s},\\mathbf{x};\\boldsymbol{\\theta}_p)\\), relative to a location (here, \\(\\mathbf{s}\\)), control propagation (linear advection) and spread (diffusion), respectively. Figure 5.2 shows Hovmöller plots of four one-dimensional (in space) simulations of a spatio-temporal IDE process and their respective transition kernels evaluated at \\(s_0 = 0.5\\). Panels (a) and (b) show the inherent diffusive nature of the process depending on kernel width; that is, spatially coherent disturbances tend to spread across space (diffuse) at a greater rate when the kernel is wider (i.e., has a larger aperture), which leads to more averaging from one time to the next. However, note that there is no “slanting” in the plot through time, indicating that there is no propagation through time (see Section 2.3.3 for an interpretation of Hovmöller plots). In contrast, panels (c) and (d) show clear evidence of propagation, to the left when the kernel is offset to the right, and to the right when the kernel is offset to the left. The intuition here is that the offset kernel pulls information from one particular direction, and redistributes it in the opposite direction, leading to propagation. More complex kernels (e.g., multimodal, or spatially varying) can lead to even more complex behavior. As we discuss in Section 5.3, these basic properties of the transition (redistribution) kernel can suggest efficient parameterizations of linear DTSM process models.\n\n\n\n\n\n\nFigure 5.2: Transition kernels \\(m(0.5, x; \\boldsymbol{\\theta}_p)\\) for different \\(\\boldsymbol{\\theta}_p\\) and associated Hovmöller plots of spatio-temporal IDE process simulation (one-dimensional in space) with \\(iid\\) noise forcing. (a) Relatively narrow symmetric kernel. (b) Wider symmetric kernel. (c) Asymmetric kernel produced by shifting a symmetric kernel to the right. (d) Same as (c), but to the left. Note that the wider the kernel the greater the diffusion, and a shift implies propagation in the direction away from the shift.\n\n\n\nAs mentioned above, there are conditions on the transition kernel (or matrix in the discrete-space case) that correspond to unstable (explosive in time) behavior. From a dynamic perspective, a stable process implies that small perturbations to the spatial field will eventually decay to the equilibrium (mean) state. Because many real-world spatio-temporal processes are nonlinear, it can be the case that if one fits an unconstrained linear DSTM to data that come from such a nonlinear process, then the fitted model is unstable (explosive, with exponential “growth”). This is not necessarily a bad thing, as it provides immediate feedback that the wrong model is being fitted or that the finite-time window for the observations suggests a transient period of growth (see Note 5.2). In some cases, it can actually be helpful if the confidence (or credible) intervals of the transition-matrix parameters include the explosive boundary because the mean of the predictive distribution may show growth (a nonlinear feature) since it effectively averages over realizations that are both explosive and non-explosive. Of course, long-lead-time forecasts from such a model are problematic as exponential-growth models are only useful for very short-term predictions unless there is some nonlinear control mechanism (e.g., density-dependent carrying capacities in ecological applications).\n\n\n\n\n\n\nNote 5.2: Transient Growth\n\n\n\nAn interesting and less appreciated aspect of linear DSTMs is the fact that they can be stable yet still accommodate so-called “transient growth.” That is, there are periods in time when the process does have brief excursions from its stable state. Essentially, if the transition operator is “non-normal” (i.e., in the discrete-space case, if \\(\\mathbf{M}\\mathbf{M}' \\neq \\mathbf{M}' \\mathbf{M}\\), in which case, the eigenvectors of \\(\\mathbf{M}\\) are non-orthogonal), but still stable (e.g., the maximum modulus of the eigenvalues of \\(\\mathbf{M}\\) is less than 1; see Note 5.1), then the linear dynamic process can exhibit transient growth. This means that even though each eigenvector of the stable \\(\\mathbf{M}\\) is decaying asymptotically in time, there can be local-in-time (transient) periods where there is significant (even orders of magnitude) growth. This is due to the constructive interference of the non-orthogonal eigenvectors of the transition operator, \\(\\mathbf{M}\\). Since almost all real-world linear processes correspond to non-normal transition operators, this has important implications concerning how one might parameterize \\(\\mathbf{M}\\), as discussed in Section 5.3 below.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Dynamic Spatio-Temporal Models</span>"
    ]
  },
  {
    "objectID": "Chapter5.html#sec-ProcParamRed",
    "href": "Chapter5.html#sec-ProcParamRed",
    "title": "5  Dynamic Spatio-Temporal Models",
    "section": "5.3 Process and Parameter Dimension Reduction",
    "text": "5.3 Process and Parameter Dimension Reduction\nThe latent linear Gaussian DSTM described in Section 5.2 above has unknown parameters associated with the process model \\(\\mathbf{C}_\\eta\\), the transition operator \\(m(\\mathbf{s},\\mathbf{x};\\boldsymbol{\\theta}_p)\\) or matrix \\(\\mathbf{M}\\), and the initial-condition distribution (e.g., \\(\\boldsymbol{\\mu}_0\\) and \\(\\mathbf{C}_0\\)). With the linear Gaussian data model, one typically considers a fairly simple parameterization of \\(\\mathbf{C}_\\epsilon\\) (e.g., \\(\\mathbf{C}_\\epsilon = \\sigma^2_\\epsilon \\mathbf{I}\\)) or perhaps the covariance matrix implied by a simple spatial random process that has just a few parameters (e.g., a Matérn spatial covariance function or a spatial conditional autoregressive process). One of the greatest challenges when considering DSTMs in hierarchical statistical settings is the curse of dimensionality associated with the process-model level of the DSTM. For the fairly common situation where the number of spatial locations (\\(n\\)) is much larger than the number of time replicates (\\(T\\)), even the fairly simple linear DSTM process model Equation 5.11 is problematic, as there are on the order of \\(n^2\\) parameters to estimate. To proceed, one must reduce the number of free parameters to be inferred in the model and/or reduce the dimension of the spatio-temporal dynamic process. These two approaches are discussed briefly below.\n\n5.3.1 Parameter Dimension Reduction\nConsider the process-error spatial variance-covariance matrix, \\(\\mathbf{C}_\\eta\\). In complex modeling situations, it is seldom the case that one would estimate this as a full positive-definite matrix in the DSTM. Rather, given that these are spatial covariance matrices, we would either use one of the common spatial covariance-function representations or a basis-function random-effects representation (as in Chapter 4 or in Section 5.3.2 below).\nGenerally, the transition-matrix parameters in the DSTM process model require the most care, as there could be as many as \\(n^2\\) of them and, as discussed above, the linear dynamics of the process are largely controlled by these parameters. In the case of the simple linear DSTM model Equation 5.11, one could parameterize the transition matrix \\(\\mathbf{M}\\) simply as a random walk (i.e., \\(\\mathbf{M}= \\mathbf{I}\\)), a spatially homogeneous autoregressive process (i.e., \\(\\mathbf{M}= \\theta_p \\mathbf{I}\\)), or a spatially varying autoregressive process (\\(\\mathbf{M}= \\mbox{diag}(\\boldsymbol{\\theta}_p)\\)). The first two parameterizations are somewhat unrealistic for most real-world dynamic processes and are not recommended, but the last parameterization is more useful for real-world processes.\nAs an example of the last parameterization described above, consider the process model where \\(\\mathbf{C}_\\eta = \\sigma_\\eta^2 \\mathbf{I}\\), and \\(\\mathbf{M}= \\mbox{diag}(\\boldsymbol{\\theta}_p)\\). We can decompose the first-order conditional distributions in this case as\n\\[\n[\\mathbf{Y}_t | \\mathbf{Y}_{t-1}, \\boldsymbol{\\theta}_p, \\sigma^2_\\eta] = \\prod_{i=1}^n [Y_t(\\mathbf{s}_i) | Y_{t-1}(\\mathbf{s}_i), \\theta_{p}(i), \\sigma^2_\\eta],\\quad t=1,2,\\ldots.\n\\]\nThus, conditional on the parameters \\(\\boldsymbol{\\theta}_p = (\\theta_p(1),\\ldots,\\theta_p(n))'\\), we have spatially independent univariate AR(1) processes at each spatial location (i.e., only the \\(Y\\)-value at the previous time at the same spatial location influences the transition). However, if \\(\\boldsymbol{\\theta}_p\\) is random and has spatial dependence, then if we integrate it out, the marginal conditional distribution, \\([\\mathbf{Y}_t | \\mathbf{Y}_{t-1}, \\sigma^2_\\eta]\\), can imply that all of the elements of \\(\\mathbf{Y}_{t-1}\\) influence the transition to time \\(t\\) at all spatial locations (i.e., this is a spatio-temporal process). Recall from Section 4.3 that this building of dependence through marginalization is a fundamental principle of deep hierarchical modeling, and it provides a simple and often effective way to construct complex spatio-temporal models (see also Note 4.3). But, although we can accommodate fairly complex spatio-temporal dependence in this marginalization, it is important to note that the conditional model does not account directly for interactions across space and time. This limits its utility in applications, where more realistic conditional dynamic specifications are required. Thus, we often seek parameterizations that directly include such interactions in the conditional model.\nRecall from our discussion of the intuition behind linear dynamics in Section 5.2.3 that the transition kernel is very important. This suggests that we can model realistic linear behavior by parameterizing the kernel shape (particularly its decay in the spatial domain and its asymmetry) in terms of a relatively small number of parameters (e.g., in the transition kernel case, the kernel width, or variance, and shift, or mean, parameters). More importantly, if we allow these relatively few parameters to vary with space in a principled fashion, then we can accommodate a variety of quite complex dynamic behaviors. The strength of the HM approach is that one can fairly easily do this by endowing these kernel parameters with spatial structure at the parameter-model level of the hierarchy (e.g., allowing them to be a function of covariates and/or specifying them as spatial random processes).\nAs an example, consider the IDE process model given in Equation 5.9, where we specify a Gaussian-shape transition kernel as a function of \\(x\\) relative to the location \\(s\\) (for simplicity, in a one-dimensional spatial domain):\n\\[\nm(s,x;\\boldsymbol{\\theta}_p) = {\\theta_{p,1}} \\exp\\left(-\\frac{1}{\\theta_{p,2}}(x - \\theta_{p,3} - s)^2 \\right) ,\n\\tag{5.12}\\]\nwhere the kernel amplitude is given by \\(\\theta_{p,1}\\), the length-scale (variance) parameter \\(\\theta_{p,2}\\) corresponds to a kernel scale (aperture) parameter (i.e., the kernel width increases as \\(\\theta_{p,2}\\) increases), and the mean (shift) parameter \\(\\theta_{p,3}\\) corresponds to a shift of the kernel relative to location \\(s\\). Notice that Equation 5.12 is positive but need not integrate to 1 over \\(x\\). Recall from Figure 5.2 the dynamical implication of changing the shift parameter. Specifically, if \\(\\theta_{p,3}\\) is positive (negative) it leads to leftward (rightward) movement because it induces asymmetry relative to location \\(s\\). In addition, Figure 5.2 shows the dynamic implications when changing the kernel width/scale (e.g., wider kernels suggest faster decay). So, to obtain more complex dynamical behavior, we can allow these parameters to change with space. For example, suppose the mean (shift) parameter satisfies \\(\\theta_{p,3}(s) = \\mathbf{x}(s)' \\boldsymbol{\\beta}+ \\omega(s)\\), where \\(\\mathbf{x}(s)\\) corresponds to covariates at spatial location \\(s\\), \\(\\boldsymbol{\\beta}\\) are the associated regression parameters, and \\(\\omega(s)\\) could correspond to a spatial (although, in some cases, it may be sufficient to omit the error term \\(\\omega(s)\\); see Lab 5.2 for an example). We can also allow the parameter \\(\\theta_{p,2}\\) to vary with space, but it is typically the case that \\(\\theta_{p,3}\\) is the more important of the two parameters. Figure 5.3 shows an example of a spatially varying kernel in two dimensions, and the kernel evaluated at one specific spatial location. Lab 5.1 implements the simple one-dimensional IDE process model and explores its simulation. Lab 5.2 shows how one can do spatio-temporal modeling and inference in \\(\\texttt{R}\\) using the package IDE.\n\n\n\n\n\n\nFigure 5.3: An example of a spatially varying kernel in an IDE spatio-temporal model. The left panel shows the direction (arrow orientation and color) and magnitude (arrow length) of flow induced by the kernel as a function of \\(\\mathbf{x}= (x_1,x_2)'\\) in two-dimensional space. The red cross indicates a specific location in space \\((s_1^o, s_2^o)\\) around which the kernel is evaluated and plotted in the right panel. Note that the kernel, which is shifted to the right, induces a flow to the left.\n\n\n\nAlthough the IDE kernel representation suggests efficient parameterizations for linear dynamics in continuous space, there are many occasions where we seek efficient parameterizations in a discrete-space setting or in the context of random effects in basis-function expansions. In the case of the former, one of the most useful such parameterizations corresponds to transition operators that only consider local spatial neighborhoods. We describe these below and provide a mechanistically motivated example. We defer the discussion of dynamics for random effects in basis-function expansions to Section 5.3.2.\n\nLagged-Nearest-Neighbor Representations\nThe importance of the rate of decay and asymmetry in IDE transition-kernel representations suggests that for discrete space a very parsimonious, yet realistic, dynamic model can be specified in terms of a simple lagged-nearest-neighbor (LNN) parameterization, for example,\n\\[\nY_t(\\mathbf{s}_i) = \\sum_{\\mathbf{s}_j \\in {\\cal N}_i} m_{ij} Y_{t-1}(\\mathbf{s}_j) + \\eta_t(\\mathbf{s}_i),\n\\tag{5.13}\\]\nwhere \\({\\cal N}_i\\) corresponds to a pre-specified neighborhood of the location \\(\\mathbf{s}_i\\) (including \\(\\mathbf{s}_i\\)), for \\(i=1,\\ldots,n\\), and where we specify \\(m_{ij} = 0, \\mbox{ for all }  \\;\\mathbf{s}_j \\not\\in {\\cal N}_i\\). Such a parameterization reduces the number of free parameters from the order of \\(n^2\\) to the order of \\(n\\). It is often reasonable to further parameterize the transition coefficients in Equation 5.13 to account for decay (spread or diffusion) rate and asymmetry (propagation direction). In some cases, homogeneities of the transitions would result in a single parameter to control a particular type of neighbor (e.g., a parameter for the west neighbor and east neighbor transition coefficients), or, in other cases, it would be more appropriate to let these parameters vary in space as well (as with the IDE transition-kernel example above).\n\n\nMotivation of an LNN with a Mechanistic Model\nThe LNN parameterization can be motivated by many mechanistic models, such as those suggested by standard discretization of integro-differential or partial differential equations (PDEs). In the latter case, the parameters \\(m_{ij}\\) in Equation 5.13 can be parameterized in terms of other mechanistically motivated knowledge, such as spatially varying diffusion or advection coefficients. Again, in this framework the \\(\\{m_{ij}\\}\\) are either estimated directly in an EHM or modeled at the next level of a BHM (typically, with some sort of spatial structure). As an example, consider the basic linear, non-random, advection–diffusion PDE,\n\\[\n\\frac{\\partial Y}{\\partial t} =  a \\frac{\\partial^2 Y}{\\partial x^2}+ b \\frac{\\partial^2 Y}{\\partial y^2} + u \\frac{\\partial Y}{\\partial x} + v \\frac{\\partial Y}{\\partial y},\n\\tag{5.14}\\]\nconditional on \\(a\\), \\(b\\), \\(u\\), and \\(v\\), where \\(a\\) and \\(b\\) are diffusion coefficients that control the rate of spread, and \\(u\\) and \\(v\\) are advection parameters that account for the process “flow” (i.e., advection). Simple finite-difference discretization of such PDEs on a two-dimensional equally spaced finite grid (see Section D.1) can lead to LNN specifications of the form\n\\[\n\\mathbf{Y}_t = \\mathbf{M}(\\boldsymbol{\\theta}_p) \\mathbf{Y}_{t-1} + \\mathbf{M}_b(\\boldsymbol{\\theta}_p) \\mathbf{Y}_{b,t} + \\boldsymbol{\\eta}_t,\n\\]\nwhere \\(\\mathbf{Y}_t\\) corresponds to a vectorization of the non-boundary grid points, with \\(\\mathbf{M}(\\boldsymbol{\\theta}_p)\\) a five-diagonal transition matrix with diagonals corresponding to functions of \\(a\\), \\(b\\), \\(u\\), \\(v\\) and the discretization parameters (e.g., these five diagonals correspond to \\(\\{\\theta_{p,1}, \\ldots,\\theta_{p,5}\\}\\) in Section D.1). Such discretizations should account for boundary affects, and so we specify \\(\\mathbf{Y}_{b,t}\\) to be a boundary process (either fixed or assumed to be random) with \\(\\mathbf{M}_b(\\boldsymbol{\\theta}_p)\\) the associated transition operator based on the finite-difference discretization of the differential operator. The additive error process \\(\\{\\boldsymbol{\\eta}_t\\}\\) is assumed to be Gaussian, mean-zero, and independent in time. In the more realistic case where the parameters \\(a, b\\) and/or \\(u, v\\) vary with space, the vector \\(\\boldsymbol{\\theta}_p\\) varies with space as well, and we model it either in terms of covariates or as a spatial random process. The point is that we allow these mechanistic models to suggest or motivate LNN parameterizations rather than our specifying the structure directly. Appendix D presents detailed examples of DSTMs motivated by mechanistic models, and the case study in Appendix E presents an implementation of such a model for the Mediterranean winds data set described in Chapter 2.\n\n\n\n5.3.2 Dimension Reduction in the Process Model\nAs discussed in Section 4.4, it is often the case that to reduce process dimensionality we could consider the spatio-temporal process of interest as a decomposition in terms of “fixed” effects and random effects in a basis-function expansion. This is particularly helpful for DSTM process models, as it is often the case that the important dynamics exist on a fairly low-dimensional space (i.e., manifold). Consider an extension to the spatial basis-function mixed-effects model (Equation 4.29) from Section 4.4.2,\n\\[\nY_t(\\mathbf{s}) = \\mathbf{x}_t(\\mathbf{s})' \\boldsymbol{\\beta}+ \\sum_{i=1}^{n_\\alpha} \\phi_i(\\mathbf{s}) \\alpha_{i,t} + \\sum_{j=1}^{n_\\xi} \\psi_j(\\mathbf{s}) \\xi_{j,t} + \\nu_t(\\mathbf{s}),\n\\tag{5.15}\\]\nwhere the term with covariates, \\(\\mathbf{x}_t(\\mathbf{s})' \\boldsymbol{\\beta}\\), might be interpreted as a “fixed” or “deterministic” component with fixed effects \\(\\boldsymbol{\\beta}\\); the first basis-expansion term, \\(\\sum_{i=1}^{n_\\alpha} \\phi_i(\\mathbf{s}) \\alpha_{i,t}\\), contains known spatial \\(\\{\\phi_i(\\cdot)\\}\\) and associated dynamically evolving random coefficients (i.e., random effects), \\(\\{\\alpha_{i,t}\\}\\); the residual basis-expansion term, \\(\\sum_{j=1}^{n_\\xi} \\psi_j(\\mathbf{s}) \\xi_{j,t}\\), can account for non-dynamic spatio-temporal structure, where the , \\(\\{\\psi_j(\\cdot)\\}\\), are again assumed known, and the random effects \\(\\{\\xi_{j,t}\\}\\) are typically non-dynamic or at least contain simple temporal behavior. The micro-scale term, \\(\\nu_t(\\cdot)\\), is assumed to be a with mean zero and independent in time. The focus here is on the dynamically evolving random effects, \\(\\{\\alpha_{i,t}\\}\\).\nAs mentioned above, useful reductions in process dimension can be formulated with the understanding that the essential dynamics for spatio-temporal processes typically exist in a fairly low-dimensional space. This is helpful because, instead of having to model the evolution of, say, the \\(n\\)-dimensional vector \\(\\mathbf{Y}_t\\), one can model the evolution of a much lower-dimensional (of dimension \\(n_\\alpha\\)) process \\(\\{\\boldsymbol{\\alpha}_t\\}\\), where \\(n_\\alpha \\ll n\\). It is helpful to consider the vector form of Equation 5.15:\n\\[\n\\mathbf{Y}_t = \\mathbf{X}_t \\boldsymbol{\\beta}+  \\boldsymbol{\\Phi}\\boldsymbol{\\alpha}_t + \\boldsymbol{\\Psi}\\boldsymbol{\\xi}_t + \\boldsymbol{\\nu}_t,\n\\tag{5.16}\\]\nwhere \\(\\mathbf{X}_t\\) is an \\(n \\times p\\) matrix that could be time-varying and can be interpreted as a spatial offset corresponding to large-scale non-dynamical features and/or covariate effects, \\({\\boldsymbol{\\Phi}}\\) is an \\(n \\times n_\\alpha\\) matrix of basis vectors corresponding to the latent dynamic coefficient process, \\(\\{\\boldsymbol{\\alpha}_t\\}\\), and \\(\\boldsymbol{\\Psi}\\) is an \\(n \\times n_\\xi\\) matrix of basis vectors corresponding to the latent coefficient process, \\(\\{\\boldsymbol{\\xi}_t\\}\\). Typically, \\(\\{\\boldsymbol{\\xi}_t\\}\\) is assumed to have different dynamic characteristics than \\(\\{\\boldsymbol{\\alpha}_t\\}\\), or this component might account for non-dynamic spatial variability. The error process \\(\\{\\boldsymbol{\\nu}_t\\}\\) is Gaussian and assumed to have mean zero with relatively simple temporal dependence structure (usually independence).\nThe evolution of the latent process \\(\\{\\boldsymbol{\\alpha}_t\\}\\) can proceed according to the linear equations involving a transition matrix, discussed earlier. For example, one could specify a first-order vector autoregressive model (VAR(1)),\n\\[\n\\boldsymbol{\\alpha}_t = \\mathbf{M}_\\alpha \\boldsymbol{\\alpha}_{t-1} + \\boldsymbol{\\eta}_t,\n\\tag{5.17}\\]\nwhere \\(\\mathbf{M}_\\alpha\\) is the \\(n_\\alpha \\times n_\\alpha\\) transition matrix, and \\(\\boldsymbol{\\eta}_t \\sim Gau(\\mathbf{0},\\mathbf{C}_\\eta)\\) (which are assumed to be independent of \\(\\boldsymbol{\\alpha}_{t-1}\\) and independent in time). The matrices \\(\\mathbf{M}_\\alpha\\) and \\(\\mathbf{C}_\\eta\\) in Equation 5.17 are often relatively simple in structure, depending on the nature of the real-world process and the type of basis functions considered. However, even in this low-dimensional context (\\(n_\\alpha \\ll n\\)), in many cases parameter-space reduction may still be necessary. One could consider the simple structures that were discussed in the context of linear DSTM process models (e.g., random walks, independent AR models, nearest-neighbor models). Typically, it is important, for the reasons discussed in Section 5.2.3 and Note 5.2, that the transition operator be non-normal (i.e., \\(\\mathbf{M}_\\alpha' \\mathbf{M}_\\alpha \\neq \\mathbf{M}_\\alpha \\mathbf{M}_\\alpha'\\)), so one should consider non-diagonal transition matrices in most cases. Also, the notion of “neighbors” is not always well defined in these formulations. If the basis functions given in \\(\\boldsymbol{\\Phi}\\) are such that the elements of \\(\\boldsymbol{\\alpha}_t\\) are not spatially indexed (e.g., in the case of global basis functions such as some types of splines, Fourier, EOFs, etc.), then a neighbor cannot be based on physical space (but perhaps it can be based on other characteristics, such as spatial scale). It is important to note that mechanistic knowledge can also be used in this case to motivate parameterizations for \\(\\mathbf{M}_\\alpha\\). We illustrate a couple of such cases, one for a “spectral” representation of a PDE in Section D.2, and one for an IDE process in Section D.3. Lab 5.3 provides an example in which \\(\\mathbf{M}_\\alpha\\) and \\(\\mathbf{C}_\\eta\\) are estimated by the method of moments and by an EM algorithm (see Appendix C for more details about these algorithms).\n\n\n\n\n\n\nTip\n\n\n\nIf one is able to write down the data model as \\(\\mathbf{Z}_t =  \\mathbf{H}_t \\mathbf{Y}_t + \\boldsymbol{\\varepsilon}_t,\\ \\boldsymbol{\\varepsilon}_t \\sim Gau(\\mathbf{0},\\mathbf{C}_{\\epsilon,t}),\\) and the process model as \\(\\mathbf{Y}_t = \\mathbf{M}\\mathbf{Y}_{t-1} + \\boldsymbol{\\eta}_t,\\ \\boldsymbol{\\eta}_t \\sim Gau(\\mathbf{0},\\mathbf{C}_{\\eta})\\), where the \\(\\{\\mathbf{H}_t\\}\\) are known, then the problem of predicting \\(\\{\\boldsymbol{\\alpha}_t\\}\\) and the estimation of all the other parameters is the well-known dual state-parameter estimation problem for state-space models (see Appendix C). Several R packages are available for this, such as KFAS, MARSS, and Stem. Software for DSTMs is, however, less developed than that for descriptive models, and estimation/prediction with complex linear DSTMs and nonlinear DSTMs is likely to require customized R code.\n\n\n\nBasis Functions\nIn the mechanistically motivated PDE and IDE cases presented in Appendices Section D.2 and Section D.3, the natural choice for basis functions are the Fourier modes (i.e., sines and cosines). This is typically not the case for DSTM process models. Indeed, there are many choices for the basis functions that could be used to define \\(\\boldsymbol{\\Phi}\\) and \\(\\boldsymbol{\\Psi}\\) in Equation 5.16 (see, for example, Figure 4.7). In the context of DSTMs, it is usually important to specify basis functions such that interactions across spatial scales are allowed to accommodate transient growth. This can be more difficult to do in “knot-based” representations (e.g., splines, kernel convolutions, predictive processes), where the coefficients \\(\\boldsymbol{\\alpha}_t\\) of \\(\\boldsymbol{\\Phi}\\) are spatially referenced but not necessarily multi-resolutional. Most other basis-function representations are in some sense multi-scale, and the associated expansion coefficients \\(\\{\\boldsymbol{\\alpha}_t\\}\\) are not indexed in space. In this case, the dynamical evolution in the DSTM can easily accommodate scale interactions. The example in Lab 5.3 uses EOFs as the basis functions in such a decomposition of SSTs. Recall that the coefficients \\(\\{\\boldsymbol{\\xi}_t\\}\\) associated with the matrix \\(\\boldsymbol{\\Psi}\\) are typically specified to have much simpler dynamic structure (if at all), since the controlling dynamics are assumed to be associated principally with \\(\\{\\boldsymbol{\\alpha}_t\\}\\). Thus, one has more freedom in the choice of basis functions that define \\(\\boldsymbol{\\Psi}\\).",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Dynamic Spatio-Temporal Models</span>"
    ]
  },
  {
    "objectID": "Chapter5.html#sec-nonlinearDSTMs",
    "href": "Chapter5.html#sec-nonlinearDSTMs",
    "title": "5  Dynamic Spatio-Temporal Models",
    "section": "5.4 Nonlinear DSTMs",
    "text": "5.4 Nonlinear DSTMs\nThe linear Gaussian DSTMs described in Section 5.2 and Section 5.3 are widely used, but the state of the art for more complicated models is rapidly advancing. The purpose of this section is not to give a complete overview of these more advanced models but just a brief perspective on nonlinear DSTMs without the implementation details.\nMany mechanistic processes are best modeled nonlinearly, at least at some spatial and temporal scales of variability. We might write this as a nonlinear spatio-temporal AR(1) process (of course, higher-order lags could be considered as well):\n\\[\nY_t(\\cdot) = {\\cal M}(Y_{t-1}(\\cdot), \\eta_t(\\cdot);\\boldsymbol{\\theta}_p), \\quad t=1,2,\\ldots,\n\\tag{5.18}\\]\nwhere \\({\\cal M}\\) is a nonlinear function that models the process transition from time \\(t-1\\) to \\(t\\), \\(\\eta_t(\\cdot)\\) is an error process, and \\(\\boldsymbol{\\theta}_p\\) are parameters. Unfortunately, although there is one basic linear model, there are an infinite number of nonlinear statistical models that could be considered. One could either take a nonparametric view of the problem and essentially learn the dynamics from the data, or one could propose specific model classes that can accommodate the type of behavior desired. In this section we briefly describe examples of these approaches to accommodate nonlinear spatio-temporal dynamics.\n\nState-Dependent Models\nThe general nonlinear model (Equation 5.18) can be simplified by considering a state-dependent model (the terminology comes from the time-series literature, where these models were first developed), in which the transition matrix depends on the process (state) value at each time. For example, in the discrete spatial case, we can write\n\\[\n\\mathbf{Y}_t = \\mathbf{M}(\\mathbf{Y}_{t-1};\\boldsymbol{\\theta}_p) \\; \\mathbf{Y}_{t-1} + \\boldsymbol{\\eta}_t,\n\\tag{5.19}\\]\nwhere the transition operator depends on \\(\\mathbf{Y}_{t-1}\\) and parameters \\(\\boldsymbol{\\theta}_p\\) (which, more generally, may also vary with time and/or space). Models such as Equation 5.19 are still too general for spatio-temporal applications and must be further specified. One type of state-dependent model is the threshold vector autoregressive model, given by\n\\[\n\\mathbf{Y}_t =  \\left\\{\n\\begin{array}{ll}\n\\mathbf{M}_1 \\mathbf{Y}_{t-1} + \\boldsymbol{\\eta}_{1,t}\\,, & \\;\\; \\mbox{ if } f(\\omega_t) \\in d_1, \\\\\n\\;\\;\\; \\vdots & \\;\\;\\;\\;\\; \\vdots \\\\\n\\mathbf{M}_K \\mathbf{Y}_{t-1} + \\boldsymbol{\\eta}_{K,t}\\,, & \\;\\; \\mbox{ if } f(\\omega_t) \\in d_K, \\\\\n\\end{array}\n\\right.\n\\tag{5.20}\\]\nwhere \\(f(\\omega_t)\\) is a function of a time-varying parameter \\(\\omega_t\\) that can itself be a function of the process, \\(\\mathbf{Y}_{t-1}\\), in which case it is a state-dependent model. We implicitly assume that conditions on the right-hand side of Equation 5.20 are mutually exclusive; that is, \\(d_1, \\ldots, d_K\\) are disjoint. A simpler threshold model results if the parameters \\(\\{\\omega_t\\}\\) do not depend on the process. Of course, the transition matrices \\(\\{\\mathbf{M}_1,\\ldots, \\mathbf{M}_K\\}\\) and error covariance matrices \\(\\{\\mathbf{C}_{\\eta_1},\\ldots,\\mathbf{C}_{\\eta_K} \\}\\) depend on unknown parameters, and the big challenge in DSTM modeling is to reduce the dimensionality of this parameter space to facilitate estimation. Some of the approaches discussed above for the linear DSTM process model can also be applied in this setting.\n\n\n\n\n\n\nTip\n\n\n\nThreshold vector autoregressive time-series models can be implemented with the TVAR command in the package tsDyn.\n\n\n\n\nGeneral Quadratic Nonlinearity\nA very large number of real-world processes in the physical and biological sciences exhibit quadratic interactions. For example, consider the following one-dimensional reaction–diffusion PDE:\n\\[\n\\frac{\\partial Y}{\\partial t} = \\frac{\\partial}{\\partial x}\\left(\\delta \\frac{\\partial Y}{\\partial x}\\right) +  Y \\exp\\left(\\gamma_0 \\left(1 - \\frac{Y}{\\gamma_1}\\right)\\right),\n\\tag{5.21}\\]\nwhere the first term corresponds to a diffusion (spread) term that depends on a parameter \\(\\delta\\), and the second term corresponds to a density-dependent (Ricker) growth term with growth parameter \\(\\gamma_0\\) and carrying capacity parameter \\(\\gamma_1\\). More generally, each of these parameters could vary with space and/or time. Notice that the diffusion term is linear in \\(Y\\) but the density-dependent growth term is nonlinear in that it is a function of \\(Y\\) multiplied by a nonlinear transformation of \\(Y\\). This can be considered a general case of a quadratic interaction.\nA fairly general class of nonlinear statistical DSTM process models can be specified to accommodate such behavior. In discrete space and time, such a general quadratic nonlinear (GQN) DSTM can be written, for \\(i=1,\\ldots,n\\), as\n\\[\nY_t(\\mathbf{s}_i) = \\sum_{j=1}^n m_{ij} Y_{t-1}(\\mathbf{s}_j) + \\sum_{k=1}^n \\sum_{\\ell=1}^n b_{i,k \\ell} \\; g(Y_{t-1}(\\mathbf{s}_{\\ell});\\boldsymbol{\\theta}_g) \\; Y_{t-1}(\\mathbf{s}_k)  + \\eta_t(\\mathbf{s}_i),\n\\tag{5.22}\\]\nwhere \\(m_{ij}\\) are the linear-transition coefficients seen previously, and the quadratic-interaction transition coefficients are denoted by \\(b_{i,k \\ell}\\). Importantly, a transformation of one of the components of the quadratic interaction is included through the function \\(g(\\cdot)\\), which can depend on parameters \\(\\boldsymbol{\\theta}_g\\). This function \\(g(\\cdot)\\) is responsible for the term “general” in GQN, and such transformations are important for many processes such as density-dependent growth that one may see in an epidemic or invasive-species population processes (see, for example, Equation 5.21 above), and they can keep forecasts from “blowing up” in time. The spatio-temporal error process \\(\\{\\eta_t(\\cdot)\\}\\) is again typically assumed to be independent in time and Gaussian with mean zero and a spatial covariance matrix. Note that the conditional GQN model for \\(Y_{t}(\\cdot)\\) conditioned on \\(Y_{t-1}(\\cdot)\\) is Gaussian, but the marginal model for \\(Y_t(\\cdot)\\) will not in general be Gaussian because of the nonlinear interactions. The GQN model Equation 5.22 can be shown to be a special case of the state-dependent model in Equation 5.19.\nThere are multiple challenges when implementing models such as Equation 5.22. Chief among these is the curse of dimensionality. There are \\(O(n^3)\\) parameters and, unless one has an enormous number of time replicates (\\(T \\gg n\\)), inference on them is problematic without some sort of regularization (shrinkage) and/or the incorporation of prior information. In addition to parameter estimation, depending on the specification of \\(g(\\cdot)\\) (which can act to control the growth of the process), these models can be explosive when used to forecast multiple time steps into the future. GQN models have been implemented on an application-specific basis in BHMs (see Chapter 7 of Cressie & Wikle, 2011 for more discussion).\n\n\nSome Other Nonlinear Models\nThere are currently several promising approaches for nonlinear spatio-temporal modeling in addition to those mentioned above. For example, there are a wide variety of methods being developed in machine learning to predict and/or classify high volumes of dependent data, including spatio-temporal data (e.g., sequences of images). These methods often relate to variants of neural networks (e.g., convolutional and recurrent neural networks (RNNs)), and they have revolutionized many application areas such as image classification and natural-language processing. In their original formulations, these methods do not typically address uncertainty quantification. However, there is increasing interest in considering such models within broader uncertainty-based paradigms. As mentioned in Chapter 1, there is a connection between deep hierarchical statistical models (BHMs) and many of these so-called “deep learning” algorithms.\nFor example, the GQN model described above is flexible, interpretable, and can accommodate many different types of dynamic processes and uncertainty quantification strategies. Similarly, the typical RNN model is also flexible and can accommodate a wide variety of spatio-temporal dependence structures. However, both the GQN and RNN models can be difficult to implement computationally due to the high dimensionality of the hidden states and parameters, and it typically requires sophisticated regularization (and/or a large amount of data or prior information) to make them work. A computationally efficient alternative is the so-called echo state network (ESN) methodology that was developed as an alternative to RNNs in the engineering literature (Lukoševičius, 2012; for overviews, see Lukoševičius & Jaeger, 2009). Importantly, ESNs consider sparsely connected hidden layers that allow for sequential interactions yet assume most of the parameters (“weights”) are randomly generated and then fixed, with the only parameters estimated being those that connect the hidden layer to the response. This induces a substantially more parsimonious structure in the model. Yet, these models traditionally do not explicitly include quadratic interactions or formal uncertainty quantification. McDermott & Wikle (2017) consider a quadratic spatio-temporal ESN model they call a quadratic ESN (QESN) and implement it in a bootstrap context to account for parameter uncertainty. Details concerning the QESN are given in Appendix F, and the associated case study provides an example of how to use an ensemble of QESNs to generate a long-lead forecast of the SST data.\nAnother type of nonlinear spatio-temporal model that is increasingly being considered in statistical applications is the agent-based model (or, in some literatures, the individual-based model). In this case, the process is built from local individual-scale interactions by means of fairly simple rules that lead to complex nonlinear behavior. Although these models are parsimonious in that they have relatively few parameters, they can be quite computationally expensive, and parameter inference can be challenging (although approximate likelihood methods and BHMs have shown recent promise). For examples, see Cressie & Wikle (2011, Section 7.3.4) and Wikle & Hooten (2016).\nThere is yet another parsimonious approach to nonlinear spatio-temporal modeling that is somewhat science-based and relies on so-called “analogs.” Sometimes referred to as a “mechanism-free” approach, in its most basic form, analog seeks to find historical sequences of maps (analogs) that match a similar sequence culminating at the current time. Then it assumes that the forecast made at the current time will be what actually occurred with the best analog matches. (This is somewhat like the so-called “hot-deck imputation” in statistics.) Analog forecasting can be shown to be a type of spatio-temporal nearest-neighbor-regression methodology. There are many modifications to this procedure related to various conditions as to what “best” means when comparing analogs to the current state, distance metrics, how many analogs to use, and so forth. Traditionally, these methods have not been part of statistical methodology, and so uncertainty quantification and parameter estimation are not generally considered from a formal probabilistic perspective. Recent implementations have sought to consider uncertainty quantification and formal inference, including prediction, within a Bayesian inferential framework McDermott & Wikle (2016); McDermott et al. (2018).",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Dynamic Spatio-Temporal Models</span>"
    ]
  },
  {
    "objectID": "Chapter5.html#chapter-5-wrap-up",
    "href": "Chapter5.html#chapter-5-wrap-up",
    "title": "5  Dynamic Spatio-Temporal Models",
    "section": "5.5 Chapter 5 Wrap-Up",
    "text": "5.5 Chapter 5 Wrap-Up\nRecall that one of the big challenges with the descriptive spatio-temporal models described in Chapter 4 was the specification of realistic covariance structures. We showed that building such structures through conditioning on random effects could be quite useful. The present chapter considered spatio-temporal models from a conditional-in-time (dynamic) perspective that respected the belief that most real-world spatio-temporal processes are best described as spatial processes that evolve through time. Like the random-effects models of Chapter 4, this perspective relied very much on conditional-probability models. First, there was a strong assumption (which is also present in the descriptive models of Chapter 4) that the data, when conditioned on the true spatio-temporal process of interest, could be considered independent in time (and, typically, have fairly simple error structure as well). Second, a Markov assumption in time was made, so that the joint distribution of the process could be decomposed as the product of low-order Markov (in time) conditional-probability distributions. These conditional distributions corresponded to dynamic models that describe the transition of the spatial process from the previous time(s) to the current time. This dynamic model was further conditioned on parameters that control the transition and the innovation-error structure. We showed that the models can often benefit from these parameters being random processes (and/or dependent on covariates) as well.\nWe presented the most commonly used DSTMs with data models that have additive Gaussian error and process models that have linear transition structure with additive Gaussian error. In the simplest case, where time is discrete and interest is in a finite set of spatial locations, we showed that these models are essentially multivariate state-space time series models , and many of the sequential prediction and estimation algorithms from that literature (e.g., filters, smoothers plus estimation through EM, and Bayesian algorithms) can then be used. We also discussed that non-Gaussian data models are fairly easily accommodated if one can obtain conditional independence when conditioning on a latent Gaussian process model (e.g., a data model obtained from a generalized linear model). Additional details on such estimation methods can be found in Shumway & Stoffer (1982), Shumway & Stoffer (2006), Gamerman & Lopes (2006), Prado & West (2010), Cressie & Wikle (2011), and Douc et al. (2014).\nWe emphasized that the biggest challenge with these models is accommodating high dimensionality (either in data volume, number of prediction locations, or number of parameters to be estimated). Thus, one of the fundamental differences between DSTMs and multivariate time series models is that DSTMs require scalable parameterization of the evolution model. We showed that this modeling can be facilitated greatly by understanding some of the fundamental properties of linear dynamical systems and using this mechanistic knowledge to parameterize transition functions/matrices. Additional details on the mechanistic motivation for DSTMs can be found in Cressie & Wikle (2011).\nWe discussed that nonlinear DSTMs are an increasingly important area of spatio-temporal modeling. It is important that statistical models for such processes include realistic structural components (e.g., quadratic interactions) and account formally for uncertainty quantification. We mentioned that a significant challenge with these models in both the statistics and machine learning literature is to mitigate the curse of dimensionality in the parameter space Cressie & Wikle (2011), Goodfellow et al. (2016). This often requires mechanistic-based parameterizations, informative prior distributions s, and/or regularization approaches. This has led to increased interest in very parsimonious representations for nonlinear DSTMs, such as echo state networks, agent-based models , and analog models .\nIn general, DSTMs require many assumptions in order to build conditional models at each level of the hierarchy. They can also be difficult to implement in some cases due to complex dependence and deep levels, often requiring fully Bayesian implementations. This also makes it necessary to validate these assumptions carefully through model diagnostics and evaluation of their predictions. Some approaches to spatio-temporal model evaluation are discussed in Chapter 6.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Dynamic Spatio-Temporal Models</span>"
    ]
  },
  {
    "objectID": "Chapter5.html#lab-5.1-implementing-an-ide-model-in-one-dimensional-space",
    "href": "Chapter5.html#lab-5.1-implementing-an-ide-model-in-one-dimensional-space",
    "title": "5  Dynamic Spatio-Temporal Models",
    "section": "Lab 5.1: Implementing an IDE Model in One-Dimensional Space",
    "text": "Lab 5.1: Implementing an IDE Model in One-Dimensional Space\nIn this Lab we take a look at how one can implement a stochastic integro-difference equation (IDE) in one-dimensional space and time, from first principles. Specifically, we shall consider the dynamic model,\n\\[Y_{t}(s) = \\int_{D_s}m(s,x;\\boldsymbol{\\theta}_p)Y_{t-1}(x) \\textrm{d}x + \\eta_t(s),\\quad s,x \\in D_s,\\]\nwhere \\(Y_t(\\cdot)\\) is the spatio-temporal process at time \\(t\\); \\(\\boldsymbol{\\theta}_p\\) are parameters that we fix (in practice, these will be estimated from data; see Lab 5.2); and \\(\\eta_t(\\cdot)\\) is a spatial process, independent of \\(Y_t(\\cdot)\\), with covariance function that we shall assume is known.\nWe only need the packages dplyr, ggplot2, and STRbook for this lab and, for reproducibility purposes, we fix the seed.\n\nlibrary(\"dplyr\")\nlibrary(\"ggplot2\")\nlibrary(\"STRbook\")\nset.seed(1)\n\n\nConstructing the Process Grid and Kernel\nWe start off by constructing a discretization of the one-dimensional spatial domain \\(D_s = [0,1]\\). We shall use this discretization, containing cells of width \\(\\Delta_s\\), for both approximate integrations as well as visualizations. We call this our spatial grid.\n\nds &lt;- 0.01\ns_grid &lt;- seq(0, 1, by = ds)\nN &lt;- length(s_grid)\n\nOur space-time grid is formed by calling expand.grid with s_grid and our temporal domain, which we define as the set of integers spanning 0 up to \\(T = 200\\).\n\nnT &lt;- 201\nt_grid &lt;- 0:(nT-1)\nst_grid &lt;- expand.grid(s = s_grid, t = t_grid)\n\nThe transition kernel \\(m(s,x; \\boldsymbol{\\theta}_p)\\) is a bivariate function on our spatial grid. It is defined below to be a Gaussian kernel, where the entries of \\(\\boldsymbol{\\theta}_p = (\\theta_{p,1},\\theta_{p,2},\\theta_{p,3})'\\) are the amplitude, the scale (aperture, twice the variance), and the shift (offset) of the kernel, respectively. Specifically,\n\\[\nm(s,x; \\boldsymbol{\\theta}_p) \\equiv \\theta_{p,1}\\exp\\left(-\\frac{1}{\\theta_{p,2}}(x - \\theta_{p,3} - s)^2\\right),\n\\]\nwhich can be implemented as an R function as follows.\n\nm &lt;- function(s, x, thetap) {\n  gamma &lt;- thetap[1]                 # amplitude\n  l &lt;- thetap[2]                     # length scale\n  offset &lt;- thetap[3]                # offset\n  D &lt;- outer(s + offset, x, '-')     # displacements\n  gamma * exp(-D^2/l)                # kernel eval.\n}\n\nNote the use of the function outer with the subtraction operator. This function performs an “outer operation” (a generalization of the outer product) by computing an operation between every two elements of the first two arguments, in this case a subtraction.\nWe can now visualize some kernels by seeing how the process at \\(s = 0.5\\) depends on \\(x\\). Four such kernels are constructed below: the first is narrow and centered on 0.5; the second is slightly wider; the third is shifted to the right; and the fourth is shifted to the left. We store the parameters of the four different kernels in a list thetap.\n\nthetap &lt;- list()\nthetap[[1]] &lt;- c(40, 0.0002, 0)\nthetap[[2]] &lt;- c(5.75, 0.01, 0)\nthetap[[3]] &lt;- c(8, 0.005, 0.1)\nthetap[[4]] &lt;- c(8, 0.005, -0.1)\n\nPlotting proceeds by first evaluating the kernel for all \\(x\\) at \\(s = 0.5\\), and then plotting these evaluations against \\(x\\). The first kernel is plotted below; plotting the other three is left as an exercise for the reader. The kernels are shown in the top panels of Figure 5.2.\n\nm_x_0.5 &lt;- m(s = 0.5, x = s_grid,            # construct kernel\n             thetap = thetap[[1]]) %&gt;%       # at s = 0.5\n           as.numeric()                      # convert to numeric\ndf &lt;- data.frame(x = s_grid, m = m_x_0.5)      # allocate to df\nggplot(df) + geom_line(aes(x, m)) + theme_bw() # plot\n\nThe last term we need to define is \\(\\eta_t(\\cdot)\\). Here, we define it as a spatial process with an exponential covariance function with range parameter 0.1 and variance 0.1. The covariance matrix at each time point is then\n\nSigma_eta &lt;- 0.1 * exp( -abs(outer(s_grid, s_grid, '-') / 0.1))\n\nSimulating \\(\\eta_t(s)\\) over s_grid proceeds by generating a multivariate Gaussian vector with mean zero and covariance matrix Sigma_eta. To do this, one can use the function mvrnorm from the package MASS. Alternatively, one may use the lower Cholesky factor of Sigma_eta and multiply this by a vector of numbers generated from a mean-zero, variance-one, independent-elements Gaussian random vector (see Rue & Held, 2005, Algorithm 2.3).\n\nL &lt;- t(chol(Sigma_eta))  # chol() returns upper Cholesky factor\nsim &lt;- L %*% rnorm(nrow(Sigma_eta))  # simulate\n\nType plot(s_grid, sim, 'l') to plot this realization of \\(\\eta_t(s)\\) over s_grid.\n\n\nSimulating from the IDE\nNow we have everything in place to simulate from the IDE. Simulation is most easily carried out using a for loop as shown below. We shall carry out four simulations, one for each kernel constructed above, and store the simulations in a list of four data frames, one for each simulation. The following command initializes this list.\n\nY &lt;- list()\n\nFor each simulation setting (which we iterate using the index i), we simulate the time points (which we iterate using j) to obtain the process. The “nested for loop” below accomplishes this. In the outer loop, the kernel is constructed and the process is initialized to zero. In the inner loop, the integration is approximated using a Riemann sum,\n\\[\n\\int_{D_s}m(s,x;\\boldsymbol{\\theta}_p)Y_{t-1}(x) \\textrm{d}x \\approx \\sum_i m(s,x_i;\\boldsymbol{\\theta}_p)Y_{t-1}(x_i)\\Delta_s ,\n\\]\nwhere we recall that we have set \\(\\Delta_s = 0.01\\). Next, at every time point \\(\\eta_t(s)\\) is simulated on the grid and added to the sum (an approximation of the integral) above.\n\nfor(i in 1:4) {                         # for each kernel\n  M &lt;- m(s_grid, s_grid, thetap[[i]])   # construct kernel\n  Y[[i]] &lt;- data.frame(s = s_grid,      # init. data frame with s\n                       t = 0,           # init. time point 0, and\n                       Y = 0)           # init. proc. value = 0\n  for(j in t_grid[-1]) {                # for each time point\n    prev_Y &lt;- filter(Y[[i]],            # get Y at t - 1\n                     t == j - 1)$Y\n    eta &lt;- L %*% rnorm(N)               # simulate eta\n    new_Y &lt;- (M %*% prev_Y * ds + eta) %&gt;%\n             as.numeric()               # Euler approximation\n\n    Y[[i]] &lt;- rbind(Y[[i]],             # update data frame\n                    data.frame(s = s_grid,\n                               t = j,\n                               Y =  new_Y))\n  }\n}\n\nRepeatedly appending data frames, as is done above, is computationally inefficient. For large systems it would be quicker to save a data frame for each time point in another list and then concatenate using rbindlist from the package data.table.\nSince now Y[[i]], for i\\(\\,=1,\\ldots,4\\), contains a data frame in long format, it is straightforward to visualize. The code given below constructs the Hovmöller plot for the IDE process for i\\(\\,=1\\). Plotting for i\\(\\,=2,3,4\\) is left as an exercise for the reader. The resulting plots are shown in the bottom panels of Figure 5.2.\n\nggplot(Y[[1]]) + geom_tile(aes(s, t, fill = Y)) +\n    scale_y_reverse() + theme_bw() +\n    fill_scale(name = \"Y\")\n\n\n\nSimulating Observations\nNow assume that we want to simulate noisy observations from one of the process models that we have just simulated from. Why would we want to do this? Frequently, the only way to test whether algorithms for inference are working as they should is to mimic both the underlying true process and the measurement process. Working with simulated data is the first step in developing reliable algorithms that are then ready to be applied to real data.\nTo map the observations to the data, we need an incidence matrix that picks out the process value that has been observed. This incidence matrix is simply composed of several rows, one for each observation, with zeros everywhere except for the entry corresponding to the process value that has been observed (recall Section 5.2.1). When the locations we are observing change over time, the incidence matrix correspondingly changes over time.\nSuppose that at each time point we observe the process at 50 locations which, for convenience, are a subset of s_grid. (If this is not the case, some nearest-neighbor mapping or deterministic interpolation method can be used.)\n\nnobs &lt;- 50\nsobs &lt;- sample(s_grid, nobs)\n\nThen the incidence matrix at time \\(t\\), \\(\\mathbf{H}_t\\), can be constructed by matching the observation locations on the space-time grid using the function which.\n\nHt &lt;- matrix(0, nobs, N)           # construct empty matrix\nfor(i in 1:nobs) {                 # for each obs\n  idx &lt;- which(sobs[i] == s_grid)  # find the element to set to 1\n  Ht[i, idx] &lt;- 1                  # set to 1\n}\n\nNote that Ht is sparse (contains many zeros), so sparse-matrix representations can be used to improve computational and memory efficiency; look up the packages Matrix or spam for more information on these representations.\nWe can repeat this procedure for every time point to simulate our data. At time \\(t\\), the data are given by \\(\\mathbf{Z}_t = \\mathbf{H}_t\\mathbf{Y}_t + \\boldsymbol{\\varepsilon}_t\\), where \\(\\mathbf{Y}_t\\) is the latent process on the grid at time \\(t\\), and \\(\\boldsymbol{\\varepsilon}_t\\) is independent of \\(\\mathbf{Y}_t\\) and represents a Gaussian random vector whose entries are \\(iid\\) with mean zero and variance \\(\\sigma^2_\\epsilon\\). Assume \\(\\sigma^2_\\epsilon = 1\\) and that \\(\\mathbf{H}_t\\) is the same for each \\(t\\). Then observations are simulated using the following for loop.\n\nz_df &lt;- data.frame()               # init data frame\nfor(j in 0:(nT-1)) {               # for each time point\n  Yt &lt;- filter(Y[[1]], t == j)$Y   # get the simulated process\n  zt &lt;- Ht %*% Yt + rnorm(nobs)    # map to obs and add noise\n  z_df &lt;- rbind(z_df,              # update data frame\n                data.frame(s = sobs, t = j, z = zt))\n}\n\nPlotting of the simulated observations proceeds using ggplot2 as follows.\n\nggplot(z_df) + geom_point(aes(s, t, colour = z))  +\n  col_scale(name = \"z\") + scale_y_reverse() + theme_bw()\n\nNote that the observations are noisy and reveal sizeable gaps. Filling in these gaps by first estimating all the parameters in the IDE from the data and then predicting at unobserved locations is the subject of Lab 5.2.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Dynamic Spatio-Temporal Models</span>"
    ]
  },
  {
    "objectID": "Chapter5.html#lab-5.2-spatio-temporal-inference-using-the-ide-model",
    "href": "Chapter5.html#lab-5.2-spatio-temporal-inference-using-the-ide-model",
    "title": "5  Dynamic Spatio-Temporal Models",
    "section": "Lab 5.2: Spatio-Temporal Inference using the IDE Model",
    "text": "Lab 5.2: Spatio-Temporal Inference using the IDE Model\nIn this Lab we use the package IDE to fit spatio-temporal IDE models as well as predict and forecast from spatio-temporal data. We explore three cases. The first two cases consider simulated data where the true model is known, and the third considers the Sydney radar data set described in Chapter 2.\nFor this Lab, we need the package IDE and also the package FRK, which will be used to construct basis functions to model the spatially varying parameters of the kernel. In addition, we shall use the package plyr for binding data frames with unequal column number later on in the Lab.\n\nlibrary(\"plyr\")\nlibrary(\"dplyr\")\nlibrary(\"IDE\")\nlibrary(\"FRK\")\nlibrary(\"ggplot2\")\nlibrary(\"sp\")\nlibrary(\"spacetime\")\nlibrary(\"STRbook\")\n\nThe kernel \\(m(\\mathbf{s},\\mathbf{x};\\boldsymbol{\\theta}_p)\\) used by the package IDE is given by\n\\[\nm(\\mathbf{s},\\mathbf{x};\\boldsymbol{\\theta}_p) = {\\theta_{p,1}(\\mathbf{s})} \\exp\\left(-\\frac{1}{\\theta_{p,2}(\\mathbf{s})}\\left[(x_1 - \\theta_{p,3}(\\mathbf{s}) - s_1)^2 + (x_2 - \\theta_{p,4}(\\mathbf{s}) - s_2)^2 \\right] \\right),\n\\tag{5.23}\\]\nwhere \\(\\theta_{p,1}(\\mathbf{s})\\) is the spatially varying amplitude, \\(\\theta_{p,2}(\\mathbf{s})\\) is the spatially varying kernel aperture (or width), and the mean (shift) parameters \\(\\theta_{p,3}(\\mathbf{s})\\) and \\(\\theta_{p,4}(\\mathbf{s})\\) correspond to a spatially varying shift of the kernel relative to location \\(\\mathbf{s}\\). Spatially invariant kernels (i.e., where the elements of \\(\\boldsymbol{\\theta}_p\\) are not functions of space) are also allowed.\nThe package IDE uses a bisquare spatial-basis-function decomposition for both the process \\(Y_t(\\cdot)\\) and the spatial process \\(\\eta_t(\\cdot)\\), \\(t = 1,2,\\dots.\\) The covariance matrix of the basis-function coefficients associated with \\(\\eta_t(\\cdot)\\) is assumed to be proportional to the identity matrix, where the constant of proportionality is estimated. In IDE, the latent process \\(\\widetilde{Y}_t(\\mathbf{s})\\) is the IDE dynamical process superimposed on some fixed effects, which can be expressed as a linear combination of known covariates \\(\\mathbf{x}_t(\\mathbf{s})\\),\n\\[\n\\widetilde{Y_t}(\\mathbf{s}) = \\mathbf{x}_t(\\mathbf{s})'\\boldsymbol{\\beta}+ Y_t(\\mathbf{s});\\quad \\mathbf{s}\\in D_s,\n\\tag{5.24}\\]\nfor \\(t = 1,2,\\dots\\), where \\(\\boldsymbol{\\beta}\\) are regression coefficients. The data vector \\(\\mathbf{Z}_t \\equiv (Z_t(\\mathbf{r}_{1}),\\dots,Z_t(\\mathbf{r}_{m_t}))'\\) is then the latent process observed with noise,\n\\[\nZ_t(\\mathbf{r}_{j}) = \\widetilde{Y}_t(\\mathbf{r}_{j}) + \\epsilon_t(\\mathbf{r}_{j}),\\quad j = 1,\\dots,m_t,\n\\]\nfor \\(t = 1,2,\\dots\\), where \\(\\epsilon_t(\\mathbf{r}_{j}) \\sim iid\\,Gau(0, \\sigma^2_\\epsilon)\\).\n\nSimulation Example with a Spatially Invariant Kernel\nThe package IDE contains a function simIDE that simulates the behavior of a typical dynamic system governed by linear transport. The function can simulate from a user-defined IDE model, or from a pre-defined one. In the latter case, the number of time points to simulate (T), the number of (spatially fixed) observations to use (nobs), and a flag indicating whether to use a spatially invariant kernel (k_spat_invariant = 1) or not (k_spat_invariant = 0), need to be provided. The pre-defined model includes a linear trend in \\(s_1\\) and \\(s_2\\).\n\nSIM1 &lt;- simIDE(T = 10, nobs = 100, k_spat_invariant = 1)\n\nThe returned list SIM1 contains the simulated process in the data frame s_df, the observed data in the data frame z_df, and the observed data as an STIDF object z_STIDF. It also contains two ggplot2 plots, g_truth and g_obs, which can be readily plotted as follows.\n\nprint(SIM1$g_truth)\nprint(SIM1$g_obs)\n\nWhile the action of the transport is clearly noticeable in the evolution of the process, there is also a clear spatial trend. Covariates are included through the use of a standard R formula when calling the function IDE. Additional arguments to IDE include the data set, which needs to be of class STIDF, the temporal discretization to use (we will use 1 day) of class difftime, and the resolution of the grid upon which the integrations (as well as predictions) will be carried out. Other arguments include user-specified basis functions for the process and what transition kernel will be used, which for now we do not specify. By default, the IDE model will decompose the process using two resolutions of bisquare basis functions and will assume a spatially invariant Gaussian transition kernel.\n\nIDEmodel &lt;- IDE(f = z ~ s1 + s2,\n                data = SIM1$z_STIDF,\n                dt = as.difftime(1, units = \"days\"),\n                grid_size = 41)\n\nThe returned object IDEmodel is of class IDE and contains initial parameter estimates, as well as predictions of \\(\\boldsymbol{\\alpha}_t,\\) for \\(t = 1,\\dots,T\\), at these initial parameter estimates. The parameters in this case are the measurement-error variance, the variance of the random disturbance \\(\\eta_t(\\cdot)\\) (whose covariance structure is fixed), the kernel parameters, and the regression coefficients \\(\\boldsymbol{\\beta}\\).\nEstimating the parameters in the IDE model using maximum likelihood is a computationally intensive procedure. The default method currently implemented uses a differential evolution optimization algorithm from the package DEoptim, which is a global optimization algorithm that can be easily parallelized. Fitting takes only a few minutes on a 60-core high-performance computer, but can take an hour or two on a standard desktop computer. Fitting can be done by running the following code.\n\nfit_results_sim1 &lt;- fit.IDE(IDEmodel,\n                           parallelType = 1)\n\nHere parallelType = 1 ensures that all available cores on the computer are used for fitting. Alternatively, the results can be loaded from cache using the following command.\n\ndata(\"IDE_Sim1_results\", package = \"STRbook\")\n\nThe list fit_results_sim1 contains two fields: optim_results that contains the output of the optimization algorithm, and IDEmodel that contains the fitted IDE model. The fitted kernel can be visualized by using the function show_kernel.\n\nshow_kernel(fit_results_sim1$IDEmodel)\n\nNote how the fitted kernel is shifted to the left and upwards, correctly representing the southeasterly transport evident in the data. The estimated kernel parameters \\(\\boldsymbol{\\theta}_p\\) are given below.\n\nfit_results_sim1$IDEmodel$get(\"k\") %&gt;% unlist()\n\n         par1          par2          par3          par4 \n152.836345912   0.001977115  -0.101601099   0.100368743 \n\n\nThese estimates compare well to the true values c(150, 0.002, -0.1, 0.1) (see the help file for simIDE). The estimated regression coefficients are given below.\n\ncoef(fit_results_sim1$IDEmodel)\n\nIntercept        s1        s2 \n0.2073442 0.1966224 0.1907062 \n\n\nThese also compare well to the true values c(0.2, 0.2, 0.2). Also of interest are the moduli of the possibly complex eigenvalues of the evolution matrix \\(\\mathbf{M}\\). These can be extracted as follows.\n\nabs_ev &lt;- eigen(fit_results_sim1$IDEmodel$get(\"M\"))$values %&gt;%\n          abs()\nsummary(abs_ev)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n0.00285 0.28953 0.36604 0.33107 0.39853 0.46409 \n\n\nSince the largest of these is less than 1, the IDE exhibits stable behavior.\nFor prediction, one may either specify a prediction grid or use the default one used for approximating the integrations set up by IDE. The latter is usually sufficient, so we use this without exception for the examples we consider. When a prediction grid is not supplied, the function predict returns a data frame with predictions spanning the temporal extent of the data (forecasts and hindcasts are explored later).\n\nST_grid_df &lt;- predict(fit_results_sim1$IDEmodel)\n\nThe prediction map and prediction-standard-error map can now be plotted using standard ggplot2 commands as follows.\n\ngpred &lt;- ggplot(ST_grid_df) +       # Plot the predictions\n  geom_tile(aes(s1, s2, fill=Ypred)) +\n  facet_wrap(~t) +\n  fill_scale(name = \"Ypred\", limits = c(-0.1, 1.4)) +\n  coord_fixed(xlim=c(0, 1), ylim = c(0, 1))\n\ngpredse &lt;- ggplot(ST_grid_df) +     # Plot the prediction s.es\n  geom_tile(aes(s1, s2, fill = Ypredse)) +\n  facet_wrap(~t) +\n  fill_scale(name = \"Ypredse\") +\n  coord_fixed(xlim=c(0, 1), ylim = c(0, 1))\n\nIn Figure 5.4, we show the observations, the true process, the predictions, and the prediction standard errors from the fitted model. Notice that the prediction standard errors are large in regions of sparse observations, as expected.\n\n\n\n\n\n\nFigure 5.4: Simulated process (top left), simulated data (top right), predictions following the fitting of the IDE model (bottom left) and the respective prediction standard errors (bottom right).\n\n\n\n\n\nSimulation Example with a Spatially Varying Kernel\nIn the previous example we considered the case of a spatially invariant kernel, that is, the case when the kernel \\(m(\\mathbf{s},\\mathbf{x};\\boldsymbol{\\theta}_p)\\) is just a function of \\(\\mathbf{x}- \\mathbf{s}\\). In this example, we consider the case when one or more of the \\(\\boldsymbol{\\theta}_p\\) are allowed to be spatially referenced. Such models are needed when the spatio-temporal process exhibits, for example, considerable spatially varying drift (i.e., advection). Such a process can be simulated using the function simIDE by specifying k_spat_invariant = 0. To model data from a process of this sort, we need to have a large nobs and many time points; we set T = 15. This is important, as it is difficult to obtain reasonable estimates of spatially distributed parameters unless the data cover a large part of the spatial domain for a sustained amount of time.\n\nSIM2 &lt;- simIDE(T = 15, nobs = 1000, k_spat_invariant = 0)\n\nAs above, the process and the observed data can be plotted as two ggplot2 plots.\n\nprint(SIM2$g_truth)\nprint(SIM2$g_obs)\n\nNote how the process appears to rotate quickly counter-clockwise and come to a nearly complete standstill towards the lower part of the domain. The spatially varying advection that generated this field can be visualized using the following command.\n\nshow_kernel(SIM2$IDEmodel, scale = 0.2)\n\nIn this command, the argument scale is used to scale the arrow sizes by 0.2; that is, the shift per time point is five times the displacement indicated by the arrow.\nSpatially varying kernels can be introduced by specifying the argument kernel_basis inside the call to IDE. The basis functions that IDE uses are of the same class as those used by FRK. We construct nine bisquare basis functions below that are equally spaced in the domain.\n\nmbasis_1 &lt;- auto_basis(manifold = plane(),   # fns on the plane\n                       data = SIM2$z_STIDF,  # data\n                       nres = 1,             # 1 resolution\n                       type = 'bisquare')    # type of functions\n\nTo plot these basis functions, type show_basis(mbasis_1).\nNow, recall that \\(\\theta_{p,1}\\) (identified as thetam1 in IDE) corresponds to the amplitude of the kernel, \\(\\theta_{p,2}\\) (thetam2) to the scale (width) or aperture, \\(\\theta_{p,3}\\) (thetam3) to the horizontal drift, and \\(\\theta_{p,4}\\) (thetam4) to the vertical drift. In what follows, suppose that \\(\\theta_{p,1}\\) and \\(\\theta_{p,2}\\) are spatially invariant (usually a reasonable assumption), and decompose \\(\\theta_{p,3}\\) and \\(\\theta_{p,4}\\) as sums of basis functions given in mbasis_1.\n\nkernel_basis &lt;- list(thetam1 = constant_basis(),\n                     thetam2 = constant_basis(),\n                     thetam3 = mbasis_1,\n                     thetam4 = mbasis_1)\n\nModeling proceeds as before, except that now we specify the argument kernel_basis when calling IDE.\n\nIDEmodel &lt;- IDE(f = z ~ s1 + s2 + 1,\n                data = SIM2$z_STIDF,\n                dt = as.difftime(1, units = \"days\"),\n                grid_size = 41,\n                kernel_basis = kernel_basis)\n\nFitting also proceeds by calling the function fit.IDE. We use the argument itermax = 400 below to specify the maximum number of iterations for the optimization routine to use.\n\nfit_results_sim2 &lt;- fit.IDE(IDEmodel,\n                           parallelType = 1,\n                           itermax = 400)\n\nAs above, since this is computationally intensive, we provide cached results that can be loaded using the following command.\n\ndata(\"IDE_Sim2_results\", package = \"STRbook\")\n\nThe fitted spatially varying kernel can be visualized using the following command.\n\nshow_kernel(fit_results_sim2$IDEmodel)\n\nThe true and fitted spatially varying drift parameters are shown side by side in Figure 5.5. Note how the fitted drifts capture the broad directions and magnitudes of the true underlying process. Predictions and prediction standard errors can be obtained and mapped using predict as above. This is left as an exercise for the reader.\n\n\n\n\n\n\nFigure 5.5: True drifts (left) and estimated drifts (right).\n\n\n\n\n\nThe Sydney Radar Data Set\nAnalysis of the Sydney radar data set proceeds in much the same way as in the simulation examples. In this case, we choose to have a spatially invariant kernel, since the data are not suggestive of spatially varying dynamics. We first load the Sydney radar data set as an STIDF object.\n\ndata(\"radar_STIDF\", package = \"STRbook\")\n\nAs was seen in Chapter 2, the Sydney radar data set exhibits clear movement (drift), making the IDE a good modeling choice for these data. We now call the function IDE as before, with the added arguments hindcast and forecast, which indicate how many time intervals into the past, and how many into the future, we wish to predict for periods preceding the training period (hindcast) and periods following the training period (forecast), respectively (see Section 6.1.3 for more information on hindcasts and forecasts). In this case the data are at 10-minute intervals (one period), and we forecast and hindcast for two periods each (i.e., 20 minutes).\n\nIDEmodel &lt;- IDE(f = z ~ 1,\n                data = radar_STIDF,\n                dt = as.difftime(10, units = \"mins\"),\n                grid_size = 41,\n                forecast = 2,\n                hindcast = 2)\n\nFitting proceeds by calling fit.IDE.\n\nfit_results_radar &lt;- fit.IDE(IDEmodel,\n                             parallelType = 1)\n\nSince this command will take a considerable amount of time on a standard machine, we load the results directly from cache.\n\ndata(\"IDE_Radar_results\", package = \"STRbook\")\n\nThe fitted kernel can be visualized as it was above.\n\nshow_kernel(fit_results_radar$IDEmodel)\n\nThe kernel is again clearly shifted off-center and suggestive of transport in a predominantly easterly (and slightly northerly) direction. This is corroborated by visual inspection of the data. The estimated shift parameters are as follows.\n\nshift_pars &lt;- (fit_results_radar$IDEmodel$get(\"k\") %&gt;%\n                   unlist())[3:4]\nprint(shift_pars)\n\npar3 par4 \n-5.5 -1.9 \n\n\nThe magnitude of the estimated shift vector is hence indicative of a transport of \\(\\sqrt{(5.5)^2 + (1.9)^2} = 5.82\\) km per 10-minute period, or 34.91 km per hour.\nThe modulus of the possibly complex eigenvalues of the evolution matrix \\(\\mathbf{M}\\) can be extracted as follows.\n\nabs_ev &lt;- eigen(fit_results_radar$IDEmodel$get(\"M\"))$values %&gt;%\n          abs()\nsummary(abs_ev)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n   0.01    0.58    0.67    0.62    0.72    0.79 \n\n\nThe largest absolute eigenvalue is considerably larger than that in the simulation study, suggesting more field persistence (although, since it is less than 1, the process is still stable). This persistence is expected, since the data clearly show patches of precipitation that are sustained and transported, rather than decaying, over time.\nWhen calling the function IDE, we set up the object to be able to forecast 20 minutes into the future and hindcast 20 minutes into the past. These forecasts and hindcasts will be in the object returned from predict.\n\nST_grid_df &lt;- predict(fit_results_radar$IDEmodel)\n\nThe data frame ST_grid_df contains the predictions in the field Ypred and the prediction standard errors in the field Ypredse. The field t, in both our data and predictions, contains the date as well as the time; we now create another field time that contains just the time of day.\n\nradar_df$time &lt;- format(radar_df$t, \"%H:%M\")\nST_grid_df$time &lt;- format(ST_grid_df$t, \"%H:%M\")\n\nThe code given below plots the data as well as the smoothed fields containing the hindcasts, the predictions, and the forecasts. So that we match the data plots with the prediction plots, timewise, we create empty fields corresponding to hindcast and forecast periods in the data frame containing the observations. This can be achieved easily using rbind.fill from the package plyr.\n\n\n\n\n\n\nFigure 5.6: Observed data (left), and hindcasts, predictions, and forecasts using the IDE model (right).\n\n\n\n\n## Add time records with missing data\nradar_df &lt;- rbind.fill(radar_df,\n                       data.frame(time = c(\"08:05\", \"08:15\",\n                                           \"10:25\", \"10:35\")))\n\n\n## Plot of data, with color scale capped to (-20, 60)\ngobs &lt;- ggplot(radar_df) +\n  geom_tile(aes(s1, s2, fill = pmin(pmax(z, -20), 60))) +\n  fill_scale(limits = c(-20, 60), name = \"Z\") +\n  facet_wrap(~time) + coord_fixed() + theme_bw()\n\n\n## Plot of predictions with color scale forced to (-20, 60)\ngpred &lt;- ggplot(ST_grid_df) +\n  geom_tile(aes(s1, s2, fill = Ypred)) +\n  facet_wrap(~time) + coord_fixed() + theme_bw() +\n  fill_scale(limits = c(-20, 60), name = \"Ypred\")\n\nThe plots are shown in Figure 5.6. Notice how both the forecasts and the hindcasts incorporate the information on transport that is evident in the data. We did not plot prediction standard errors in this case, which is left as an exercise for the reader.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Dynamic Spatio-Temporal Models</span>"
    ]
  },
  {
    "objectID": "Chapter5.html#lab-5.3-spatio-temporal-inference-with-unknown-evolution-operator",
    "href": "Chapter5.html#lab-5.3-spatio-temporal-inference-with-unknown-evolution-operator",
    "title": "5  Dynamic Spatio-Temporal Models",
    "section": "Lab 5.3: Spatio-Temporal Inference with Unknown Evolution Operator",
    "text": "Lab 5.3: Spatio-Temporal Inference with Unknown Evolution Operator\nIf we have no prior knowledge to guide us on how to parameterize \\(\\mathbf{M}\\), then \\(\\mathbf{M}\\) can be estimated in full in the context of a standard state-space modeling framework. When taking this approach, it is important that a very low-dimensional representation of the spatio-temporal process is adopted – the dimension of the parameter space increases quadratically with the dimension of the process, and thus the model can easily become over-parameterized.\nEmpirical orthogonal functions (EOFs) are ideal basis functions to use in this case, since they capture most of the variability in the observed signal, by design. In this Lab we look at the SST data set, take the EOFs that we generated in Lab 2.3, and estimate all unknown parameters, first within a classical time-series framework based on a vector autoregression and using the method of moments (see Section C.1), and then in a state-space framework using the EM algorithm (see Section C.2).\n\nTime-Series Framework\nThe aim of this first part of the Lab is to show how even simple methods can be used in a dynamical setting to provide prediction and prediction standard errors on a variable of interest. These methods work particularly well when we have complete spatial coverage and a high signal-to-noise ratio; this is the case with the SST data.\nFirst, we load the usual packages.\n\nlibrary(\"ggplot2\")\nlibrary(\"dplyr\")\nlibrary(\"STRbook\")\n\nThen we load expm for raising matrices to a specified power and Matrix, which here we only use for plotting purposes.\n\nlibrary(\"expm\")\nlibrary(\"Matrix\")\n\nWe now load the SST data, but this time we truncate it at April 1997 in order to forecast the SSTs 6 months ahead, in October 1997.\n\ndata(\"SSTlandmask\", package = \"STRbook\")\ndata(\"SSTlonlat\", package = \"STRbook\")\ndata(\"SSTdata\", package = \"STRbook\")\ndelete_rows &lt;- which(SSTlandmask == 1)   # remove land values\nSST_Oct97 &lt;- SSTdata[-delete_rows, 334]  # save Oct 1997 SSTs\nSSTdata &lt;- SSTdata[-delete_rows, 1:328]  # until April 1997\nSSTlonlat$mask &lt;- SSTlandmask            # assign mask to df\n\nNext, we construct the EOFs using only data up to April 1997. The following code follows closely what was done in Lab 2.3, where the entire data set was used.\n\nZ &lt;- t(SSTdata)                         # data matrix\nspat_mean &lt;- apply(SSTdata, 1, mean)    # spatial mean\nnT &lt;- ncol(SSTdata)                     # no. of time points\nZspat_detrend &lt;- Z - outer(rep(1, nT),  # detrend data\n                           spat_mean)\nZt &lt;- 1/sqrt(nT-1)*Zspat_detrend        # normalize\nE &lt;- svd(Zt)                            # SVD\n\nThe number of EOFs we use here to model the SST data is \\(n = 10\\). These 10 leading EOFs capture 74% of the variability in the data.\n\nn &lt;- 10\n\nRecall that the object E contains the SVD, that is, the matrices \\(\\mathbf{U}\\) and \\(\\mathbf{V}\\) and the singular values. The dimension-reduced time series of coefficients are given by the EOFs multiplied by the spatially detrended data, that is, \\(\\boldsymbol{\\alpha}_t = \\boldsymbol{\\Phi}'(\\mathbf{Z}_t - \\hat{\\boldsymbol{\\mu}})\\), \\(t = 1,\\dots,T\\), where \\(\\hat{\\boldsymbol{\\mu}} = (1/T)\\sum_{t=1}^T\\mathbf{Z}_t\\) is the estimated spatial mean.\n\nTS &lt;- Zspat_detrend %*% E$v[, 1:n]\nsummary(colMeans(TS))\n\n     Min.   1st Qu.    Median      Mean   3rd Qu.      Max. \n-4.33e-16 -5.80e-17  3.17e-16  2.99e-16  5.01e-16  1.53e-15 \n\n\nIn the last line above, we have verified that the time series have mean zero, which is needed to compute covariances by taking outer products. Next, we estimate the matrices \\(\\mathbf{M}\\) and \\(\\mathbf{C}_\\eta\\) using the method of moments. First we create two sets of time series that are shifted by \\(\\tau\\) time points with respect to each other; in this case we let \\(\\tau = 6,\\) so that we analyze dynamics on a six-month scale. The \\(i\\)th column in TStplustau below corresponds to the time series at the \\((i+6)\\)th time point, while that in TSt corresponds to the time series at \\(i\\)th time point.\n\ntau &lt;- 6\nnT &lt;- nrow(TS)\nTStplustau &lt;- TS[-(1:tau), ] # TS with first tau time pts removed\nTSt &lt;- TS[-((nT-5):nT), ]    # TS with last tau time pts removed\n\nThe lag-0 empirical covariance matrix and the lag-\\(\\tau\\) empirical cross-covariance matrices are now computed by taking their matrix cross-product and dividing by the appropriate number of time points; see Equation 2.4.\n\nCov0 &lt;- crossprod(TS)/nT\nCovtau &lt;- crossprod(TStplustau,TSt)/(nT - tau)\n\nThe estimates for \\(\\mathbf{M}\\) and \\(\\mathbf{C}_\\eta\\) can now be estimated from these empirical covariance matrices. As discussed in Section C.1, this can be done using the following code.\n\nC0inv &lt;- solve(Cov0)\nMest &lt;- Covtau %*% C0inv\nCeta &lt;- Cov0 - Covtau %*% C0inv %*% t(Covtau)\n\nThere are more efficient ways to compute the quantities above that ensure symmetry and positive-definiteness of the results. In particular, the inverse rarely needs to be found explicitly. For further information, the interested reader is referred to standard books on linear algebra (see, for example, Schott, 2017).\nThe matrices can be visualized using the function image.\n\nimage(Mest)\nimage(Ceta)\n\nFrom visual inspection, the estimate of the propagator matrix, Mest, is by no means diagonally dominant, implying that there is benefit in assuming interactions between the EOFs across time steps (see Section 5.3.2). Further, the estimated variances along the diagonal of the covariance matrix of the additive disturbance in the IDE model, \\(\\mathbf{C}_\\eta\\), decrease with the EOF index; this is expected as EOFs with higher indices tend to have higher-frequency components.\nForecasting using this EOF-reduced model is straightforward as we take the coefficients at the final time point, \\(\\boldsymbol{\\alpha}_t\\), propagate those forward, and re-project onto the original space. For example, \\(\\hat{\\boldsymbol{\\mu}} + \\boldsymbol{\\Phi}\\mathbf{M}^2\\boldsymbol{\\alpha}_t\\) gives a one-year forecast. Matrix powers (which represent multiple matrix multiplications and do not come from elementwise multiplications) can be implemented using the operator %^% from the package expm; this will be used when we implement the state-space model below. Here we consider six-month-ahead forecasts; in the code below we project ahead the EOF coefficients of the time series at the 328th time point (which corresponds to April 1997) six months into the future.\n\nSSTlonlat$pred &lt;- NA\nalpha_forecast &lt;- Mest %*% TS[328, ]\n\nThe projection onto the original space is done by pre-multiplying by the EOFs and adding back the estimated spatial mean (see Section C.1).\n\nidx &lt;- which(SSTlonlat$mask == 0)\nSSTlonlat$curr[idx]  &lt;- as.numeric(E$v[, 1:n] %*% TS[328, ] +\n                                       spat_mean)\nSSTlonlat$pred[idx]  &lt;- as.numeric(E$v[, 1:n] %*% alpha_forecast +\n                                       spat_mean)\n\nNow we add the data to the data frame for plotting purposes.\n\nSSTlonlat$obs1[idx]  &lt;- SSTdata[, 328]\nSSTlonlat$obs2[idx]  &lt;- SST_Oct97\n\nThe six-month-ahead prediction variances can also be computed (see Section C.1).\n\nC &lt;- Mest %*% Cov0 %*% t(Mest) + Ceta\n\nThe prediction variances are found by projecting the covariance matrix C onto the original space and extracting the diagonal elements. The prediction standard errors are the square root of the prediction variances and hence obtained as follows.\n\nSSTlonlat$predse[idx] &lt;-\n    sqrt(diag(E$v[, 1:n] %*% C %*% t(E$v[, 1:n])))\n\nPlotting proceeds in a straightforward fashion using ggplot2. In Figure 5.7 we show the April 1997 data and the EOF projection for that month, as well as the October 1997 data and the forecast for that month. From visual inspection, the El Niño pattern of high SSTs is captured but the predicted anomaly is too low. This result is qualitatively similar to what we obtained in Lab 3.3 using linear regression models.\n\n\n\n\n\n\nFigure 5.7: Top: SST data for April 1997 (left) and October 1997 (right). Middle: the EOF projection for April 1997 (left), and the forecast for October 1997 (right). Note the different color scales for the predictions (up to 1\\(^\\circ\\)C) and for the observations (up to 5\\(^\\circ\\)C). Bottom: Prediction standard errors for the forecast.\n\n\n\n\n\nState-Space Framework\nThe function DSTM_EM, provided with the package STRbook, runs the EM algorithm that carries out maximum likelihood estimation in a state-space model. The function takes the data Z, the initial covariance \\(\\mathbf{C}_0\\) in Cov0, the initial state \\(\\boldsymbol{\\mu}_0\\) in muinit, the evolution operator \\(\\mathbf{M}\\) in M, the covariance matrix \\(\\mathbf{C}_\\eta\\) in Ceta, the measurement-error variance \\(\\sigma^2_\\epsilon\\) in sigma2_eps, the matrix \\(\\mathbf{H}\\) in H, the maximum number of EM iterations in itermax, and the tolerance in tol (the tolerance is the smallest change in the log-likelihood, multiplied by \\(2\\), required across two consecutive iterations of the EM algorithm, before terminating). All parameters supplied to the function need to be initial guesses (usually those from the method of moments suffice); these will be updated using the EM algorithm.\n\nDSTM_Results &lt;- DSTM_EM(Z = SSTdata,\n                        Cov0 = Cov0,\n                        muinit = matrix(0, n, 1),\n                        M = Mest,\n                        Ceta = Ceta,\n                        sigma2_eps = 0.1,\n                        H = H &lt;- E$v[, 1:n],\n                        itermax = 10,\n                        tol = 1)\n\nThe returned object DSTM_Results contains the estimated parameters, the smoothed states and their covariances, and the complete-data negative log-likelihood. In this case, estimates of \\(\\{\\boldsymbol{\\alpha}_t\\}\\) using the state-space framework are practically identical to those obtained using the time-series framework presented in the first part of the Lab. We plot estimates of \\(\\alpha_{1,t}\\), \\(\\alpha_{2,t}\\), and \\(\\alpha_{3,t}\\) below for the two methods; see Figure 5.8.\n\npar(mfrow = c(1,3))\nfor(i in 1:3) {\n  plot(DSTM_Results$alpha_smooth[i, ], type = 'l',\n       xlab = \"t\", ylab = bquote(alpha[.(i)]))\n  lines(TS[, i], lty = 'dashed', col = 'red')\n}\n\n\n\n\n\n\n\nFigure 5.8: Estimates of \\(\\alpha_{1,t}\\) (left), \\(\\alpha_{2,t}\\) (center), and \\(\\alpha_{3,t}\\) (right) using the method of moments (red dashed line) and the EM algorithm (black solid line).\n\n\n\nLet us turn now to inference on the parameters. From Section C.2 note that the EM algorithm utilizes a Kalman filter that processes the data one time period (e.g., month) at a time. (Recall that with the method of moments we let \\(\\tau = 6\\) months, so we estimated directly the state transitions over six months.) Therefore, inferences on the parameters and their interpretations differ considerably. For example, the left and right panels of Figure 5.9 show the estimates of the evolution matrix for the two methods. At first sight, it appears that the matrix estimated using the EM algorithm is indicating a random-walk behavior. However, if we multiply the matrix \\(\\mathbf{M}\\) by itself six times (which then describes the evolution over six months), we obtain something that is relatively similar to what was estimated using the method of moments using a time lag of \\(\\tau = 6\\) months.\nTo make the plots in Figure 5.9, we first cast the matrices into objects of class Matrix. Note that using the function image on objects of class matrix generates similar plots that are, however, less informative. On the other hand, plots of Matrix objects are done using the function levelplot in the lattice package.\n\nimage(as(DSTM_Results$Mest, \"Matrix\"))\nimage(as(DSTM_Results$Mest %^% 6, \"Matrix\"))\nimage(as(Mest, \"Matrix\"))\n\n\n\n\n\n\n\nFigure 5.9: Estimate of a one-step-ahead evolution operator using the EM algorithm (left); EM estimate raised to the sixth power (center); and estimate of the six-steps-ahead evolution operator using the method of moments (right).\n\n\n\n\n\n\n\n\n\nFigure 5.10: Forecasts (left) and prediction standard errors (right) for the EOF coefficients in October 1997 using a lag-6 time-series model estimated using the method of moments (black) and a lag-1 state-space model estimated using the EM algorithm (red).\n\n\n\nForecasting proceeds the same way as in the method of moments. Specifically, we take the last smoothed time point (which corresponds to April 1997) and use the EM-estimated one-month propagator matrix to forecast the SSTs six months ahead. This is implemented easily using a for loop.\n\nalpha &lt;- DSTM_Results$alpha_smooth[, nT]\nP &lt;- DSTM_Results$Cov0\nfor(t in 1:6) {\n   alpha &lt;- DSTM_Results$Mest %*% alpha\n   P &lt;- DSTM_Results$Mest %*% P %*% t(DSTM_Results$Mest) +\n       DSTM_Results$Ceta\n}\n\nIt is instructive to compare the predictions and the prediction standard errors of the forecasted EOF coefficients using the two models; see Figure 5.10. While the prediction standard errors for the state-space model are slightly lower (which is expected since a measurement-error component of variability has been filtered out), it is remarkable that forecasts from the lag-6 time-series model are quite similar to those of the lag-1 state-space model. These two models and their respective inferences can be expected to differ when there is more nonlinearity in the process and/or the data are less complete in space and time.\n\nplot(alpha_forecast, xlab = \"i\",\n     ylab = expression(hat(alpha[i])), type = 'l')\nlines(alpha, col = 'red')\nplot(sqrt(diag(C)), xlab = \"i\",\n     ylab = expression(s.e.(hat(alpha[i]))), type = 'l')\nlines(sqrt(diag(P)), col = 'red')\n\nIn our concluding remarks, we remind the reader that in this Lab we considered a linear DSTM for modeling SSTs. Recent research has suggested that nonlinear DSTMs may provide superior prediction performance; see the case study in Appendix F.\n\n\n\n\nCressie, N., & Wikle, C. K. (2011). Statistics for spatio-temporal data. John Wiley & Sons.\n\n\nDouc, R., Moulines, E., & Stoffer, D. (2014). Nonlinear time series: Theory, methods and applications with R examples. Chapman & Hall/CRC.\n\n\nGamerman, D., & Lopes, H. F. (2006). Markov chain monte carlo: Stochastic simulation for bayesian inference (2nd ed.). Chapman & Hall/CRC.\n\n\nGoodfellow, I., Bengio, Y., Courville, A., & Bengio, Y. (2016). Deep learning. MIT Press.\n\n\nLukoševičius, M. (2012). A practical guide to applying echo state networks. In G. Montavon, G. B. Orr, & K.-R. Müller (Eds.), Neural networks: Tricks of the trade (2nd ed., pp. 659–686). Springer.\n\n\nLukoševičius, M., & Jaeger, H. (2009). Reservoir computing approaches to recurrent neural network training. Computer Science Review, 3(3), 127–149.\n\n\nMcDermott, P. L., & Wikle, C. K. (2016). A model-based approach for analog spatio-temporal dynamic forecasting. Environmetrics, 27(2), 70–82.\n\n\nMcDermott, P. L., & Wikle, C. K. (2017). An ensemble quadratic echo state network for non-linear spatio-temporal forecasting. Stat, 6(1), 315–330.\n\n\nMcDermott, P. L., Wikle, C. K., & Millspaugh, J. (2018). A hierarchical spatiotemporal analog forecasting model for count data. Ecology and Evolution, 8(1), 790–800.\n\n\nPrado, R., & West, M. (2010). Time series: Modeling, computation, and inference. Chapman & Hall/CRC.\n\n\nRue, H., & Held, L. (2005). Gaussian markov random fields: Theory and applications. Chapman & Hall/CRC.\n\n\nSchott, J. R. (2017). Matrix analysis for statistics (3rd ed.). John Wiley & Sons.\n\n\nShumway, R. H., & Stoffer, D. S. (1982). An approach to time series smoothing and forecasting using the EM algorithm. Journal of Time Series Analysis, 3(4), 253–264.\n\n\nShumway, R. H., & Stoffer, D. S. (2006). Time series analysis and its applications with r examples (2nd ed.). Springer.\n\n\nWikle, C. K., & Hooten, M. B. (2016). Hierarchical agent-based spatio-temporal dynamic models for discrete-valued data. In R. A. Davis, S. H. Holan, R. Lund, & N. Ravishanker (Eds.), Handbook of discrete-valued time series. Chapman & Hall/CRC.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Dynamic Spatio-Temporal Models</span>"
    ]
  },
  {
    "objectID": "Chapter6.html",
    "href": "Chapter6.html",
    "title": "6  Evaluating Spatio-Temporal Statistical Models",
    "section": "",
    "text": "6.1 Comparing Model Output to Data- What Do We Compare?\nHow do you know that the model you fitted actually fits well? At the core of our approach to the analysis of spatio-temporal data is a more or less detailed model containing statistical components that are designed to capture the spatio-temporal variability in the data. This chapter is about evaluating the spatio-temporal model that you fitted to describe (or to some extent explain) the variability in your data.\nModel building is an iterative process. We have data and/or a scientific hypothesis and we build the model around them (e.g., using the methods of Chapters 3–5). Then we must evaluate whether that model is a reasonable representation of the real world, and we should modify it accordingly if it is not. Sometimes this process is called model criticism because we are critiquing the strengths and weaknesses of our model, analogously to a movie critic summing up a film in terms of the things that work and the things that do not. In our case, we already know our model is wrong (recall Box’s aphorism), but we do not know just how wrong it is. Just as there is no correct model, there is no correct way to do model evaluation either. Rather, think of it as an investigation, using evidence from a variety of sources, into whether the model is reasonable or not. In this sense, we are “detectives” searching for evidence that our model can represent what we hope it represents in our particular application, or we are like medical doctors running tests on their patients. With that in mind, this chapter is about providing helpful suggestions on how to evaluate models for spatio-temporal data.\nWe split our model-evaluation suggestions into three primary components: model checking, model validation, and model selection. From our perspective, model checking consists of evaluating our model diagnostically to check its assumptions and its sensitivity to these assumptions and/or model choices. Model validation consists of evaluating how well our model actually reproduces the real-world quantities that we care about. Model selection is a framework in which to compare several plausible models. We consider each of these in some detail in this chapter.\nIt is important to note that the boundaries between these three components of model evaluation are fairly “fluid,” and the reader may well notice that there is a great deal of overlap in the sense that approaches discussed in one of these sections could be applied in other sections. Such is the nature of the topic, especially in the context of spatio-temporal modeling where, we must say, it is not all that well developed.\nIn this chapter, we focus less on how to implement the methods in R and more on the methods themselves. The reasons for this are twofold. First, several diagnostics are straightforward to calculate once predictive distributions are available. Second, there are only a few packages that have a comprehensive suite of diagnostic tools. We also note that quite a few more primary literature citations are included in this chapter than are given in the other chapters of the book, because there has not been extensive discussion of the topic in the spatio-temporal modeling literature.\nIn the next section, we digress slightly to discuss how model-based predictions can be compared to observations appropriately, since the two have different statistical properties.\nBefore we can talk about model evaluation, we have to decide what we will compare our model to. Hierarchical spatio-temporal modeling procedures give us the predictive distribution of the latent spatio-temporal process, \\(Y\\), given a (training) set of observations, \\(\\mathbf{Z}\\), which we represent as \\([Y | \\mathbf{Z}]\\). Because we are most often interested in this latent process, we would like to evaluate our model based on its ability to provide reasonable representations of \\(Y\\). But by definition, this process is hidden or latent—meaning that it is not observed directly—and thus we cannot directly evaluate our modeled process against the true process unless we do it through simulation (see Section 6.1.1 below). Alternatively, we can evaluate our model using predictive distributions of data, where we compare predictions of data (not of \\(Y\\)), based on our model, against the actual observed data. In particular, there are four types of predictive distributions of the data that we might use: the prior predictive distribution, the posterior predictive distribution, what we might call the empirical predictive distribution, and the empirical marginal distribution. Note that considering predictions of the data \\(Z\\) instead of \\(Y\\) involves the additional uncertainty associated with the measurement process. This is similar to standard regression modeling where the uncertainty of the prediction of an unobserved response is higher than the uncertainty of inferring the corresponding mean response. The four types of predictive distributions are defined in Section 6.1.2. Finally, given that we have simulated predictive distributions of either the data or the latent process, there is still the issue of which samples to compare. We touch on this in Section 6.1.3, with a brief discussion of various types of validation and cross-validation samples that we might use to evaluate our model.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Evaluating Spatio-Temporal Statistical Models</span>"
    ]
  },
  {
    "objectID": "Chapter6.html#sec-ModComp",
    "href": "Chapter6.html#sec-ModComp",
    "title": "6  Evaluating Spatio-Temporal Statistical Models",
    "section": "",
    "text": "6.1.1 Comparison to a Simulated “True” Process\nAlthough we do not have access to the latent process, \\(Y\\), for evaluating our model, there is a well-established simulation-based alternative for complex processes known as an observation system simulation experiment (OSSE; see Note 6.1). The basic idea of an OSSE is that one uses a complex simulation model to generate the true underlying process, say \\(Y_{\\mathrm{osse}}\\), and then, one generates simulated data, say \\(\\mathbf{Z}_{\\mathrm{osse}}\\), by applying an observation/sampling scheme to this true process that mimics the real-world sampling design and measurement technology. One can then use these OSSE-simulated observations in the statistical model and compare \\(Y\\) obtained from the predictive distribution based on the statistical model (i.e., \\([Y | \\mathbf{Z}_{\\mathrm{osse}}]\\)) against the simulated \\(Y_{\\mathrm{osse}}\\). The metrics used for such a comparison could be any of the metrics that are described in the following sections of this chapter. Not surprisingly, OSSEs are very useful when exploring different sampling schemes and, in the geophysical sciences, they are important for studying complex earth observing systems before expensive observing-system hardware is deployed. They are also very useful for comparing competing methodologies that infer \\(Y\\) or scientifically meaningful functions of \\(Y\\).\n\n\n\n\n\n\nNote 6.1: Observation System Simulation Experiment, OSSE\n\n\n\nObservation system simulation experiments are model-based simulation experiments that are designed to consider the effect of potential observing systems on the ability to recover the true underlying process of interest, especially when real-world observations are not available. For example, these are used extensively in the geophysical sciences to evaluate new remote sensing observation systems and new data assimilation forecast systems. However, they can also be used to evaluate the effectiveness of process modeling for complex real-world processes in the presence of incomplete observations, or when observations come at different levels of spatial and temporal support (see, for example, Berliner et al. (2003)). The typical OSSE consists of the following steps. Steps 1 and 2 correspond to simulation, and steps 3–5 are concerned with the subsequent statistical analysis.\n\nSimulate the spatio-temporal process of interest with a well-established (usually mechanistic) model. This simulation corresponds to the “true process.” Note that this is usually not a simulation from the statistical model of interest, since as much real-world complexity as possible is put into the simulation; call it \\(Y_{\\mathrm{osse}}\\).\nApply an observation-sampling protocol to the simulated true process to obtain synthetic observations. This sampling protocol introduces realistic observation error (bias, uncertainty, and change of support) and typically considers various missing-data scenarios; call the observations \\(\\mathbf{Z}_{\\mathrm{osse}}\\).\nUse \\(\\mathbf{Z}_{\\mathrm{osse}}\\) from step 2 in the spatio-temporal statistical model of interest, and obtain the predictive distribution \\([Y | \\mathbf{Z}_{\\mathrm{osse}}]\\) of the true process given the synthetic observations.\nCompare features of the predictive distribution of the true process from step 3 to \\(Y_{\\mathrm{osse}}\\) simulated in step 1.\nUse the results of step 4 to either (a) refine the statistical model that was used to obtain \\([Y | \\mathbf{Z}_{\\mathrm{osse}}]\\), or (b) refine the observation process, or both.\n\n\n\n\n\n6.1.2 Predictive Distributions of the Data\nThe posterior predictive distribution (ppd) is best thought of in the context of a Bayesian hierarchical model (BHM) and is given by Gelman et al. (2014)\n\\[\n[\\mathbf{Z}_{\\mathrm{ppd}} | \\mathbf{Z}] = \\iint [\\mathbf{Z}_{\\mathrm{ppd}} | \\mathbf{Y}, \\boldsymbol{\\theta}][\\mathbf{Y}, \\boldsymbol{\\theta}| \\mathbf{Z}] \\textrm{d}\\mathbf{Y}\\textrm{d}\\boldsymbol{\\theta},\n\\tag{6.1}\\]\nwhere \\(\\mathbf{Z}_{\\mathrm{ppd}}\\) is a vector of predictions at some chosen spatio-temporal locations. We have assumed that if we are given the true process, \\(\\mathbf{Y}\\), and parameters, \\(\\boldsymbol{\\theta}\\), then \\(\\mathbf{Z}_{\\mathrm{ppd}}\\) is independent of the observations \\(\\mathbf{Z}\\). (Note that we are using the vector \\(\\mathbf{Y}\\) to represent the process here to emphasize the fact that we are dealing with high-dimensional spatio-temporal processes.) In the models considered in this book, one can easily generate samples of \\(\\mathbf{Z}_{\\mathrm{ppd}}\\) through composition sampling. For example, generating posterior samples of \\(\\mathbf{Y}\\) and \\(\\boldsymbol{\\theta}\\) in the BHM context comes naturally with Markov chain Monte Carlo (MCMC) implementations, and these samples are just “plugged into” the data model \\([\\mathbf{Z}_{\\mathrm{ppd}} | \\mathbf{Y}, \\boldsymbol{\\theta}]\\) to generate the random draws of \\(\\mathbf{Z}_{\\mathrm{ppd}}\\).\nThe prior predictive distribution (pri) corresponds to the marginal distribution of the data and is given by\n\\[\n[\\mathbf{Z}_{\\mathrm{pri}}] = \\iint [\\mathbf{Z}_{\\mathrm{pri}} | \\mathbf{Y}, \\boldsymbol{\\theta}][\\mathbf{Y}| \\boldsymbol{\\theta}][\\boldsymbol{\\theta}] \\textrm{d}\\mathbf{Y}\\textrm{d}\\boldsymbol{\\theta},\n\\tag{6.2}\\]\nwhere \\(\\mathbf{Z}_{\\mathrm{pri}}\\) is a vector of predictions at selected spatio-temporal locations. As with the ppd, realizations from this distribution can be easily generated through composition sampling, where in this case we simply generate samples of \\(\\boldsymbol{\\theta}\\) from its prior distribution, use those to generate samples of the process \\(\\mathbf{Y}\\) from the process model, and then use these samples in the data model to generate realizations of the data, \\(\\mathbf{Z}_{\\mathrm{pri}}\\). In contrast to the ppd, no MCMC posterior samples need to be generated for this distribution.\nFinally, in the empirical hierarchical model (EHM) context we define the empirical predictive distribution (epd) as\n\\[\n[\\mathbf{Z}_{\\mathrm{epd}} | \\mathbf{Z}] = \\int [\\mathbf{Z}_{\\mathrm{epd}}| \\mathbf{Y}, \\widehat{\\boldsymbol{\\theta}}][\\mathbf{Y}| \\mathbf{Z}, \\widehat{\\boldsymbol{\\theta}}] \\textrm{d}\\mathbf{Y},\n\\tag{6.3}\\]\nand the empirical marginal distribution (emp) as\n\\[\n[\\mathbf{Z}_{\\mathrm{emp}}] = \\int [\\mathbf{Z}_{\\mathrm{emp}}| \\mathbf{Y}, \\widehat{\\boldsymbol{\\theta}}][\\mathbf{Y}| \\widehat{\\boldsymbol{\\theta}}] \\textrm{d}\\mathbf{Y},\n\\tag{6.4}\\]\nwhere \\(\\mathbf{Z}_{\\mathrm{epd}}\\) and \\(\\mathbf{Z}_{\\mathrm{emp}}\\) are vectors of predictions at selected spatio-temporal locations. The difference between Equation 6.3 and Equation 6.1, and between Equation 6.4 and Equation 6.2, is that instead of integrating over \\(\\boldsymbol{\\theta}\\) (which is assumed to be random in the BHM framework), we substitute an estimate \\(\\widehat{\\boldsymbol{\\theta}}\\) (e.g., a ML or REML estimate). Again, it is easy to sample from Equation 6.3 and Equation 6.4 by composition sampling since, once \\(\\widehat{\\boldsymbol{\\theta}}\\) is obtained, we can generate samples of \\(\\mathbf{Y}\\) easily from \\([\\mathbf{Y}| \\mathbf{Z}, \\widehat{\\boldsymbol{\\theta}}]\\) and from \\([\\mathbf{Y}| \\widehat{\\boldsymbol{\\theta}}]\\) with an MCMC. In the spatio-temporal Gaussian case, these are known multivariate normal distributions. Then \\(\\widehat{\\boldsymbol{\\theta}}\\) and the samples of \\(\\mathbf{Y}\\) are “plugged into” the data model to obtain samples of \\(\\mathbf{Z}_{\\mathrm{epd}}\\) and \\(\\mathbf{Z}_{\\mathrm{emp}}\\), respectively.\nFor illustration, consider the IDE model fitted to the Sydney radar data set in Lab 5.2. The top panels of Figure 6.1 show two samples from the epd empirical predictive distribution for the time points 08:45, 08:55, and 09:05, while the bottom panels show two samples from the emp empirical marginal distribution at the same time points. We shall discuss model validation using the predictive distributions of the data in Section 6.3.1, but simply “eyeballing” the plots may also reveal interesting features of the fitted model. First, the two samples from the epd are qualitatively quite similar, and this is usually an indication that the data have considerable influence on our predictive distributions. These epd samples are also very different from the emp samples, adding weight to the argument that the predictions in the top panels are predominantly data driven. Second, the samples from emp are very useful in revealing potential flaws and strengths of the model. For example, in this case the samples reveal that negative values for dBZ (green and blue) are just as likely as positive values for dBZ (orange and red), while we know that this is not a true reflection of the underlying science. On the other hand, the spatial length scales, and the persistence of the spatial features in time are similar to what one would expect just by looking at the data (see Section 2.1). These qualitative impressions, which will be made rigorous in the following sections, play a big role in selecting and tuning spatio-temporal models to improve their predictive ability.\n\n\n\n\n\n\nFigure 6.1: Two samples from the empirical predictive distribution (top) and the empirical marginal distribution (bottom), respectively, using the IDE model fitted to the Sydney radar data set in Lab 5.2.\n\n\n\n\n\n\n\n\n\nTip\n\n\n\nSeveral R packages contain built-in functionality for sampling from one or more of the predictive distributions listed in Equation 6.1–Equation 6.4. For example, the function krige in the package gstat can be used to generate simulations from both epd and emp, while the function simIDE in the package IDE can be used to generate simulations from emp after fitting an IDE model.\n\n\n\n\n6.1.3 Validation and Cross-Validation\nMost often we will have to compare real-world validation observations, say \\(\\mathbf{Z}_v\\), to observations predicted from our model, say \\(\\mathbf{Z}_p\\), from one (or all) of the four possibilities (ppd, pri, epd, emp) given in the previous section. The question here is, to which observations do we compare \\(\\mathbf{Z}_p\\)? The generalization ability of a model is a property that says how well it can predict a test data set (also referred to as a validation data set) that is different from the data used to train the model. (Note that the words “test” and “validation” are often used interchangeably in this context; we prefer to use “validation.”) So, assume that we have used a sample of data, \\(\\mathbf{Z}\\), to train our model. Before we describe the different possibilities for selecting validation data \\(\\mathbf{Z}_v\\), note that spatio-temporal processes have certain properties that should be considered when comparing model predictions to real-world observations. In particular, as with time series, spatio-temporal processes have a unidirectional time dependence and, like spatial processes, they have various degrees of spatial dependence. These dependencies should be considered whenever possible when evaluating a spatio-temporal model.\nIn general, the choice for validation observations \\(\\mathbf{Z}_v\\) can then be one of the following.\n\n\nTraining-data validation. It can be informative to use predicted observations of the training data set (\\(\\mathbf{Z}_v = \\mathbf{Z}\\)) to evaluate our model, particularly when evaluating the model’s ability to fit the data and for checking model assumptions via diagnostics. However, in the context of prediction, it is not typically recommended to use the training data for validating the model’s predictive ability, as the model’s training error is typically optimistic in the sense that it underestimates the predictive error that would be observed in an independent sample. Perhaps not surprisingly, the amount of this optimism is related to how strongly a predicted value from the training data set affects its own prediction (see Hastie et al., 2009, Chapter 7, for a comprehensive overview).\n\n\nWithin-sample validation. It is often useful to consider validation samples in which one leaves out a collection of spatial observations at time(s) within the spatio-temporal window defined by the extent of the training data set. Although one can leave out data at random in such settings, a more appropriate evaluation of spatio-temporal models results from leaving out “chunks” of data. This is because the spatio-temporal dependence structure must be very well characterized to adequately fill in large gaps for spatio-temporal processes (particularly dynamic processes). We saw such an example in Chapter 4, where we left out one period of the NOAA maximum temperature data but had observations both before and after that period.\n\n\nForecast validation. One of the most-used validation methods for time-dependent data is to leave out validation data beyond the last time period of the training period, and then to use the model to forecast at these future time periods. To predict the evolution of spatial features through time, the spatio-temporal model must adequately account for (typically non-separable) spatio-temporal dependence. Hence, forecast validation provides a gold standard for such evaluations.\n\n\nHindcast validation. Hindcasting (sometimes known as backtesting) refers to using the model to predict validation data at time periods before the first time period in the training sample. Of course, this presumes that we have access to data that pre-dates our training sample! This type of out-of-sample validation has similar advantages to forecast validations.\n\n\nCross-validation. There are many modeling situations where one needs all of the available observations to train the model, especially at the beginning and end of the data record. Or perhaps one is not certain that the periods in the forecast or hindcast validation sample are representative of the entire period (e.g., when the process is non-stationary in time). This is a situation where cross-validation can be quite helpful. Recall that we described cross-validation in Note 3.1. In the context of spatio-temporal models with complex dependence, one has to be careful that the cross-validation scheme chosen respects the dependence structure. In addition, many implementations of spatio-temporal models are computationally demanding, which can make traditional cross-validation very expensive.\n\n\nIn Lab 6.1 we provide an example of within-sample validation, where a 20-minute interval from the Sydney radar data set is treated as validation data, and a model using spatio-temporal basis functions is compared to an IDE model through their prediction performances in this 20-minute interval.\n\nSpatio-Temporal Support of Validation Data and Model Predictions\nSo far we have assumed that the validation data set, \\(\\mathbf{Z}_v\\), and the model-predicted observations, \\(\\mathbf{Z}_{\\mathrm{ppd}}\\) (say), are available at the same spatial and temporal support. In many applications, this is not the case. For example, our model may produce spatial fields (defined over a grid) at daily time increments, but observations may be station data observed every hour. In some sense, if our data model is realistic, then we may have already accounted for these types of change of support. In other cases, one may perform ad hoc interpolation or aggregation to bring the validation and model support into agreement. This is a standard approach in many meteorological forecasting studies (see, for example, Brown et al., 2012). The hierarchical modeling paradigm discussed here does provide the flexibility for incorporating formal change of support, but this is beyond the scope of this book (for more details, see Cressie & Wikle, 2011, Chapter 7, and the references therein). In the remainder of this chapter, we shall assume that the validation sample and the associated model predictions are at the same spatio-temporal support.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Evaluating Spatio-Temporal Statistical Models</span>"
    ]
  },
  {
    "objectID": "Chapter6.html#sec-ModCheck",
    "href": "Chapter6.html#sec-ModCheck",
    "title": "6  Evaluating Spatio-Temporal Statistical Models",
    "section": "6.2 Model Checking",
    "text": "6.2 Model Checking\nNow that we know what to compare to what, consider the first of our three types of model evaluation: model checking. From our perspective, this corresponds to checking model assumptions and the sensitivity of the model output to these assumptions and/or model choices. That is, we evaluate our spatio-temporal model using statistical diagnostics. We begin with a brief description of possible extensions of standard regression diagnostics, followed by some simple graphical diagnostics, and then we give a brief description of robustness checks.\n\n6.2.1 Extensions of Regression Diagnostics\nAs in any statistical-modeling problem, one should evaluate spatio-temporal modeling assumptions by employing various diagnostic tools. In regression models and GLMs, one often begins such an analysis by evaluating residuals, usually obtained by subtracting the estimated or predicted response from the data. Looking at residuals may bring our attention to certain aspects of the data that we have missed in our model.\nAs discussed in Chapter 3, for additive Gaussian measurement error, we can certainly do this in the spatio-temporal case by evaluating the spatio-temporal residuals,\n\\[\n\\widehat{e}(\\mathbf{s}_i;t_j) \\equiv Z(\\mathbf{s}_i;t_j) - \\widehat{Z}_p(\\mathbf{s}_i;t_j),\n\\tag{6.5}\\]\nfor \\(i=1,\\ldots,m\\) and \\(j=1,\\ldots,T\\), where \\(\\widehat{Z}_p(\\mathbf{s}_i;t_j)\\) is the mean of the ppd or epd as discussed in Section 6.1.2. Note that for notational simplicity we assume in this chapter that we have the same number of observations (\\(m\\)) at the same spatial locations for each time point. This need not be the case, and the equations can easily be modified to represent the more general setting of a different number of observations at different locations for each time point.\nIn Figure 6.2 we show the histograms of the spatio-temporal residuals obtained for the two models evaluated in Lab 6.1 using validation data for an entire 20-minute block (left panel) and at random locations (right panel). It is clear from these histograms that for both types of missingness, the variance of the residuals based on the IDE model is slightly lower than that based on the model used by the FRK model.\n\n\n\n\n\n\nFigure 6.2: Histograms of errors at validation locations for the fitted IDE (blue) and FRK (red) models for the time points that are omitted from the data (left) and for space-time locations that are missing at random (right).\n\n\n\nIn addition to the classical residuals given in Equation 6.5, we can consider deviance or Pearson chi-squared residuals for non-Gaussian data models (as discussed in Chapter 3). Given spatio-temporal residuals, it is usually helpful to visualize them using the various tools discussed in Chapter 2 (see Lab 6.1). In addition, as discussed in Chapter 3, one can consider quantitative summaries to evaluate residual temporal, spatial, or spatio-temporal dependence, such as with the PACF, Moran’s \\(I\\), and S-T covariogram summaries. In the case of the latter, one may also consider more localized summaries, known as local indicators of spatial association (LISAs) or their spatio-temporal equivalents (ST-LISAs) where the component pieces of a summary statistic are indexed by their location and evaluated individually (see Cressie & Wikle, 2011, Section 5.1).\nDiagnostics have also been developed specifically for models with spatial dependence that are easily extended to spatio-temporal models. For example, building on the ground-breaking work of Cook (1977), Haslett (1999) considered a simple approach for “deletion diagnostics” in models with correlated errors. For example, if one has a model such as \\(\\mathbf{Z}\\sim Gau(\\mathbf{X}\\boldsymbol{\\beta}, \\mathbf{C}_z)\\), then interest is in the effect of leaving out elements of \\(\\mathbf{Z}\\) on the estimation of \\(\\boldsymbol{\\beta}\\). Analogously to \\(K\\)-fold cross-validation discussed in Chapter 3, assume we split our observations into two groups, \\(\\mathbf{Z}= \\{\\mathbf{Z}_b, \\mathbf{Z}_v\\}\\), and then we predict \\(\\mathbf{Z}_v\\) based only on training data \\(\\mathbf{Z}_b\\), which we denote by \\(\\widehat{\\mathbf{Z}}^{(-v)}\\). Then, as with standard (independent and identically distributed (\\(iid\\)) errors) regression, one can form diagnostics in the correlated-error context, analogous to the well-known DFBETAS and Cook’s distance diagnostics. These compare the regression coefficients estimated under the hold-out scenario (say, \\(\\widehat{\\boldsymbol{\\beta}}^{(-v)}\\)) to the parameters estimated using all of the data (\\(\\widehat{\\boldsymbol{\\beta}}\\)), and Haslett (1999) provides some efficient approaches to obtain \\(\\widehat{\\boldsymbol{\\beta}}^{(-v)}\\). It is important to note that these diagnostics are based on the cross-validated residuals,\n\\[\n\\widehat{\\mathbf{e}}_v \\equiv \\mathbf{Z}_v - \\widehat{\\mathbf{Z}}^{(-v)},\n\\tag{6.6}\\]\nrather than the within-sample residuals given by Equation 6.5.\n\n\n6.2.2 Graphical Diagnostics\nSeveral diagnostic plots have proven useful for evaluating predictive models, and these largely depend on the observation type. Recall, from our discussions in Chapters 3–5, that it is fairly straightforward to model spatio-temporal binary or count data using the techniques we described within a GLM framework. Our discussion below on graphical diagnostics covers the most common types of data encountered in practice.\nWhen considering binary outcomes, which are common when observing processes such as occupancy (presence–absence) in ecology, and precipitation (rain or no rain) in meteorology, there is a long tradition in statistics and engineering of considering a receiver operating characteristic (ROC) curve. For binary data, a statistical model (say, a Bernoulli data model with a logit link function) provides an estimate of the probability that the outcome is a 1 (versus a 0). Then, for predictions, a threshold probability is typically set, and the predicted outcome is put equal to 1 if the estimated probability is larger than the threshold, and put equal to 0 if not. Clearly, the performance of the predictions will depend on the threshold. The ROC plot presents the true positive rate (i.e., sensitivity, namely the percentage of 1s that were correctly predicted) on the \\(y\\)-axis versus the false positive rate (i.e., 1 minus the specificity, namely the percentage of 0s that were incorrectly predicted to be 1s) on the \\(x\\)-axis as the value of the threshold probability changes (from 0 to 1). Since we prefer a model that gives a high true positive rate and low false positive rate, we like to see ROC curves that are well above the 45-degree line. One often summarizes an ROC curve by the area under the ROC curve (sometimes abbreviated as “area under the curve” (AUC)), with the best possible area being 1.0 and with a value of 0.5 corresponding to a “no information” (i.e., a coin-flipping) model. Figure 6.3 shows two ROC curves for a data set based on 100 simulated Bernoulli responses from a logistic regression model with simulated covariates. The black ROC curve corresponds to a simple model (a logistic regression model with fewer covariates than used for the simulation) and the red ROC curve corresponds to flipping a coin (random guessing). The AUCs for the two models are 0.89 and 0.59, respectively. Although useful for evaluating prediction, the ROC curve is limited in that it is generally insensitive to prediction biases (Wilks, 2011, Chapter 8).\n\n\n\n\n\n\nTip\n\n\n\nROC curves can be easily generated in R using the functions prediction and performance from the package ROCR or the function roc.plot from the package verification.\n\n\n\n\n\n\n\n\nFigure 6.3: ROC curves for two models fitted to a simulated Bernoulli (binary) data set with 100 observations that was generated from a logistic regression model with simulated covariates. The black line corresponds to the ROC curve for a simpler logistic regression model (“Model A”) with a corresponding AUC = 0.89, and the red line is the ROC curve for a model (“Model B”) based just on random guessing with AUC = 0.59. This figure was obtained using the roc.plot function in the verification R package.\n\n\n\nThere are several diagnostic plots that are used for meteorological forecast validation but are less commonly used in statistics (see Wilks, 2011, Chapter 8). Some of these plots attempt to show elements of the joint distribution of the prediction and the corresponding validation observation. As an example, conditional quantile plots are used for continuous responses (e.g., temperature). In particular, these plots consider predicted values on the \\(x\\)-axis and the associated quantiles from the empirical predictive distribution of the observations associated with the predictions on the \\(y\\)-axis. This allows one to observe potential problems with the predictive model (e.g., biases). This is better seen in an example. The left panel of Figure 6.4 shows a conditional quantile plot for simulated data in a situation where the predictive model is, on average, biased high relative to the observations by about 3 units. This can easily be seen in this plot since the conditional distribution of the observations given the predictions is shifted below the 45-degree line. In the right panel of Figure 6.4 we show the conditional quantile plot for the IDE model predictions in Lab 6.1 for the missing 20-minute interval. The predictions appear to be unbiased except when the observed reflectivity is close to zero.\nSimilar decomposition-based plots can be used for probabilistic predictions of discrete events (e.g., the reliability diagram and the discrimination diagram; see Wilks (2011), Chapter 8, and the R package verification) and have an advantage over the ROC plot since they display the joint distribution of prediction and corresponding observations and thus can reveal forecast biases.\n\n\n\n\n\n\nTip\n\n\n\nConditional quantile plots can be generated in R using the function conditional.quantile from the package verification.\n\n\n\n\n\n\n\n\nFigure 6.4: Left: Conditional quantile plot for 1000 simulated observations and predictions in which the model produces predictions that are biased approximately 3 units high relative to the observations. Right: Conditional quantile plot for the IDE model predictions in the missing 20-minute gap in Lab 6.1. These figures were obtained using the conditional.quantile function in the verification R package. Note that the \\(x\\)-axis gives the histogram associated with the verification observations \\(\\{Z_v^i,\\ i=1,\\ldots,n_f\\}\\) and the colored lines in the plot correspond to smooth quantiles from the conditional distribution of predicted values for each of these verification observations .\n\n\n\nWhen one has samples from a predictive distribution (as described in Section 6.1.2) or an ensemble forecasting model (such as described in Appendix F), there are additional graphical assessments that can be informative to evaluate a model’s predictive performance. Consider the so-called verification ranked histogram. Suppose we have \\(n_f\\) different predictive situations, each with an observation (an element of \\(\\mathbf{Z}_v\\), say \\(Z^i_{v}\\), for \\(i=1,\\ldots,n_f\\)) to be used in verification, and for each of these predictions we have \\(n_s\\) samples from the predictive distribution, say \\([\\mathbf{Z}_{\\mathrm{epd}}^{i} | \\mathbf{Z}_b]\\), \\(i=1,\\ldots,n_f\\), where \\(\\mathbf{Z}_{\\mathrm{epd}}\\) is a sample of size \\(n_s\\) (note that we could just as easily consider the ppd here). For each of the \\(n_f\\) predictive situations we calculate the rank of the observation relative to the ordered \\(n_s\\) samples; for example, if the observation is less than the smallest sample member, then it gets a rank of 1, if it is larger than the largest sample member, it gets a rank of \\(n_s +1\\), and so on. If the observation and the samples are from the same distribution, then the rank of the observation should be uniformly distributed (since it is equally likely to fall anywhere in the sample). Thus, we plot the \\(n_f\\) ranks in a histogram and look for deviations from uniformity. As shown in Wilks (2011, Chapter 8), deviations from uniformity can suggest problems such as bias or over-/under-dispersion.\nAs an example, Figure 6.5 shows verification histograms for three cases of (simulated) observations using the Rankhist and PlotRankhist functions in the package SpecsVerification. Each example is based on \\(n_f = 2000\\) verification observations (i.e., \\(\\{Z^i_v,\\ i=1,\\ldots,2000\\}\\)) and \\(n_s = 20\\) samples from the associated predictive distribution \\([\\mathbf{Z}^i_{\\mathrm{epd}} | \\mathbf{Z}_b]\\) for each of these verification observations. The left panel shows a case where the predictive distribution is under-dispersed relative to the observations and the right panel shows a case where the predictions are biased low relative to the observations. The center panel shows a case where the observations and predictions are from the same distribution, which implies rank uniformity. Note that there is a reasonable amount of sampling variability in these rank histograms. It is fairly straightforward to use a chi-squared test to test a null hypothesis that the histogram corresponds to a uniform distribution (see Weigel, 2012). The SpecsVerification package will implement this test in the context of the rank histogram. For the simulated example, the \\(p\\)-values for the left-panel and right-panel cases in Figure 6.5 are very close to 0, resulting in rejection of the null hypothesis of rank uniformity, whereas the case represented by the center panel has a \\(p\\)-value close to 0.8, so that rank uniformity is not rejected.\n\n\n\n\n\n\nFigure 6.5: Verification ranked histograms corresponding to \\(n_f = 2000\\) simulated observations and \\(n_s = 20\\) samples from the associated predictive distribution for these 2000 observations. Left: the predictive distribution is under-dispersed relative to the observations; Center: the predictive distribution and the observation distribution appear the same (rank uniformity); Right: the predictive distribution is biased low relative to the observations. This figure was obtained using the Rankhist function in the SpecsVerification R package.\n\n\n\nThe graphical methods described here are not really designed for spatio-temporal data. One might be able to consider predictions at different time periods and spatial locations as different cases for comparison, but spatio-temporal dependence is not explicitly accounted for in such comparisons. This could be problematic as predictions in close proximity in space and time are spatio-temporally correlated, and it is therefore relatively easy to select a subset of points that indicate that predictions are biased, when in reality they are not. Any apparent bias could be a fortuitous outcome of the space-time locations chosen for validation. One way to get around this issue is to consider predictions at different (well-separated) time points at the same location in space (so as to break the spatio-temporal dependence). Then one could look at several such plots for different locations in space to gain an appreciation for the geographical influence on model performance. In the context of the rank histogram, there have been some attempts to consider multivariate predictands (e.g., multiple locations in space and/or multivariate responses), but the challenge then is to develop ranks in this multivariate setting. Perhaps the most useful such approach is based on the so-called minimum spanning tree histograms; see the summary in Wilks (2011). The development of graphical diagnostics for spatio-temporal data is very much a research topic at the time of writing.\n\n\n6.2.3 Sensitivity Analysis\nAn important part of model evaluation is the notion of robustness. Informally, we might say that model robustness is an evaluation of whether certain model assumptions have too much influence on model predictions. (This is a bit different from the more classical topic of robust estimation of model parameters.) Here we focus on the relatively simple notion of sensitivity analysis in the context of spatio-temporal modeling. In a sensitivity analysis, we evaluate how much our predictions change as we vary some aspect of our model (e.g., the number of basis functions or the degree of spatial dependence in an error distribution). We briefly describe some heuristic approaches to sensitivity analysis in this section, but we note that the validation statistics described below in Section 6.3 could also be used as metrics to evaluate model sensitivity.\nIn the case where we fix certain parameters at their estimates (e.g., covariance parameters in S-T kriging), we should evaluate the sensitivity of the model predictions to the estimated values. Note that a common criticism of such empirical plug-in approaches (used in an EHM implementation) is that they do not capture sufficient variability (e.g., relative to a BHM implementation) because they do not take the uncertainty of the parameter estimates directly into account. Nonparametric bootstrapping could be used, but it can be challenging to take bootstrap samples that adequately represent the dependence structure in the spatio-temporal data. So, to evaluate the sensitivity of model predictions to fixing parameters at their data-based estimates, one might consider how sensitive the prediction errors are to the fixed parameters, \\(\\boldsymbol{\\theta}\\), being estimated by two different methods, say using MLE and REML. Then, as in the spatial setting of Kang et al. (2009), we can consider heuristic measures such as the ratio of predictive standard deviations. In the spatio-temporal setting, this can be written as\n\\[\n\\left[\\frac{\\textrm{var}(Y(\\mathbf{s};t) | \\mathbf{Z}, \\widehat{\\boldsymbol{\\theta}}_a )}{\\textrm{var}(Y(\\mathbf{s};t) | \\mathbf{Z}, \\widehat{\\boldsymbol{\\theta}}_b) }   \\right]^{1/2},\n\\tag{6.7}\\]\nwhere \\(\\mathbf{Z}\\) represents the data, \\(\\widehat{\\boldsymbol{\\theta}}_a\\) and \\(\\widehat{\\boldsymbol{\\theta}}_b\\) are two parameter estimates (e.g., ML and REML estimates), and \\(\\textrm{var}(Y(\\mathbf{s};t) | \\mathbf{Z}, \\boldsymbol{\\theta})\\) represents the process’ predictive variance at \\((\\mathbf{s};t)\\) for fixed \\(\\boldsymbol{\\theta}\\). Clearly, if the ratio in Equation 6.7 is close to 1, then it suggests that there is little sensitivity in the predictive standard deviations relative to differences in the parameter estimates \\(\\widehat{\\boldsymbol{\\theta}}_a\\) and \\(\\widehat{\\boldsymbol{\\theta}}_b\\).\nSimilarly, we might compare the standardized differences in predictive means,\n\\[\n\\frac{E(Y(\\mathbf{s};t) | \\mathbf{Z}, \\widehat{\\boldsymbol{\\theta}}_a ) - E(Y(\\mathbf{s};t) | \\mathbf{Z}, \\widehat{\\boldsymbol{\\theta}}_b) }{\\{\\textrm{var}(Y(\\mathbf{s};t) | \\mathbf{Z}, \\widehat{\\boldsymbol{\\theta}}_b)\\}^{1/2} },\n\\tag{6.8}\\]\nwhere \\(E(Y(\\mathbf{s};t) | \\mathbf{Z}, \\boldsymbol{\\theta})\\) is the predictive mean for fixed \\(\\boldsymbol{\\theta}\\). In this case, if Equation 6.8 is close to 0, it suggests that the predictive means are not overly sensitive to these parameter-estimate differences. We also note that Equation 6.7 and Equation 6.8 are given for an individual location \\((\\mathbf{s};t)\\) in space and time, but one could do additional averaging over regions in space and/or time periods and/or produce plots in space and time.\nFor illustration, consider the maximum temperature in the NOAA data set fitted using a Gaussian process, as in Lab 4.1. One can fit the theoretical semivariogram to the data using either least squares or weighted least squares. What, then, is the sensitivity of our predictions to the choice of fitting method? With gstat, one can call fit.StVariogram with fit.method = 6 (default) for least squares, or fit.method = 2 for weights based on the number of data pairs in the spatio-temporal bins used to construct the empirical semivariogram (see Cressie, 1993, Chapter 2). For a grid cell at (100\\(^\\circ\\)W, 34.9\\(^\\circ\\)N) on 14 July 1993, the ratio of the predictive standard deviations is 1.03, while the standardized difference in the predictive means is 0.00529. When can a spatio-temporal model be considered robust in terms of its predictions? The answer to this question largely depends on the reason why the model was fitted in the first place and is application-dependent, but, in the context of these maximum-temperature data, it is reasonable to say that the estimation method chosen does not seem to impact the predictions at the chosen space-time location in a substantial way.\nIn Bayesian implementations of spatio-temporal models, we may still be interested in the sensitivity of our posterior distributions to certain parameters or model assumptions. In this case, we could make different model assumptions and compare samples of \\(Y\\) from the posterior distribution or samples of \\(\\mathbf{Z}_{\\mathrm{ppd}}\\) from the ppd. Comparisons could be made using measures analogous to Equation 6.7 and Equation 6.8 or more general measures of distributional comparisons discussed below in Section 6.3. In the context of MCMC algorithms that generate posterior samples, this can be costly in complex models as it requires that one fit the full model with possibly many different data-model, process-model, and parameter-model distributions.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Evaluating Spatio-Temporal Statistical Models</span>"
    ]
  },
  {
    "objectID": "Chapter6.html#sec-ModValidate",
    "href": "Chapter6.html#sec-ModValidate",
    "title": "6  Evaluating Spatio-Temporal Statistical Models",
    "section": "6.3 Model Validation",
    "text": "6.3 Model Validation\nRecall that model validation is simply an attempt to determine how closely our model represents the real-world process of interest, as manifested by the data we observe. Specifically, after checking our model assumptions through diagnostics and sensitivity analysis, we can validate it against the real world. Although by no means exhaustive, this section presents some of the more common model-validation approaches that are used in practice.\n\n6.3.1 Predictive Model Validation\nOne of the simplest ideas in model validation is to assess whether the data that are generated from our fitted model “look” like data that we have observed. That is, we can consider samples of \\(\\mathbf{Z}_{\\mathrm{ppd}}\\) or \\(\\mathbf{Z}_{\\mathrm{epd}}\\) from the ppd or the epd, respectively, as described in Section 6.1.2. Given that we have samples of \\(\\mathbf{Z}_{\\mathrm{ppd}}\\) or of \\(\\mathbf{Z}_{\\mathrm{epd}}\\), what do we do with them?\nAs in Section 6.2, we refer to these samples simply as \\(\\mathbf{Z}_p\\). We can look at any diagnostics we like to help us discern how similar these draws from the ppd or the epd are to the observed data – remember, we are trying to answer the question as to whether the observed data look reasonable based on the predictive distribution obtained from our model. These diagnostics are sometimes called predictive diagnostics. Here, discussion focuses on posterior predictive diagnostics based on the ppd, but there is an obvious analog of empirical predictive diagnostics where one considers the epd rather than the ppd.\nAs outlined in Gelman et al. (2014), a formalization of this notion is to consider a discrepancy measure, \\(T(\\mathbf{Z}; \\mathbf{Y}, \\boldsymbol{\\theta})\\). The discrepancy \\(T(\\cdot)\\) is specified by the modeler and may be a measure of overall fit (e.g., a scoring rule such as described in Section 6.3.4) or any other feature of the data, the process, and the parameters. So, one calculates \\(T(\\cdot)\\) for each of \\(L\\) replicates of the simulated data, and also for the observed data.\nWe now change notation slightly to show in detail how posterior predictive diagnostics can be constructed. Specifically, for the simulated observations, we calculate \\(\\{T(\\mathbf{Z}^{(\\ell)}_{p}; \\mathbf{Y}^{(\\ell)}, \\boldsymbol{\\theta}^{(\\ell)}):\\) \\(\\ell = 1,\\ldots,L\\}\\) for the \\(L\\) replicates \\(\\{\\mathbf{Z}^{(\\ell)}_{p}\\}\\) sampled from \\([\\mathbf{Z}_p | \\mathbf{Z}]\\) based on the samples \\(\\{\\mathbf{Y}^{(\\ell)}, \\boldsymbol{\\theta}^{(\\ell)}\\}\\) from \\([\\mathbf{Y}, \\boldsymbol{\\theta}| \\mathbf{Z}]\\). Simple scatter plots of the discrepancy measures from the replicated data samples, \\(T(\\mathbf{Z}^{(\\ell)}_{p}; \\mathbf{Y}^{(\\ell)}, \\boldsymbol{\\theta}^{(\\ell)})\\), versus the discrepancy measure from the observed data, \\(T(\\mathbf{Z}_{p}; \\mathbf{Y}^{(\\ell)}, \\boldsymbol{\\theta}^{(\\ell)})\\), can be informative. For example, if the points are scattered far from a 45-degree line, then we can assume that for this choice of \\(T\\) the model is not generating data that behave like the observations (e.g., see Gelman et al., 2014, Section 6.3).\nWe can make this procedure less subjective by considering posterior predictive \\(p\\)-values, which are given by\n\\[\np_B = \\Pr(T(\\mathbf{Z}_p; \\mathbf{Y}, \\boldsymbol{\\theta}) \\ge T(\\mathbf{Z}; \\mathbf{Y}, \\boldsymbol{\\theta}) | \\mathbf{Z}),\n\\]\nwhere the probability is calculated based on the samples \\(\\{T(\\mathbf{Z}^{(\\ell)}_{p}; \\mathbf{Y}^{(\\ell)}, \\boldsymbol{\\theta}^{(\\ell)}): \\ell = 1,\\ldots,L\\}\\); note, the “B” subscript in \\(p_B\\) refers to “Bayes.” In general, values of \\(p_B\\) close to 0 or 1 cast doubt on whether the model produces data similar to the observed \\(\\mathbf{Z}\\) (relative to the chosen discrepancy measure), in which case one may need to reconsider the model formulation. It is important to reiterate that this “\\(p\\)-value” is best used as a diagnostic procedure, not for formal statistical testing. As mentioned, one can also construct analogous predictive diagnostics based on the prior predictive distribution (i.e., prior predictive \\(p\\)-values), the empirical predictive distribution (i.e., empirical predictive \\(p\\)-values), and the empirical marginal distribution (i.e., empirical marginal \\(p\\)-values). In the epd context, this has been formulated as a Monte Carlo test for validation (e.g., Kornak et al., 2006).\nFor illustration, consider the example of Section 6.1.2 (Sydney radar data set and the IDE model). We chose discrepancy measures to be the minimum (\\(T_{\\min}\\)) and maximum (\\(T_{\\max}\\)) radar reflectivity across the grid boxes with centroid at \\(s_1 = 26.25\\) (i.e., a vertical transect) over the three time points shown in Figure 6.1. In Figure 6.6 we plot the empirical marginal distributions and empirical predictive distributions for these two discrepancy measures as obtained from \\(L=500\\) replications, together with the observed minimum and maximum. In both of these cases, and for both distributions, the \\(p\\)-values are greater than 0.05, suggesting a reasonable fit. Specifically, the empirical marginal \\(p\\)-value and empirical predictive \\(p\\)-value for \\(T_{\\min}\\) were 0.09 and 0.442, respectively, while the \\(p\\)-values for \\(T_{\\max}\\) were 0.364 and 0.33, respectively (note that the \\(p\\)-values we report are \\(\\min(p_B, 1-p_B)\\)).\n\n\n\n\n\n\nFigure 6.6: Empirical marginal distribution (green) and empirical predictive distribution (blue) densities for the minimum (\\(T_{\\min}\\), left) and maximum (\\(T_{\\max}\\), right) radar reflectivities across all grid boxes with centroid at \\(s_1 = 26.25\\) (i.e., a vertical transect) for the times shown in Figure 6.1. In both panels, the red line denotes the observed statistic.\n\n\n\n\n\n6.3.2 Spatio-Temporal Validation Statistics\nPerhaps the most common scalar validation statistic for continuous-valued spatio-temporal processes is the mean squared prediction error (MSPE), which for spatio-temporal validation sample \\(\\{Z_v(\\mathbf{s}_i;t_j): j=1,\\ldots,T;\\ i=1,\\ldots,m\\}\\), and corresponding predictions \\(\\{\\widehat{Z}_v(\\mathbf{s}_i;t_j) \\}\\), is given by\n\\[\nMSPE = \\frac{1}{T m} \\sum_{j=1}^T \\sum_{i=1}^m \\{Z_v(\\mathbf{s}_i;t_j) - \\widehat{Z}_v(\\mathbf{s}_i;t_j)\\}^2,\n\\]\nwhere again, for convenience, we have assumed the same number of spatial observations for each time period (which simplifies the notation, but different numbers of spatial locations for each time are easily accommodated). In this section we assume that \\(\\{\\widehat{Z}_v(\\mathbf{s}_i;t_j)\\}\\) are predictions based on all of the data, \\(\\mathbf{Z}\\) (we relax that assumption in Section 6.3.3). Sometimes one might be interested in looking at MSPE for a particular time point, averaged across space, or for a particular spatial location (or region), averaged across time. The MSPE summary is so popular because it is an empirical measure of expected squared error loss which, when minimized, results in the S-T kriging predictor. In addition, the MSPE can be decomposed into a term corresponding to the bias (squared) of the predictor plus a term corresponding to the variance of the predictor. This is important because a large part of model-building consists of exploring the trade-offs between bias and variance. It is equally common to consider the root mean squared prediction error (RMSPE), which is simply the square root of the MSPE. This is sometimes favored because the units of the RMSPE are the same as those of the observations.\nIn cases where one wishes to protect against the influence of outliers, it is common to consider the mean absolute prediction error (MAPE), which can be computed from\n\\[\nMAPE = \\frac{1}{T  m} \\sum_{j=1}^T \\sum_{i=1}^m |Z_v(\\mathbf{s}_i;t_j) - \\widehat{Z}_v(\\mathbf{s}_i;t_j)|.\n\\]\nAlthough a useful summary for validation, the MAPE does not have the natural decomposition into bias and variance components that the MSPE does. But we note that for errors that do not exhibit bias, the MAPE can be interpreted as a robust version of the RMSPE.\nAnother common scalar validation statistic for spatio-temporal data is the so-called anomaly correlation coefficient (ACC). This is the usual Pearson product moment formula for correlation (i.e., the empirical correlation) applied to anomalies of the observations and predictions. Anomalies (a term that comes from the atmospheric sciences) are just deviations with respect to a long-term average of the observations (e.g., climatology in atmospheric applications). That is, let \\(Z'_v(\\mathbf{s}_i;t_j) \\equiv Z_v(\\mathbf{s}_i;t_j) - Z_a(\\mathbf{s}_i)\\) and \\(\\widehat{Z}'_v(\\mathbf{s}_i;t_j) \\equiv \\widehat{Z}_v(\\mathbf{s}_i;t_j) - Z_a(\\mathbf{s}_i)\\) be the anomalies of the validation observations and corresponding predictions relative to the time-averaged observation, \\(Z_a(\\mathbf{s}_i)\\), at location \\(\\mathbf{s}_i\\), for \\(i=1,\\ldots,m\\). Then the ACC is just the empirical correlation between \\(\\{Z'_v(\\mathbf{s}_i;t_j)\\}\\) and \\(\\{\\widehat{Z}_v'(\\mathbf{s}_i;t_j)\\}\\). This can be calculated across all time periods and spatial locations, or across time for each spatial location separately (and plotted on a map), or across space for each time period separately (and plotted as a time series). As with any correlation measure, the ACC does not account for bias in predictions relative to the observations, but it is still useful for spatial-field validation as it does detect phase differences (shifts) between fields. In contrast, the MSPE captures bias and variance and is not invariant to linear association.\nThe statistics literature has considered several simple heuristic validation metrics for spatio-temporal data. For example, in the context of within-sample validation, for spatio-temporal validation data \\(\\{Z_v(\\mathbf{s}_i;t_j)\\}\\) and corresponding mean predictions \\(\\{\\widehat{Z}_v(\\mathbf{s}_i;t_j)\\}\\), one can consider the following spatial validation statistics based on residuals and predictive variances as outlined in Carroll & Cressie (1996):\n\\[\nV_1(\\mathbf{s}_i) = \\frac{(1/T) \\sum_{j=1}^T \\{Z_v(\\mathbf{s}_i;t_j) - \\widehat{Z}_v(\\mathbf{s}_i;t_j)\\}}{(1/T) \\{\\sum_{j=1}^T \\textrm{var}(Z_v(\\mathbf{s}_i;t_j) | \\mathbf{Z})\\}^{1/2}},\n\\tag{6.9}\\]\n\\[\nV_2(\\mathbf{s}_i) = \\left[\\frac{(1/T) \\sum_{j=1}^T \\{Z_v(\\mathbf{s}_i;t_j) - \\widehat{Z}_v(\\mathbf{s}_i;t_j)\\}^2}{(1/T) \\sum_{j=1}^T  \\textrm{var}(Z_v(\\mathbf{s}_i;t_j) | \\mathbf{Z})}\\right]^{1/2},\n\\tag{6.10}\\]\n\\[\nV_3(\\mathbf{s}_i) = \\left[ \\frac{1}{T} \\sum_{j=1}^T \\{Z_v(\\mathbf{s}_i;t_j) - \\widehat{Z}_v(\\mathbf{s}_i;t_j)\\}^2   \\right]^{1/2},\n\\tag{6.11}\\]\nwhere \\(\\textrm{var}(Z_v(\\mathbf{s}_i;t_j) | \\mathbf{Z})\\) is the predictive variance. The summary \\(V_1(\\mathbf{s}_i)\\) provides a sense of the bias of the predictors in space (i.e., we expect this value to be close to 0 if there is no predictive bias). Similarly, \\(V_2(\\mathbf{s}_i)\\) provides a measure of the accuracy of the MSPEs and should be close to 1 if the model estimate of prediction error is reasonable. Finally, \\(V_3(\\mathbf{s}_i)\\) is a measure of goodness of prediction, with smaller values being better – this is more useful when our model is compared to some baseline model or when there is a comparison of several models. It is often helpful to plot these summary measures as a function of space to identify if certain regions in space show better predictive performance. Note that equivalent temporal validation statistics, in obvious notation \\(V_1(t), V_2(t), V_3(t)\\), can be obtained by replacing the averages over the time points with averages over the spatial locations. These can then be evaluated analogously to the spatial versions, and plotted as time series to see if certain time periods show better performance than others.\n\n\n\n\n\n\nTip\n\n\n\nSeveral R packages contain functionality for computing these simple validation statistics. However, these can be implemented directly by the user with a few lines of code using functions that take three arguments (the data, the predictions, and the prediction standard errors) as input. For example,\nV1 &lt;- function(z, p, pse) sum(z - p) / sqrt(sum(pse^2))\nimplements Equation 6.9. Our suggestion is to implement them once and keep them handy!\n\n\n\n\n6.3.3 Spatio-Temporal Cross-Validation Measures\nThe validation measures presented in Section 6.3.2 above are often used for within-sample validation, and thus they are naturally optimistic measures in the sense that the data are being used twice (once to train the model and once again to validate the model). As we have discussed in Section 6.1.3, it is much better to use a hold-out validation sample if possible, but such validation may be difficult to come by (or, in the case of spatio-temporal dependence, difficult to select). In that case, it is common to use cross-validation methods (recall Note 3.1) with your favorite validation measures (e.g., \\(MSPE\\), \\(MAPE\\), Equation 6.9–Equation 6.11 above or the scoring rules presented in Section 6.3.4). There have been a few examples in the literature of specific cross-validation statistics for spatio-temporal data, which we briefly describe here.\nAs a direct example in the case of leave-one-out-cross-validation (LOOCV), one might extend the notion of cross-validation residuals given in Equation 6.6 (e.g., Kang et al., 2009) to\n\\[\n\\left\\{\\frac{Z(\\mathbf{s}_i;t_j) - E(Z(\\mathbf{s}_i;t_j) | \\mathbf{Z}^{(-i,-t_j)})}{\\{\\textrm{var}(Z(\\mathbf{s}_i;t_j) | \\mathbf{Z}^{(-i,-t_j)})\\}^{1/2}}  \\right\\},\n\\]\nwhere \\(\\mathbf{Z}^{(-i,-t_j)}\\) corresponds to the data with observation \\(Z(\\mathbf{s}_i;t_j)\\) removed. These residuals can be explored for outliers and potential spatio-temporal dependence (as described in Section 6.2.1 above). Similarly, we can consider predictive cross-validation (PCV) and standardized cross-validation (SCV) measures (e.g., Kang et al., 2009),\n\\[\nPCV \\equiv \\left(\\frac{1}{mT}\\right) \\sum_{j=1}^T \\sum_{i=1}^m \\{Z(\\mathbf{s}_i;t_j) - E(Z(\\mathbf{s}_i;t_j) | \\mathbf{Z}^{(-i,-t_j)})\\}^2\n\\tag{6.12}\\]\nand\n\\[\nSCV \\equiv \\left(\\frac{1}{mT}\\right)  \\sum_{j=1}^T \\sum_{i=1}^m  \\frac{\\{Z(\\mathbf{s}_i;t_j) - E(Z(\\mathbf{s}_i;t_j) | \\mathbf{Z}^{(-i,-t_j)})\\}^2}{\\textrm{var}(Z(\\mathbf{s}_i;t_j) | \\mathbf{Z}^{(-i,-t_j)})  }.\n\\tag{6.13}\\]\nNote the similarity between Equation 6.11 and Equation 6.12, and between Equation 6.10 and Equation 6.13. If our model is performing well, we would like to see values of PCV near 0 and values of SCV close to 1. Of course, these evaluation criteria can be considered from a \\(K\\)-fold cross-validation perspective as well.\n\n\n6.3.4 Scoring Rules\nOne of the benefits of the statistical methods presented in Chapters 4 and 5 is that they give probabilistic predictions – that is, we do not just get a single prediction but, rather, a predictive distribution. This is a good thing as it allows us to account for various sources of uncertainty in our predictions. However, it presents a bit of a problem in that ideally we want to verify a distributional prediction but we have just one set of observations. We need to find a way to compare a distribution of predictions to a single realized (validation) observation. Formally, this can be done through the notion of a score, where the predictive distribution, say \\(p(z)\\), is compared to the validation value, say \\(Z\\), with the score function \\(S(p(z),Z)\\). There is a long history in probabilistic forecast “verification,” originating in the meteorology community, of favoring scoring functions that are proper; see Note 6.2 for a description of proper scoring rules and Gneiting & Raftery (2007) for technical details.\nIntuitively, proper scoring rules are expressed in such a way that a forecaster receives the best score (on average) if their forecast distribution aligns with their true beliefs. This relates to the notion of “forecast consistency” discussed in Murphy (1993), which concerns how closely the forecaster’s prediction matches up with their judgement. The point here is that there may be incentives for a forecaster to hedge their forecast away from their true beliefs, and this should be discouraged. For example, Carvalho (2016) and Nakazono (2013) describe a situation where an expert with an established reputation might tend to report a forecast closer to the consensus of a particular group, whereas a forecaster who is just starting out might seek to increase her reputation by overstating the probabilities of particular outcomes that she thinks might be understated in the consensus. Proper scoring rules are designed such that there is no reward for this type of hedging.\nThree common, and related, (strictly) proper scoring rules used in spatial and spatio-temporal prediction are the Brier score (BRS), the ranked probability score (RPS), and the continuous ranked probability score (CRPS). The BRS can be used to compare probability predictions for categorical variables. It is most often used when the outcomes are binary, \\(\\{0,1\\}\\), events. Assuming \\(Z\\) is a binary observation and \\(p=\\Pr(Z=1 | \\mbox{data})\\) comes from the model, the BRS is defined as\n\\[\nBRS(p,Z) = (Z - p)^2,\n\\tag{6.14}\\]\nwhere, as in golf, small scores are good. (Note that in this section, where possible, we omit the space and time labels for notational simplicity and just present the rules in terms of arbitrary predictive distributions and observations.) In practice, we calculate the average BRS for a number of predictions and associated observations in the validation data set. The BRS can be decomposed into components associated with prediction “reliability, resolution, and uncertainty” (see, for example Wilks (2011), Chapter 8). Note that there are several other skill scores that could also be used for binary responses (e.g., the Heidke skill score, Peirce skill score, Clayton skill score, and Gilbert skill score) that are based on comparing components of a \\(2 \\times 2\\) contingency table (see Wilks, 2011, Chapter 8).\nSome of the scoring rules used for binary data can be extended to multi-category predictions, although in the case of ordinal data one should take into account the relative “distance” (spread or dispersion) between categories (see, for example, the Gandin–Murphy skill score and Gerrity skill score described in Wilks (2011), Chapter 8). The ranked probability score is a multi-category extension to the BRS given by\n\\[\nRPS(p,Z) = \\frac{1}{J-1} \\sum_{i=1}^J \\left(\\sum_{j=1}^i Z_j - \\sum_{j=1}^i p_j\\right)^2,\n\\tag{6.15}\\]\nwhere \\(J\\) is the number of outcome categories, \\(p_j\\) is the predicted probability of the \\(j\\)th category, and \\(Z_j=1\\) if the category occurred, and \\(Z_j = 0\\) otherwise. Note that Equation 6.15 depends on an ordering of the categories and, when \\(J=2\\), we recover the Brier score (Equation 6.14). A perfect prediction leads to the case where \\(RPS = 0\\) and the worst possible score is \\(RPS = 1\\). RPS is strictly proper and accounts for the distance between groups, which is important for ordinal data. As with the BRS, in practice we typically calculate the average RPS for a number of predictions at different spatio-temporal locations in the validation data set.\n\n\n\n\n\n\nFigure 6.7: Left: The cumulative distribution function, \\(F(x)\\), of a prediction with mean 6.5 and prediction standard error 1. The observation is \\(Z = 6\\), and the shaded area denotes the difference between the cumulative distribution function of the observation (a step function) and the predictive distribution. Right: The integrand used to compute the CRPS, the area under the curve.\n\n\n\nA natural extension of the RPS to the case of a continuous response occurs if we imagine that we bin the continuous response into \\(J\\) ordered categories and let \\(J \\rightarrow \\infty\\). The continuous ranked probability score has become one of the more popular proper scoring rules in spatio-temporal statistics. It is formulated in terms of the predictive cumulative distribution function (cdf), say \\(F(z)\\), and is given by\n\\[\nCRPS(F,Z) = \\int (\\mathbb{1}\\{Z \\leq x\\} - F(x))^2 \\textrm{d}x,\n\\]\nwhere \\(\\mathbb{1}\\{Z \\leq x\\}\\) is an indicator variable that takes the value 1 if \\(Z \\leq x\\), and the value 0 otherwise. An illustration of the procedure by which the CRPS is evaluated is shown in Figure 6.7, for an observation \\(Z = 6\\) and \\(F(x)\\) the normal distribution function with mean 6.5 and standard deviation 1. In this example, \\(CRPS = 0.331\\).\nIn the case where the cdf \\(F\\) has a finite first moment, the CRPS can be written as\n\\[\nCRPS(F,Z) = E_F | z - Z | - \\frac{1}{2} E_F |z - z'|,\n\\tag{6.16}\\]\nwhere \\(z\\) and \\(z'\\) are independent random variables with distribution function \\(F\\) (e.g., Gneiting & Raftery (2007)). Thus, analytical forms for the CRPS can be derived for many standard predictive cumulative distribution functions, and hence for these functions it can be computed efficiently (see, for example, the scoringRules R package). However, the CRPS can be difficult to compute for complex predictive distributions such as one might get from a BHM. In such situations, one can approximate the CRPS by using an empirical predictive cdf.\nFor example, given samples of predictions, \\(Z_1,\\ldots,Z_m\\), from \\(F\\), one can show that (e.g., Jordan et al., 2017)\n\\[\nCRPS(\\widehat{F}_m,Z) = \\frac{1}{m} \\sum_{i=1}^m | Z_i - Z | - \\frac{1}{2 m^2} \\sum_{i=1}^m \\sum_{j=1}^m | Z_i - Z_j|,\n\\tag{6.17}\\]\nwhere the empirical cdf,\n\\[\n\\widehat{F}_m(x) = \\frac{1}{m} \\sum_{i=1}^m \\mathbb{1}\\{Z_i \\leq x\\},\n\\tag{6.18}\\]\nis substituted into Equation 6.16. More efficient computational approaches can be used to estimate Equation 6.16, as discussed in Jordan et al. (2017). Note that Equation 6.18 implicitly assumes that the \\(\\{Z_i\\}\\) are \\(iid\\), which is a reasonable assumption when one has multiple predictions (widely separated in time) for a given location. However, the \\(iid\\) assumption is not typically realistic for spatio-temporal validation data sets with multiple observations (see the discussion below on multivariate scoring rules for an alternative).\nIn the common case where one is only interested in evaluating the predictive distribution through its first two central moments, say \\(\\mu_F\\) and \\(\\sigma^2_F\\), Gneiting & Katzfuss (2014) suggest considering the Dawid–Sebastiani score (DSS),\n\\[\nDSS(F,Z) =  \\frac{(Z - \\mu_F)^2}{\\sigma^2_F} + 2 \\log \\sigma_F,\n\\tag{6.19}\\]\nwhich is a proper scoring rule and is simple to compute. In the case of a Gaussian predictive density function \\(f(z)\\), it can be shown that the DSS in Equation 6.19 is equivalent to the so-called logarithmic score (LS),\n\\[\nLS(F,Z) =  - \\log f(Z),\n\\tag{6.20}\\]\nwhere \\(f\\) is the density function associated with \\(F\\). This is one of the most-used proper scoring rules in machine learning. Note that sometimes the LS is defined without the negative sign (i.e., \\(\\log f(Z)\\)), in which case a larger score is better. We prefer to define it as in Equation 6.20 so that a smaller score is better, and as we show below in Section 6.4, this form of the LS is often used when comparing models.\nIt can be quite useful to consider the skill (\\({\\cal S}\\)) of a predictive model, which we define here as the average of the scoring rule over a range of prediction cases. For pairs \\(\\{(F_i, Z_i ): i=1,\\ldots,N\\}\\), the skill is given by\n\\[\n{\\cal S} = \\frac{1}{N} \\sum_{i=1}^N S(F_i, Z_i),\n\\tag{6.21}\\]\nwhere \\(S\\) is a generic score function. We can use a skill score (\\({\\cal SS}\\)) to compare predictions from models to some reference prediction method. For example,\n\\[\n{\\cal SS}_{\\cal M} =  \\frac{{\\cal S}_{\\cal M} - {\\cal S}_{\\mathrm{ref}}}{{\\cal S}_{\\mathrm{opt}} - {\\cal S}_{\\mathrm{ref}}},\n\\tag{6.22}\\]\nwhere \\({\\cal S}_{\\cal M}\\), \\({\\cal S}_{\\mathrm{ref}}\\), and \\({\\cal S}_{\\mathrm{opt}}\\) represent the skill of the model \\({\\cal M}\\), the reference method, and a hypothetical optimal predictor, respectively. The skill score (Equation 6.22) takes a maximum value of \\(1\\) when the model \\({\\cal M}\\) prediction is optimal, a value of \\(0\\) when the model \\({\\cal M}\\) has skill equivalent to the reference method, and a value less than \\(0\\) when the model \\({\\cal M}\\) has lower skill than the reference method. As noted by Gneiting & Raftery (2007), \\({\\cal SS}_{\\cal M}\\) is not proper in general, even if the scoring rule used in its construction is proper.\n\n\n\n\n\n\nTip\n\n\n\nFunctions to compute the Brier score, the ranked probability score, the continuous ranked probability score, and the logarithmic score can be found in the R package verification.\n\n\n\nMultivariate Scoring Rules\nThe scoring rules given above are univariate quantities that can be averaged or more generally summarized across time and space in our setting. Although less common, there are scoring rules that explicitly account for the multivariate nature of a multivariate prediction, which can be important when there are dependencies in the process model (between variables in space or time). This addresses the \\(iid\\) caveat we put on the CRPS calculation in Equation 6.17 and Equation 6.18, and it applies also to the skill defined by Equation 6.21. For example, the scoringRules R package implements the energy score (ES) discussed in Gneiting & Raftery (2007), which is given by\n\\[\nES(F,\\mathbf{Z}) = E_F ||\\mathbf{z}- \\mathbf{Z}|| - \\frac{1}{2} E_F || \\mathbf{z}- \\mathbf{z}'||,\n\\tag{6.23}\\]\nwhere, say, \\(\\mathbf{Z}' = (Z(\\mathbf{s}_i;t_j)\\!\\!:\\!i=1,\\ldots,m; j=1,\\ldots,T)\\), \\(|| \\cdot ||\\) represents the Euclidean norm, and \\(\\mathbf{z}\\) and \\(\\mathbf{z}'\\) are independent random vectors with multivariate cdf \\(F\\). Notice from comparison to Equation 6.16 that Equation 6.23 is a multivariate extension of the CRPS. Scheuerer & Hamill (2015) state that numerous studies have shown that a good performance of this score function requires a correct specification of the dependence structure in the model. When only the first and second moments are of interest, an alternative is to consider the multivariate version of the DSS given by Equation 6.19, which we define as\n\\[\nDSS_{mv}(F,\\mathbf{Z}) =  \\log |\\mathbf{C}_F| + (\\mathbf{Z}- \\boldsymbol{\\mu}_F)' \\mathbf{C}_F^{-1} (\\mathbf{Z}- \\boldsymbol{\\mu}_F),\n\\tag{6.24}\\]\nwhere \\(\\boldsymbol{\\mu}_F = E(\\mathbf{Z}| \\mbox{data})\\) and \\(\\mathbf{C}_F = \\textrm{var}(\\mathbf{Z}| \\mbox{data})\\) are the mean vector and covariance matrix of the multivariate predictive cdf \\(F\\).\nScheuerer & Hamill (2015) note that variograms (which, as we discuss in Chapter 4, account for spatial and spatio-temporal dependence) consider the expected squared difference between observations, and they generalized this to define a multivariate score that they call the variogram score of order p (\\(VS_p\\)). This can be written as\n\\[\nVS_p(F,\\mathbf{Z}) = \\sum_{i=1}^{mT} \\sum_{j=1}^{mT} w_{ij} (|Z_i - Z_j|^p - E_F |z_i - z_j|^p)^2,\n\\]\nwhere \\(w_{ij}\\) are non-negative weights, and \\(z_i\\) and \\(z_j\\) are the \\(i\\)th and \\(j\\)th elements of a random vector, \\(\\mathbf{z}\\), from the multivariate cdf, \\(F\\), and for ease of notation we write the data vector as \\(\\mathbf{Z}= (Z_1,\\ldots,Z_{mT})'\\). The weights can be used to de-emphasize certain difference pairs (e.g., those that are farther apart) and \\(p=2\\) corresponds to the variogram defined in Chapter 4. In Lab 6.1, we illustrate the use of the ES and \\(VS_p\\).\n\n\n\n\n\n\nNote 6.2: Proper Scoring Rules\n\n\n\nThis note follows the very intuitive description found in Bröcker & Smith (2007). Let \\(p(z)\\) be a probability distribution of predictions of \\(Z\\), which we wish to compare to an observation \\(Z\\) with cdf \\(F\\) (i.e., we wish to validate our predictive model). Let a score be some comparison measure between the predictive distribution and the observed value, denoted \\(S(p, Z)\\). Typically, scores are defined so that smaller scores indicate better predictions. The score \\(S\\) is said to be proper if\n\\[\nE_{F} \\{S(p,Z)\\}  \\geq  E_F\\{S(q,Z)\\}\n\\tag{6.25}\\]\nfor any two predictive distributions, \\(p(z)\\) and \\(q(z)\\), where \\(q(z)\\) is the “true” predictive distribution. That is, Equation 6.25 says that the expected score is minimized when the predictive distribution coincides exactly with the true predictive distribution. The scoring rule is strictly proper if this minimum in the expected score occurs only when \\(p(z) = q(z)\\) for all \\(z\\), that is, when the predictive distribution is the same as the true distribution. The concept of propriety is very intuitive in that it formalizes the notion that if our predictive distribution coincided with the true distribution, \\(q(z)\\), then it should be at least as good as some other forecast distribution, \\(p(z)\\), not equal to \\(q(z)\\).\n\n\n\n\n\n6.3.5 Field Comparison\nA special case of validation concerns comparing spatial or spatio-temporal “fields.” The idea of field comparison is to compare two or more spatial or spatio-temporal fields (typically gridded observations and/or model output, but note that they do not need to be gridded), in some sense, to decide if they are “different.” This has been of interest for quite some time in the geophysical sciences such as meteorology, where data and processes are naturally dependent in space and time. As an example, assume we have a model that provides short-term predictions (i.e., nowcasts) of precipitation, and we wish to validate our model’s predictions with weather radar data by comparing the two fields. Field comparison can also be used for inference where we would like to formally test whether two spatial fields are significantly different. Many of the validation summaries and scoring rules discussed above can be used in this context, although rigorous statistical inference has proved challenging. For example, the MSPE, MAPE, RMSPE, and ACC measures are often used for field comparison. Further, some specialized summaries have been designed to compare spatial (and, in principle, spatio-temporal) features of the process and data in these comparisons, and we discuss a few of these below.\n\nField-Matching Methods\nOne of the biggest challenges in comparing spatial fields is to decide how well features match up. For example, in the context of the aforementioned radar-nowcasting problem, the goal might be to predict a feature (say, a storm cell) that is present in the observed radar data, but the prediction might be shifted in space relative to the observations. Is such a prediction better than if the prediction of the feature is not shifted, but covers an overly broad area compared to the observed feature? Another issue is that the two fields may agree at some spatial scales of resolution, but not at others. One of the primary challenges in field comparison is to account for differences in feature location, orientation, and scale.\nWhen comparing two spatial fields of discrete outcomes, particularly in the context of validating a predictive model, we can adapt many of the score functions to the spatial case, beyond the simple averaging in a score function, where we try to account for the different ways that spatial fields may match up. One of the most famous is the threat score (TS) (also known as the critical success index). The TS is a simple summary that was originally designed for \\(2 \\times 2\\) contingency tables. That is, it is the ratio of the number of successful predictions of an event divided by the number of situations where that event was predicted or observed, so notice that the number of correct predictions of the non-event is not considered. In the context of field comparison, consider\n\\[\nTS = \\frac{A_{11}}{A_{11} + A_{10} + A_{01}},\n\\tag{6.26}\\]\nwhere \\(A_{11}\\) is the area associated with the intersection of the region where the predicted event was expected to occur with the region where it did occur, \\(A_{10}\\) is the area where the event was predicted to occur but did not occur, and \\(A_{01}\\) is the area where the event occurred but was not predicted to occur.\nFor illustration, we consider the example in Lab 6.1 (Sydney radar data set), where we leave out data in the 10-minute periods at 09:35 and 09:45, and then we predict the reflectivities at these time points using both an IDE model and an FRK model with spatio-temporal basis functions. When using the TS, we first need to identify the presence, or otherwise, of an event, and we do this by setting a threshold parameter: an observation or prediction greater than this threshold is classified as an event, while an observation or prediction less than this threshold is classified as a non-event (in practice, we often compare across multiple threshold values). Figure 6.8 shows the events and non-events in the data and in the predictions from the two models at 09:35, for a threshold of 25 dBZ. Clearly, the IDE model has been more successful in capturing “events” in this instance. The TSs for both models for thresholds varying between 15 dBZ and 25 dBZ are given in Table Table 6.1: we see that the IDE model outperforms the FRK models for all thresholds using this field-matching diagnostic. Of course, kriging is not designed to predict events above a threshold Zhang et al. (2008), but neither is IDE prediction. Incorporating the dynamics appears to carry extra advantages!\n\n\n\n\n\n\nTip\n\n\n\nCheck out the SpatialVx package for a comprehensive suite of field-matching methods. In this example, we used the vxstats function from SpatialVx to obtain the threat scores; this function also returns other useful diagnostics, such as the probability of event detection and the false-alarm rate.\n\n\n\n\n\n\n\n\nFigure 6.8: Plots showing the presence or absence of events at 09:35, obtained by thresholding the observations (left) or the IDE/FRK predictions (center and right) at 25 dBZ.\n\n\n\n\n\n\nTable 6.1: Threat scores (TS) calculated using Equation 6.26 for both the IDE predictions and the FRK predictions at 09:35 for different thresholds.\n\n\n\n\n\nThreshold (dBZ)\nTS for IDE\nTS for FRK\n\n\n\n\n15.00\n0.73\n0.33\n\n\n20.00\n0.56\n0.20\n\n\n25.00\n0.39\n0.11\n\n\n\n\n\n\nField-matching approaches have attempted to deal with questions of scale decompositions and feature properties (location, orientation, phase, amplitude), and a summary of such methods from a geophysical perspective can be found in Brown et al. (2012) and Gilleland et al. (2010). A brief summary of field matching from a statistical perspective can be found in Cressie & Wikle (2011), Section 5.7. In addition to using the MSPE, ACC, and score functions, methods based on scale decomposition such as EOF-based diagnostics (Branstator et al., 1993) and wavelet decompositions (Briggs & Levine, 1997) have been used successfully for field matching. In these cases, the usual measures are applied to the various scale components rather than to the full field. Examples of feature-based methods include the location-error matching approach of Ebert & McBride (2000) and the morphometric decomposition into scale, location, rotation angle, and intensity differences presented in Micheas et al. (2007).\n\n\nField Significance\nIt has long been of interest in the geophysical sciences to ask whether the differences in two spatial fields (or a collection of such fields) are significantly different. These two spatial fields may correspond to predictions or observations. For example, is the average maximum temperature on a grid over North America for the decade 2001–2010 significantly different from the corresponding average for the decade 1971–1980? One could consider simple pointwise two-sample \\(t\\) tests for the null hypothesis of mean differences equal to zero at each grid cell. Then a Bonferroni correction of the level of significance, obtained by dividing the desired level by the number of grid cells, could be applied to deal with the multiple testing. However, such a correction leads to an overall test with very low power. Alternatively, one could look at a map of corresponding \\(p\\)-values and qualitatively try to identify regions in which a significant difference is present, which can be effective but lacks rigor.\nHowever, there is not only dependence in time that must be accounted for in any test that considers a sequence of fields (e.g., the effective degrees of freedom would likely be less than the number of time replicates in the presence of positive temporal dependence), but one must also account for the spatial dependence between nearby tests when doing multiple \\(t\\) tests. Historical approaches have attempted to deal with these issues through effective-degrees-of-freedom modifications and Monte Carlo testing (see, for example, Livezey & Chen (1983); Stanford & Ziemke (1994); Von Storch & Zwiers (2002)). More recently, expanding on the famous false discovery rate (FDR) multiplicity mitigation approach of Benjamini & Hochberg (1995), Shen et al. (2002) developed the so-called enhanced FDR (EFDR) approach for spatial field comparison that uses the FDR methodology on a wavelet-based scale decomposition of the spatial fields (which deals with the spatial dependence by carrying out the testing on the decorrelated wavelet coefficients).\nAs an illustration, consider the difference between the mean SST anomalies in the 1970s and in the 1990s for an area of the Pacific Ocean, as shown in the left panel of Figure 6.9. Visually, it seems clear that the mean SST anomaly in the 1990s was higher than that of the 1970s. However, to check which areas are significantly different, we can run the EFDR procedure on this field of differences and then plot the field corresponding to the wavelets whose coefficients are deemed to be significantly different from zero (at the 5% level). The resulting “field significance” map, shown in the right panel of Figure 6.9, highlights the regions that were significantly warmer or cooler in the 1990s. This procedure was implemented using the EFDR R package.\n\n\n\n\n\n\nFigure 6.9: Left: Difference between the average SST anomalies in the 1990s and the average SST anomalies in the 1970s. Right: The field significance map of SST anomaly differences that were found to be significantly different from zero at the 5% level. The plot is based on the EFDR procedure and was obtained using the package EFDR.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Evaluating Spatio-Temporal Statistical Models</span>"
    ]
  },
  {
    "objectID": "Chapter6.html#sec-ModSelect",
    "href": "Chapter6.html#sec-ModSelect",
    "title": "6  Evaluating Spatio-Temporal Statistical Models",
    "section": "6.4 Model Selection",
    "text": "6.4 Model Selection\nIt is often the case that diagnostic analysis of a model suggests that we consider an alternative model, or that we should use fewer covariates in our regression model. This section is concerned with the question of how to decide which model out of a group of models, say \\(\\{{\\cal M}_1,\\ldots, {\\cal M}_L\\}\\), is in some sense the “best.” We shall assume that all of the models under consideration are reasonable from a scientific perspective, and so the choice is not obvious. First, we note that any of the summaries or score functions discussed above could be used to compare models, for example, using the skill score (Equation 6.22). In this section, we focus on more traditional statistical-model-selection approaches, although our presentation is brief. Interested readers can find more details in the excellent overviews of model comparison presented in Gelman et al. (2014), Hooten & Hobbs (2015), and the references therein.\n\n6.4.1 Model Averaging\nFrom a predictive perspective, it may be the case that one obtains better predictions by averaging over several models, rather than focusing on a single model. The formal methodology for doing this is through Bayesian model averaging, which provides a probabilistically consistent mechanism for combining posterior distributions (see Hoeting et al., 1999 for an extensive overview). Our presentation follows the concise summary in Hooten & Hobbs (2015).\nSuppose we are interested in some vector quantity, \\(\\mathbf{g}\\), which can be parameters or predictions of the process or the data, and suppose we have observations, \\(\\mathbf{Z}\\), that were used to train the model. Then, for \\(\\ell \\in \\{1,\\ldots,L\\}\\), we can write\n\\[\n[\\mathbf{g}| \\mathbf{Z}] = \\sum_{\\ell=1}^{L} [\\mathbf{g}| \\mathbf{Z}, {\\cal M}_\\ell] P({\\cal M}_\\ell | \\mathbf{Z}),\n\\]\nwhere \\([\\mathbf{g}| \\mathbf{Z}, {\\cal M}_\\ell]\\) is the posterior distribution of \\({\\mathbf{g}}\\) given the data and the model \\({\\cal M}_\\ell\\); and \\(P({\\cal M}_\\ell | \\mathbf{Z})\\) is the posterior probability of the model \\({\\cal M}_\\ell\\) which, given the data, gives the importance of model \\({\\cal M}_\\ell\\) among the collection of models. We can obtain the latter distribution from\n\\[\nP({\\cal M}_\\ell | \\mathbf{Z}) = \\frac{[\\mathbf{Z}| {\\cal M}_\\ell] P({\\cal M}_\\ell)}{\\sum_{j=1}^{L} [\\mathbf{Z}| {\\cal M}_j] P({\\cal M}_j)},\n\\tag{6.27}\\]\nwhere the prior probabilities for the models, \\(\\{P({\\cal M}_j): j=1,\\ldots,L\\}\\), have been provided. Often, all the models are assumed equally likely with a priori probability \\(1/L\\), but this need not be the case. In Equation 6.27, we also require the marginal data distribution for each model (often called the integrated likelihood), \\([\\mathbf{Z}| {\\cal M}_\\ell]\\), which is simply the factor in the denominator in Bayes’ rule when one is obtaining the posterior distribution under model \\({\\cal M}_\\ell\\). That is,\n\\[\n[\\mathbf{Z}| {\\cal M}_\\ell] = \\iint [\\mathbf{Z}|  \\mathbf{Y}, \\boldsymbol{\\theta}, {\\cal M}_\\ell][\\mathbf{Y}| \\boldsymbol{\\theta}, {\\cal M}_\\ell] [\\boldsymbol{\\theta}| {\\cal M}_\\ell] \\textrm{d}\\mathbf{Y}\\textrm{d}\\boldsymbol{\\theta},\n\\tag{6.28}\\]\nwhere \\([\\mathbf{Z}| \\mathbf{Y}, \\boldsymbol{\\theta}, {\\cal M}_\\ell]\\) is the data model (likelihood) under model \\({\\cal M}_\\ell\\); and \\([\\mathbf{Y}| \\boldsymbol{\\theta}, {\\cal M}_\\ell]\\) and \\([\\boldsymbol{\\theta}| {\\cal M}_\\ell]\\) are the process and prior distributions, respectively, under model \\({\\cal M}_\\ell\\). Unfortunately, Equation 6.28 is typically intractable in BHM settings and cannot be calculated directly. This makes Bayesian model averaging difficult to implement for complex models, although there are various computational approaches used to obtain integrated likelihoods in this setting and in the context of Bayes factors described in Section 6.4.2 (see, for example, Congdon (2006)).\n\n\n6.4.2 Model Comparison via Bayes Factors\nThe posterior probability for a given model expressed in Equation 6.27 suggests a way to compare models. In particular, we note that the ratio of two such posteriors (the posterior odds) can be written as\n\\[\n\\frac{p({\\cal M}_\\ell | \\mathbf{Z})}{p({\\cal M}_k | \\mathbf{Z})} = \\frac{[\\mathbf{Z}| {\\cal M}_\\ell]P({\\cal M}_\\ell)}{[\\mathbf{Z}| {\\cal M}_k]P({\\cal M}_k)} \\equiv B_{\\ell,k}(\\mathbf{Z}) \\frac{P({\\cal M}_\\ell)}{P({\\cal M}_k)},\n\\]\nwhere the ratio of the integrated likelihoods, \\(B_{\\ell,k}(\\mathbf{Z})\\), is known as the Bayes factor. It is a constant multiplier (that depends on the data) applied to the prior odds of model \\({\\cal M}_\\ell\\) relative to model \\({\\cal M}_k\\). So, the larger \\(B_{\\ell,k}(\\mathbf{Z})\\) is, the more support there is for model \\({\\cal M}_\\ell\\) relative to model \\({\\cal M}_k\\). Note that if we take the negative log of the Bayes factor, we obtain the difference of two logarithmic scores (recall Equation 6.20); using obvious notation,\n\\[\n- \\log B_{\\ell,k} = LS(F_\\ell;\\mathbf{Z}) - LS(F_k;\\mathbf{Z}).\n\\]\n\n\n6.4.3 Model Comparison via Validation\nWe can always compare models based on the validation measure that we think is most appropriate for our problem. In this sense, any of the validation measures discussed above might be considered. In spatio-temporal statistics, we most often use a measure of predictive accuracy and typically use an out-of-sample validation or, at least, some type of cross-validation (e.g., using the MSPE or a proper scoring rule as a way to compare models). The logarithmic scoring rule Equation 6.20 is often used in this context. Note that the log predictive density is given by \\(\\log [\\mathbf{Z}_p | \\mathbf{Z}]\\), where \\(\\mathbf{Z}_p\\) corresponds to spatio-temporal data that we would like to predict with our model, given data \\(\\mathbf{Z}\\) that were used to train the model. In the context of model selection, we should explicitly denote the model under which this predictive distribution was obtained, namely, \\(\\log [\\mathbf{Z}_p | \\mathbf{Z}, {\\cal M}_\\ell]\\).\nAs stated previously, when the predictive distribution is Gaussian (which is often assumed in S-T kriging models), it is described by the predictive means, variances, and covariances. Then the negative log predictive density is the \\(LS_{mv}\\) score, which is just the \\(DSS_{mv}\\) score as we defined it in Equation 6.24. More generally, in a BHM context, we can obtain the logarithmic score by averaging over \\(j=1,\\ldots,N\\) MCMC samples from the predictive distribution. That is, up to Monte Carlo error, the log score based on the predictive distribution \\([\\mathbf{Z}_p | \\mathbf{Z}]\\) can be obtained as follows:\n\\[\nLS_{p,\\ell} = - \\log\\left(\\frac{1}{N} \\sum_{j=1}^N [\\mathbf{Z}_p | \\mathbf{Z}, \\mathbf{Y}^{(j)}, \\boldsymbol{\\theta}^{(j)}, {\\cal M}_\\ell]  \\right),\\quad \\ell = 1,\\ldots,L,\n\\tag{6.29}\\]\nwhere \\(\\mathbf{Y}^{(j)}\\) and \\(\\boldsymbol{\\theta}^{(j)}\\) correspond to the \\(j\\)th MCMC sample of the process and parameter components in the \\(\\ell\\)th model. Thus, we can compute Equation 6.29 for multiple models, \\(\\ell = 1,\\ldots,L\\), and use this to select the “best” model(s); with our definition of \\(LS\\), we prefer models with smaller values of \\(LS_{p,\\ell}\\).\nAs discussed above in Section 6.3.3, we often do not have a hold-out sample to use for validation, so we turn to cross-validation. For example, the \\(K\\)-fold cross-validation estimate of the LS based on the predictive distribution \\([\\mathbf{Z}_k | \\mathbf{Z}^{(-k)}]\\) is (up to Monte Carlo error)\n\\[\nLS_{cv,\\ell} = - \\frac{1}{K} \\sum_{k=1}^K \\log\\left( \\frac{1}{N} \\sum_{j=1}^N [\\mathbf{Z}_k | \\mathbf{Z}^{(-k)},  \\mathbf{Y}^{(j)}, \\boldsymbol{\\theta}^{(j)}, {\\cal M}_\\ell] \\right),\\quad \\ell=1,\\ldots,L,\n\\]\nwhere \\(\\mathbf{Z}_k\\) corresponds to the components of \\(\\mathbf{Z}\\) in the \\(k\\)th hold-out sample. The challenge for many spatio-temporal BHMs is that it can be expensive to perform \\(K\\)-fold cross-validation in the Bayesian setting, since the model has to be fitted \\(K\\) times. As an alternative, we can evaluate the log predictive distribution using data from our training sample and then attempt to correct for the bias associated with using the training sample for both model-parameter estimation and prediction evaluation. The common bias correction methods are often labeled information criteria and are discussed briefly in the next subsection.\n\n\n6.4.4 Information Criteria\nInformation criteria work in much the same spirit as regularization approaches; that is, they represent a trade-off between bias and variance in the sense that they penalize the bias due to overfitting that can occur when models are evaluated on the same data that were used to train them. This penalty controls for model complexity and favors models that are more parsimonious (see, for example, Hooten & Hobbs (2015)).\nPerhaps the most famous of the information criteria is the Akaike information criterion (AIC). In this case, the parameters, \\(\\boldsymbol{\\theta}\\), are assumed to be estimated using ML estimation, and the AIC can be defined as\n\\[\nAIC({\\cal M}_\\ell) \\equiv -2 \\log [\\mathbf{Z}| \\widehat{\\boldsymbol{\\theta}}, {\\cal M}_\\ell] + 2p_\\ell,\n\\tag{6.30}\\]\nwhere notice that \\(- \\log [\\mathbf{Z}| \\widehat{\\boldsymbol{\\theta}}, {\\cal M}_\\ell]\\) is the LS for model \\({\\cal M}_\\ell\\), and parameter estimates \\(\\widehat{\\boldsymbol{\\theta}}\\) are ML estimates under model \\({\\cal M}_\\ell\\) (having integrated out the hidden process \\(\\mathbf{Y}\\) to yield \\([\\mathbf{Z}| \\boldsymbol{\\theta}, {\\cal M}_\\ell]\\)). In Equation 6.30, \\(p_\\ell\\) is the number of parameters estimated in model \\({\\cal M}_\\ell\\) (after integrating out \\(\\mathbf{Y}\\)). Thus, the LS is penalized by the number of parameters in the model. When comparing two models, the model with the lower AIC is better, which, all other things being equal, favors more parsimonious models. Despite integrating out the process \\(\\mathbf{Y}\\), the AIC breaks down when one has random effects and dependence in the model \\({\\cal M}_\\ell\\), because the number of effective parameters is not equal to \\(p_\\ell\\). Although there are corrections to the AIC that attempt to deal with some of these issues, one must be careful using them in these settings (see, for example, the discussion in Hodges & Sargent (2001); Overholser & Xu (2014)). In addition, the AIC is not an appropriate criterion for model selection between different BHMs because it depends on ML estimates of parameters, and these parameters have a prior distribution on them. There is no mechanism that we know of to account for general prior distributions when using the AIC.\nAnother information criterion in common use is the Bayesian information criterion (BIC). The BIC is given by\n\\[\nBIC({\\cal M}_\\ell) = - 2 \\log [\\mathbf{Z}| \\widehat{\\boldsymbol{\\theta}}, {\\cal M}_\\ell] + \\log(m^*) p_\\ell,\n\\tag{6.31}\\]\nwhere \\(m^*\\) is the sample size (i.e., the number of spatio-temporal observations) and, as with the AIC, \\(\\widehat{\\boldsymbol{\\theta}}\\) is the ML estimate under \\({\\cal M}_\\ell\\) and \\(p_\\ell\\) is the number of parameters in the model (with the same caveats as in the AIC case). As with the AIC, we prefer models with smaller BIC values. Note that the BIC formula Equation 6.31 gives larger penalties than the AIC (when \\(m^* &gt; 7\\)) and so favors more parsimonious models than AIC. While it is referred to as a “Bayesian” information criterion, it is likewise not appropriate for model selection between different BHMs. Again the BIC relies on ML estimates of parameters and provides no way to adjust the penalty term to account for the effective number of parameters in models with random effects and dependence.\nTo account for the effective number of parameters in a BHM, Spiegelhalter et al. (2002) proposed the deviance information criterion (DIC), given by\n\\[\nDIC({\\cal M}_\\ell) = -2 \\log [\\mathbf{Z}|  E(\\boldsymbol{\\theta}| \\mathbf{Z}), {\\cal M}_\\ell] + 2 p_\\ell^D,\n\\tag{6.32}\\]\nwhere \\(E(\\boldsymbol{\\theta}| \\mathbf{Z})\\) is the posterior expectation of \\(\\boldsymbol{\\theta}\\) under model \\({\\cal M}_\\ell\\), and \\(p_\\ell^D\\) is the effective number of parameters, given by\n\\[\np_\\ell^D \\equiv \\overline{D}_\\ell - \\widehat{D}_\\ell.\n\\tag{6.33}\\]\nIn Equation 6.33, the estimated model deviance is \\(\\widehat{D}_\\ell = -2 \\log [\\mathbf{Z}| E(\\boldsymbol{\\theta}| \\mathbf{Z}), {\\cal M}_\\ell]\\) as in Equation 6.32, and \\(\\overline{D}_\\ell\\) is the posterior mean deviance, which is given by\n\\[\n\\overline{D}_\\ell = \\int - 2 \\left( \\log [\\mathbf{Z}| \\boldsymbol{\\theta}, {\\cal M}_\\ell]\\right) [\\boldsymbol{\\theta}| \\mathbf{Z}, {\\cal M}_\\ell] \\textrm{d}\\boldsymbol{\\theta}.\n\\]\nThe DIC is fairly simple to calculate in MCMC implementations of BHMs, but it has several well-known limitations, primarily related to the estimate of the effective number of parameters Equation 6.33 and the fact that it is not appropriate for mixture models (see the summary in Hooten & Hobbs (2015)). There are several alternative specifications in the literature that attempt to overcome these limitations.\nThe Watanabe–Akaike information criterion (WAIC) attempts to address some of the limitations of the DIC, and an elementwise (rather than multivariate) form can be written as\n\\[\n{W\\!AIC}({\\cal M}_\\ell) = -2 \\sum_{i=1}^{m^*} \\log  \\left( \\int [Z_i | \\boldsymbol{\\theta}, {\\cal M}_\\ell] [\\boldsymbol{\\theta}| \\mathbf{Z}, {\\cal M}_\\ell] \\textrm{d}\\boldsymbol{\\theta}\\right) + 2 p_\\ell^w,\n\\tag{6.34}\\]\nwhere the effective number of parameters in Equation 6.34 is given by\n\\[\np_{\\ell}^w = \\sum_{i=1}^{m^*} \\textrm{var}_{\\theta | Z} (\\log [Z_i | \\boldsymbol{\\theta}, {\\cal M}_\\ell]).\n\\tag{6.35}\\]\nThere are other ways to define the effective number of parameters in this setting, but Gelman et al. (2014) favor Equation 6.35 because it gives results more similar to LOOCV. Both components of the WAIC can be easily evaluated using the samples from MCMC implementations of BHMs (see, for example, Gelman et al. (2014); Hooten & Hobbs (2015)). The WAIC has several advantages over the DIC for BHM selection (it averages using the posterior predictive distribution of \\(\\boldsymbol{\\theta}\\) directly, rather than conditioning on a point estimate of the parameters; it has a more realistic effective-number-of-parameters penalty; and it is appropriate both for BHMs and Bayesian mixture models). However, we sound a warning note again in that the elementwise implementation of the WAIC may not be appropriate for dependent processes such as encountered in spatio-temporal modeling (see, for example, Gelman et al. (2014); Hooten & Hobbs (2015), for further discussion).\nHooten & Hobbs (2015) make the point that there is a similar model-selection approach that may be more appropriate for BHMs with dependent processes. In particular, consider a special case of the so-called posterior predictive loss (PPL) approach described by Laud & Ibrahim (1995) and Gelfand & Ghosh (1998). Define\n\\[\nPPL({\\cal M}_\\ell) = \\sum_{i=1}^{m^*} (Z_i - E(Z_i | \\mathbf{Z}, {\\cal M}_\\ell))^2 + \\sum_{i=1}^{m^*} \\textrm{var}(Z_i | \\mathbf{Z}, {\\cal M}_\\ell),\n\\tag{6.36}\\]\nwhere \\(E(Z_i | \\mathbf{Z}, {\\cal M}_\\ell)\\) and \\(\\textrm{var}(Z_i | \\mathbf{Z}, {\\cal M}_\\ell)\\) are the predictive mean and predictive variance, respectively, for the observation \\(Z_i\\). The PPL given by Equation 6.36 shares with the usual information criteria a first term corresponding to the quality of prediction and a second term penalizing models that are more complex.\n\n\n\n\n\n\nTip\n\n\n\nSeveral R packages used in this book contain functions that help compute or return information criteria from the fitted model. The functions AIC and BIC can be used to extract the Akaike and Bayesian information criteria, respectively, from the models discussed in Chapter 3 (linear models, generalized linear models, generalized additive models), and the function inla in the package INLA may be instructed to compute the deviance and Watanabe–Akaike information criteria. Other packages such as SpatioTemporal, FRK, and IDE contain functions to compute the log-likelihood from the fitted model, and then some of the information criteria above could be computed; see Lab 6.1.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Evaluating Spatio-Temporal Statistical Models</span>"
    ]
  },
  {
    "objectID": "Chapter6.html#chapter-6-wrap-up",
    "href": "Chapter6.html#chapter-6-wrap-up",
    "title": "6  Evaluating Spatio-Temporal Statistical Models",
    "section": "6.5 Chapter 6 Wrap-Up",
    "text": "6.5 Chapter 6 Wrap-Up\nThe evaluation of a model through model checking, validation, and selection is a very important step in the model-building process. That said, it is worth making the point here that in spatio-temporal modeling we often have a strong scientific motivation to consider a specific model (e.g., a particular survey design or a particular physical or biological process model). Cressie & Wikle (2011) (Chapter 1) and Ver Hoef & Boveng (2015) make the case that in these situations one should focus on building the best single model that is possible rather than carrying out model selection from several models or implementing multi-model inference. Indeed, as we have mentioned several times in this book, with observational data we never select the “true” model, but we can certainly build models that allow us to learn about or predict the spatio-temporal process. This notion of “iterating on a single model” (Ver Hoef & Boveng, 2015) may actually improve our ability to describe the real-world processes of interest, as it allows us to focus more on model checking (diagnostics) and model validation, which may suggest new features of the data about which we were unaware.\nThis chapter focused on model checking (Section 6.2), model validation (Section 6.3), and model selection (Section 6.4). We discussed how it is difficult to evaluate what we usually care about, the latent process, because we only have noisy and usually incompletely sampled versions of it. Although an OSSE can be used in some cases to evaluate the model with respect to the (simulated) true process of interest, we most often compare predictions obtained from our predictive distribution to various validation data. We typically favor validation data sets that are not used to train the model, and we can mimic such data through cross-validation. We mentioned how there is often a challenge in matching the validation sample with the prediction from our model, in terms of data support, although this was not a topic we covered in detail. We gave some possible spatio-temporal extensions of regression diagnostics and diagnostic plots that could be used for model checking, but we note that this is quite an under-developed area of spatio-temporal statistics.\nValidation is the area of model evaluation that has seen the most activity in the spatio-temporal literature, although most of these methods were not developed explicitly for spatio-temporal processes. We are in favor of using proper scoring rules as validation summaries, particularly those that account for the uncertainty included in the predictive distribution.\nModel selection is a vast topic, and we just touched on some of the basic approaches there. It is worth pointing out again that many of these methods are often not appropriate in fully Bayesian contexts, or when one has dependent random effects. In that sense, there is still a lot of work to be done in developing model-selection approaches for complex spatio-temporal models.\nFinally, as we have noted, spatio-temporal statistical models have primarily been used for the purpose of prediction. Disciplines such as meteorology, which have had to develop, improve, and justify predictive (forecast) models publicly on a daily basis for decades, have developed a broader terminology to consider the efficacy of predictive models. In particular, the late Alan Murphy was a pioneer in the formal study of predictive-model performance. In a classic paper, Murphy (1993) gave a list of nine “attributes” to consider when trying to describe the quality of a forecast: bias, association, accuracy, skill, reliability, resolution, sharpness, discrimination, and uncertainty. In general, his attributes describe three primary aspects of a good prediction: consistency, quality, and value. Consistency refers to how closely the prediction corresponds to the modeler’s prior beliefs or judgement, given his/her understanding of the process and the data; quality corresponds to how well the prediction agrees with observations; and value simply considers if the prediction actually contributes to beneficial decision-making (see the overview at http://www.cawcr.gov.au/projects/verification/). In statistics, we should consider these issues too, but our subject has primarily focused on bias and accuracy. These other issues are important, and this area offers a wonderful opportunity for researchers to build up this under-developed area in spatio-temporal statistics.\nAfter going through the following Lab, you are invited to go on to the epilogical chapter for some closing remarks about spatio-temporal statistics.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Evaluating Spatio-Temporal Statistical Models</span>"
    ]
  },
  {
    "objectID": "Chapter6.html#lab-6.1-spatio-temporal-model-validation",
    "href": "Chapter6.html#lab-6.1-spatio-temporal-model-validation",
    "title": "6  Evaluating Spatio-Temporal Statistical Models",
    "section": "Lab 6.1: Spatio-Temporal Model Validation",
    "text": "Lab 6.1: Spatio-Temporal Model Validation\nIn this Lab we consider the validation of two spatio-temporal models that are fitted to the same data set. To show the importance of modeling dynamics, we shall consider the Sydney radar data set and compare predictions obtained using the IDE model to those obtained using the FRK model (which does not incorporate dynamics). We shall carry out validation on data that are held out. The hold-out data set will comprise (i) a block of data spanning two time points, and (ii) a 10% random sample of the data at the other time points. We expect the IDE model to perform particularly well when validating the block of data spanning two points, where information on the dynamics is pivotal for “filling in” the temporal gaps.\nFor this Lab we use the IDE and FRK packages for modeling,\n\nlibrary(\"FRK\")\nlibrary(\"IDE\")\n\nthe scoringRules and verification packages for probabilistic validation,\n\nlibrary(\"scoringRules\")\nlibrary(\"verification\")\n\nand the usual packages for handling and plotting spatio-temporal data,\n\nlibrary(\"dplyr\")\nlibrary(\"ggplot2\")\nlibrary(\"sp\")\nlibrary(\"spacetime\")\nlibrary(\"STRbook\")\nlibrary(\"tidyr\")\n\n\nStep 1: Training and Validation Data\nFirst, we load the Sydney radar data set and create a new field timeHM that contains the time in an hours:minutes format.\n\ndata(\"radar_STIDF\", package = \"STRbook\")\nmtot &lt;- length(radar_STIDF)\nradar_STIDF$timeHM &lt;- format(time(radar_STIDF), \"%H:%M\")\n\nThe initial stage of model verification is to hold out data prior to fitting the model, so that these data can be compared to the predictions once the model is fitted on the retained data. As explained above, we first leave out data at two time points, namely 09:35 and 09:45, by finding the indices of the observations that were made at these times, and then removing them from the complete set of observation indices.\n\nvalblock_idx &lt;- which(radar_STIDF$timeHM %in% c(\"09:35\", \"09:45\"))\nobs_idx &lt;- setdiff(1:mtot, valblock_idx)\n\nWe next leave out 10% of the data at the other time points by randomly sampling 10% of the elements from the remaining observation indices.\n\nset.seed(1)\nvalrandom_idx &lt;- sample(obs_idx, \n                        0.1 * length(obs_idx), \n                        replace = FALSE) %&gt;% sort()\nobs_idx &lt;- setdiff(obs_idx, valrandom_idx)\n\nWe can now use the indices we have generated above to construct our training data set, a validation data set for the missing time points, and a validation data set corresponding to the data missing at random from the other time points.\n\nradar_obs &lt;- radar_STIDF[obs_idx, ]\nradar_valblock &lt;- radar_STIDF[valblock_idx, ]\nradar_valrandom &lt;- radar_STIDF[valrandom_idx, ]\n\n\n\nStep 2: Fitting the IDE Model\nIn Lab 5.2 we fitted the IDE model to the entire data set. Here, instead, we fit the IDE model to the training data set created above. As before, since this computation takes a long time, we can load the results directly from cache.\n\nIDEmodel &lt;- IDE(f = z ~ 1,\n                data = radar_obs,\n                dt = as.difftime(10, units = \"mins\"),\n                grid_size = 41)\n\nfit_results_radar2 &lt;- fit.IDE(IDEmodel,\n                              parallelType = 1)\n\n\ndata(\"IDE_Radar_results2\", package = \"STRbook\")\n\nIt is instructive to compare the estimated parameters from the full data set in Lab 5.2 to the estimated parameters from the training data set in this Lab. Reassuringly, we see that the intercept, the kernel parameters (which govern the system dynamics), as well as the variance parameters, have similar estimates.\n\n## load results with full data set\ndata(\"IDE_Radar_results\", package = \"STRbook\")\nwith(fit_results_radar$IDEmodel, c(get(\"betahat\")[1,1],\n                                   unlist(get(\"k\")),\n                                   get(\"sigma2_eps\"),\n                                   get(\"sigma2_eta\")))\n\n         par1   par2   par3   par4   par5   par6 \n 0.582  0.135  2.497 -5.488 -1.861 28.384  7.271 \n\nwith(fit_results_radar2$IDEmodel, c(get(\"betahat\")[1,1],\n                                   unlist(get(\"k\")),\n                                   get(\"sigma2_eps\"),\n                                   get(\"sigma2_eta\")))\n\n(Intercept)        par1        par2        par3        par4        par5 \n     0.4950      0.0926      3.6330     -5.2856     -1.8141     28.7125 \n       par6 \n     9.7691 \n\n\nPrediction proceeds with the function predict. Since we wish to predict at specific locations we now use the argument newdata to indicate where and when the predictions need to be made. In this case we supply newdata with the STIDF objects we constructed above.\n\npred_IDE_block &lt;- predict(fit_results_radar2$IDEmodel,\n                          newdata = radar_valblock)\npred_IDE_random &lt;- predict(fit_results_radar2$IDEmodel,\n                          newdata = radar_valrandom)\n\n\n\nStep 3: Fitting the FRK Model\nFor FRK we need to specify the spatial basis functions and temporal basis functions in order to construct the spatio-temporal basis functions. For the spatial basis functions we specify two resolutions of bisquare functions regularly distributed inside the domain.\n\nG_spatial &lt;- auto_basis(manifold = plane(),     # fns on plane\n                        data = radar_obs,       # project\n                        nres = 2,               # 2 res.\n                        type = \"bisquare\",      # bisquare.\n                        regular = 1)            # irregular\n\nType show_basis(G_spatial) to visualize the locations and apertures of these basis functions. For the temporal basis functions we regularly place five bisquare functions between 0 and 12 with an aperture of 3.5.\n\nt_grid &lt;- matrix(seq(0, 12, length = 5))\nG_temporal &lt;- local_basis(manifold = real_line(), # fns on R1\n                          type = \"bisquare\",      # bisquare\n                          loc = t_grid,           # centroids\n                          scale = rep(3.5, 5))    # aperture par.\n\nType show_basis(G_temporal) to visualize these basis functions. Finally, we construct the spatio-temporal basis functions by taking their tensor product.\n\nG &lt;- TensorP(G_spatial, G_temporal)  # take the tensor product\n\nNext we construct the BAUs. These are regularly placed space-time cubes covering our spatio-temporal domain. The cellsize we choose below is one that is similar to that which the IDE function constructed when specifying grid_size = 41 above. We impose a convex hull as a boundary that is tight around the data points, and not extended.\n\nBAUs &lt;- auto_BAUs(manifold = STplane(),   # ST field on plane\n                  type = \"grid\",          # gridded (not \"hex\")\n                  data = radar_obs,       # data\n                  cellsize = c(1.65, 2.38, 10), # BAU cell size\n                  nonconvex_hull = FALSE, # convex boundary\n                  convex = 0,             # no hull extension\n                  tunit = \"mins\")         # time unit is \"mins\"\nBAUs$fs = 1       # fs variation prop. to 1\n\nAs we did in Lab 4.2, we can take the measurement error to be that estimated elsewhere, in this case by the IDE model. Any remaining residual variation is then attributed to fine-scale variation that is modeled as white noise. Attribution of variation is less critical when validating against observational data, since the total variance is used when constructing prediction intervals.\n\nsigma2_eps &lt;- fit_results_radar2$IDEmodel$get(\"sigma2_eps\")\nradar_obs$std &lt;- sqrt(sigma2_eps)\n\nThe function FRK is now called to fit the random-effects model using the chosen basis functions and BAUs.\n\nS &lt;- FRK(f = z ~ 1,\n         BAUs = BAUs,\n         data = list(radar_obs), # (list of) data\n         basis = G,           # basis functions\n         n_EM = 2,            # max. no. of EM iterations\n         tol = 0.01)          # tol. on log-likelihood\n\nPrediction proceeds using the predict function.\n\nFRK_pred &lt;- predict(S)\n\nSince predict predicts over the BAUs, we need to associate each observation in our validation STIDFs to a BAU cell. This can be done simply using the function over. In the code below, the data frames df_block_over and df_random_over are data frames containing the predictions and prediction standard errors at the validation locations.\n\ndf_block_over &lt;- over(radar_valblock, FRK_pred)\ndf_random_over &lt;- over(radar_valrandom, FRK_pred)\n\n\n\nStep 4: Organizing Predictions for Further Analysis\nHaving obtained our predictions and prediction standard errors from the two models, the next step is to combine them into one data frame. We take the hold-out STIDF from the two time points, convert it to a data frame, and then put in the FRK and IDE predictions, prediction standard errors on the process, and the prediction standard errors in observation space. We distinguish between the latter two by using the labels predse and predZse, respectively.\n\nradar_valblock_df &lt;- radar_valblock %&gt;%\n             data.frame() %&gt;%\n             mutate(FRK_pred = df_block_over$mu,\n                    FRK_predse = df_block_over$sd,\n                    FRK_predZse = sqrt(FRK_predse^2 +\n                                       sigma2_eps),\n                    IDE_pred = pred_IDE_block$Ypred,\n                    IDE_predse = pred_IDE_block$Ypredse,\n                    IDE_predZse = sqrt(IDE_predse^2 +\n                                       sigma2_eps))\n\nFor plotting purposes, it is also convenient to construct a data frame in long format, where all the predictions are put into the same column, and a second column identifies to which model the prediction corresponds.\n\nradar_valblock_df_long &lt;- radar_valblock_df %&gt;%\n                          dplyr::select(s1, s2, timeHM, z,\n                                 FRK_pred, IDE_pred) %&gt;%\n                          gather(type, num, FRK_pred, IDE_pred)\n\nConstruction of radar_valrandom_df and radar_valrandom_df_long proceeds in identical fashion to the code given above (with block replaced with random) and is thus omitted.\n\n\nStep 5: Scoring\nNow we have everything in place to start analyzing the prediction errors. We start by simply plotting histograms of the prediction errors to get an initial feel of the distributions of these errors from the two models. As before, we only show the code for the left-out data in radar_valblock_df_long.\n\nggplot(radar_valblock_df_long) +\n    geom_histogram(aes(z - num, fill = type),\n                   binwidth = 2, position = \"identity\",\n                   alpha = 0.4, colour = 'black') + theme_bw()\n\nFigure 6.2 shows the resulting distributions. They are relatively similar; however, a close look reveals that the errors from the FRK model have a slightly larger spread, especially for the data at the missing time points. This is a first indication that FRK, and the lack of consideration of dynamics, will be at a disadvantage when predicting the process across time points for which we have no data.\nWe next look at the correlation between the predictions and the observations, plotted below and shown in Figure 6.10. Again, there does not seem to be much of a difference in the distribution of the errors between the two models when the data are missing at random, but there is a noticeable difference when the data are missing for entire time points. In fact, the correlation between the predictions and observations for the FRK model is, in this case, 0.716, while that for the IDE model is 0.846.\n\nggplot(radar_valblock_df) + geom_point(aes(z, FRK_pred))\nggplot(radar_valblock_df) + geom_point(aes(z, IDE_pred))\n\n\n\n\n\n\n\nFigure 6.10: Scatter plots of the observations and predictions for the FRK model (red) and the IDE model (blue), when the data are missing for entire time points (left) and at random (right).\n\n\n\nIt is interesting to see the effect of the absence of time points on the quality of the predictions. To this end, we can create a new data frame, which combines the validation data and the predictions, and compute the mean-squared prediction error (MSPE) for each time point.\n\nMSPE_time &lt;- rbind(radar_valrandom_df_long,\n                   radar_valblock_df_long) %&gt;%\n         group_by(timeHM, type) %&gt;%\n         dplyr::summarise(MSPE = mean((z - num)^2))\n\nThe following code plots the evolution of the MSPE as a function of time.\n\nggplot(MSPE_time) +\n  geom_line(aes(timeHM, MSPE, colour = type, group = type))\n\nThe evolution of the MSPE is shown in Figure 6.11, together with vertical dashed lines indicating the time points that were left out when fitting and predicting. It is remarkable to note how spatio-temporal FRK, due to its simple descriptive nature, suffers considerably, with an MSPE that is nearly twice that of the IDE model. Note that predictions close to this gap are also severely compromised. The IDE model is virtually unaffected by the missing data, as the trained dynamics are sufficiently informative to describe the evolution of the process at unobserved time points. At time points away from this gap, the MSPEs of the FRK and IDE models are comparable.\n\n\n\n\n\n\nFigure 6.11: MSPE of the FRK predictions (red) and the IDE predictions (blue) as a function of time. The dotted black lines mark the times where no data were available for fitting or predicting.\n\n\n\nThe importance of dynamics can be further highlighted by mapping the prediction standard errors at each time point. The plot in Figure 6.12, for which the commands are given below, reveals vastly contrasting spatial structures between the FRK prediction standard errors and the IDE prediction standard errors. Note that at other time points (we only show six adjoining time points) the prediction standard errors given by the two models are comparable.\n\nggplot(rbind(radar_valrandom_df_long, radar_valblock_df_long)) +\n  geom_tile(aes(s1, s2, fill = z - num)) +\n  facet_grid(type ~ timeHM) + coord_fixed() +\n  fill_scale(name = \"dBZ\") +\n  theme_bw()\n\n\n\n\n\n\n\nFigure 6.12: Spatial maps of prediction standard errors at the validation-data locations for the two missing time points and the adjoining six time points, based on the FRK model (top row) and the IDE model (bottom row).\n\n\n\nNext, we compute some of the cross-validation diagnostics. We consider the bias, the predictive cross-validation (PCV) and the standardized cross-validation (SCV) measures, and the continuous ranked probability score (CRPS). Functions for the first three are simple enough to code from scratch.\n\nBias &lt;- function(x, y) mean(x - y)          # x: val. obs.\nPCV &lt;- function(x, y) mean((x - y)^2)       # y: predictions\nSCV &lt;- function(x, y, v) mean((x - y)^2 / v) # v: pred. variances\n\nThe last one, CRPS, is a bit more tedious to implement, but it is available through the crps function of the verification package. The function crps returns, among other things, a field CRPS containing the average CRPS across all validation observations.\n\n## Compute CRPS. s is the pred. standard error\nCRPS &lt;- function(x, y, s) verification::crps(x, cbind(y, s))$CRPS\n\nFinally, we compute the diagnostics for each of the FRK and IDE models. In the code below, we show how to obtain them for the validation data at the missing time points; those for the validation data that are missing at random are obtained in a similar fashion. The diagnostics are summarized in Table Table 6.2, where it is clear that the IDE model outperforms the FRK model on most of the diagnostics considered here (note, in particular, the PCV for data associated with missing time points). For both models, we note that the SCV and CRPS need to be treated with care in a spatial or spatio-temporal setting, since the errors do exhibit some correlation, which is not taken into account when computing these measures.\n\nDiagblock &lt;- radar_valblock_df %&gt;% summarise(\n    Bias_FRK = Bias(FRK_pred, z),\n    Bias_IDE = Bias(IDE_pred, z),\n    PCV_FRK =  PCV(FRK_pred, z),\n    PCV_IDE =  PCV(IDE_pred, z),\n    SCV_FRK =  SCV(FRK_pred, z, FRK_predZse^2),\n    SCV_IDE =  SCV(IDE_pred, z, IDE_predZse^2),\n    CRPS_FRK = CRPS(z, FRK_pred, FRK_predZse),\n    CRPS_IDE = CRPS(z, IDE_pred, IDE_predZse)\n)\n\n\n\n\nTable 6.2: Cross-validation diagnostics for the FRK and IDE models fitted to the Sydney radar data set on data that are left out for two entire time intervals (top row) and at random (bottom row). The IDE model fares better for most diagnostics considered here, namely the bias (closer to zero is better), the predictive cross-validation measure (PCV, lower is better), the stand-ard-ized cross-validation measure (SCV, closer to 1 is better), and the continuous ranked probability score (CRPS, lower is better).\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nBias\n\nPCV\n\nSCV\n\nCRPS\n\n\n\n\n\n\nFRK\nIDE\nFRK\nIDE\nFRK\nIDE\nFRK\nIDE\n\n\nMissing time points\n-0.27\n0.58\n50.81\n29.90\n1.33\n0.58\n3.75\n3.03\n\n\nMissing at random\n-0.07\n-0.09\n34.20\n30.02\n0.78\n0.98\n3.14\n2.87\n\n\n\n\n\n\nThe multivariate energy score (ES) and variogram score of order \\(p\\) (\\(VS_p\\)) are available in R in the scoringRules package. The two functions we shall be using are es_sample and vs_sample. However, to compute these scores, we first need to simulate forecasts from the predictive distribution. To do this, we not only need the marginal prediction variances, but also all the prediction covariances. Due to the size of the prediction covariance matrices, multivariate scoring can only be done on at most a few thousand predictions at a time.\nFor this part of the Lab, we consider the validation data at 09:35 from the Sydney radar data set.\n\nradar_val0935 &lt;- subset(radar_valblock,\n                        radar_valblock$timeHM == \"09:35\")\nn_0935 &lt;- length(radar_val0935)  # number of validation data\n\nTo predict with the IDE model and store the covariances, we simply set the argument covariances to TRUE.\n\npred_IDE_block &lt;- predict(fit_results_radar2$IDEmodel,\n                          newdata = radar_val0935,\n                          covariances = TRUE)\n\nTo predict with the FRK model and store the covariances, we also set the argument covariances to TRUE.\n\nFRK_pred_block &lt;- predict(S,\n                          newdata = radar_val0935,\n                          covariances = TRUE)\n\nThe returned objects are lists that contain the predictions in the item newdata and the covariances in an item Cov. Now, both es_sample and vs_sample are designed to compare a sample of forecasts to data, and therefore we need to simulate some realizations from the predictive distribution before calling these functions.\nRecalling Lab 5.1, one of the easiest ways to simulate from a Gaussian random vector \\(\\mathbf{x}\\) with mean \\(\\boldsymbol{\\mu}\\) and covariance matrix \\(\\boldsymbol{\\Sigma}\\) is to compute the lower Cholesky factor of \\(\\boldsymbol{\\Sigma}\\), call this \\(\\mathbf{L}\\), and then to compute\n\\[\n\\mathbf{Z}_{\\textrm{sim}} = \\boldsymbol{\\mu}+ \\mathbf{L}\\mathbf{e},\n\\]\nwhere \\(\\mathbf{e}\\sim iid~Gau(\\mathbf{0}, \\mathbf{I})\\). In our case, \\(\\boldsymbol{\\mu}\\) contains the estimated intercept plus the predictions, while \\(\\mathbf{L}\\) is the lower Cholesky factor of whatever covariance matrix was returned in Cov with the measurement-error variance, \\(\\sigma^2_\\epsilon\\), added onto the diagonal (since we are validating against observations, and not process values). Recall that we have set \\(\\sigma^2_\\epsilon\\) to be the same for the FRK and the IDE models.\n\nVeps &lt;- diag(rep(sigma2_eps, n_0935))\n\nNow the Cholesky factors of the predictive covariance matrices for the IDE and FRK models are given by the following commands.\n\nL_IDE &lt;- t(chol(pred_IDE_block$Cov + Veps))\nL_FRK &lt;- t(chol(FRK_pred_block$Cov + Veps))\n\nThe intercepts estimated by both models are given by the following commands.\n\nIntIDE &lt;- coef(fit_results_radar2$IDEmodel)\nIntFRK &lt;- coef(S)\n\nWe can generate 100 simulations at once by adding on the mean component (intercept plus prediction) to 100 realizations simulated using the Cholesky factor as follows.\n\nnsim &lt;- 100\nE &lt;- matrix(rnorm(n_0935*nsim), n_0935, nsim)\nSims_IDE &lt;- IntIDE + pred_IDE_block$newdata$Ypred + L_IDE %*% E\nSims_FRK &lt;- IntFRK + FRK_pred_block$newdata$mu + L_FRK %*% E\n\nIn Figure 6.13 we show one of the simulations for both the FRK and the IDE model, together with the validation data, at time point 09:35. Note how the IDE model is able to capture more structure in the predictions than the FRK model.\n\n## Put into long format\nradar_val0935_long &lt;- cbind(data.frame(radar_val0935),\n                            IDE = Sims_IDE[,1],\n                            FRK = Sims_FRK[,1]) %&gt;%\n                      gather(type, val, z, FRK, IDE)\n\n## Plot\ngsims &lt;- ggplot(radar_val0935_long) +\n    geom_tile(aes(s1, s2, fill = val)) +\n  facet_grid(~ type) + theme_bw() + coord_fixed() +\n  fill_scale(name = \"dBZ\")\n\n\n\n\n\n\n\nFigure 6.13: One of the 100 simulations from the predictive distribution of the FRK model (left) and the IDE model (center), and the data (not used to train the model, right) at 09:35.\n\n\n\nWe now compute the ES for both models by supplying the data and the simulations in matrix form to es_sample.\n\nes_sample(radar_val0935$z, dat = as.matrix(Sims_IDE))\n\n[1] 144\n\nes_sample(radar_val0935$z, dat = as.matrix(Sims_FRK))\n\n[1] 205\n\n\nAs with all proper scoring rules, lower is better, and we clearly see in this case that the IDE model has a lower ES than that for the FRK model for these validation data. For \\(VS_p\\), we also need to specify weights. Here we follow the example given in the help file of vs_sample and set \\(w_{ij} = 0.5^{d_{ij}}\\), where \\(d_{ij}\\) is the distance between the \\(i\\)th and \\(j\\)th prediction locations.\n\ndistances &lt;- radar_val0935 %&gt;%\n           coordinates() %&gt;%\n           dist() %&gt;%\n           as.matrix()\nweights &lt;- 0.5^distances\n\nThe function vs_sample is then called in a similar way to es_sample, but this time specifying the weights and the order (we chose \\(p = 1\\)).\n\nvs_sample(radar_val0935$z, dat = as.matrix(Sims_IDE),\n          w_vs = weights, p = 1)\n\n[1] 66917\n\nvs_sample(radar_val0935$z, dat = as.matrix(Sims_FRK),\n          w_vs = weights, p = 1)\n\n[1] 78534\n\n\nAs expected, we find that the IDE model has a lower \\(VS_1\\) than the FRK model. Thus, the IDE model in this case has provided better probabilistic predictions than the FRK model, both marginally and jointly.\n\n\nStep 6: Model Comparison\nWe conclude this Lab by evaluating the Akaike information criterion (AIC) and Bayesian information criterion (BIC) for the two models. Recall that the AIC and BIC of a model \\({\\cal M}_\\ell\\) with \\(p_l\\) parameters estimated with \\(m^*\\) data points are given by \\[\n\\begin{aligned}\nAIC({\\cal M}_\\ell) &= -2 \\log p(\\mathbf{Z}| \\widehat{\\boldsymbol{\\theta}}, {\\cal M}_\\ell) + 2p_l, \\\\\nBIC({\\cal M}_\\ell) &= - 2 \\log p(\\mathbf{Z}| \\widehat{\\boldsymbol{\\theta}}, {\\cal M}_\\ell) + \\log(m^*) p_l.\n\\end{aligned}\n\\]\nFor both AIC and BIC, we need the log-likelihood of the model at the estimated parameters. For the models we consider, these can be extracted using the function logLik in FRK and the negative of the function negloglik supplied with the IDE object.\n\nloglikFRK &lt;- FRK:::logLik(S)\nloglikIDE &lt;- -fit_results_radar2$IDEmodel$negloglik()\n\nBefore we can compute the AIC and BIC for our models, we first need to find out how many parameters were estimated. For the IDE model, the intercept, two variance parameters (one for measurement error and one for the temporal invariant disturbance term) and four kernel parameters were estimated, for a total of seven parameters. For the FRK model, the intercept, four variance parameters (one for measurement error, one for fine-scale variation, and one for each resolution of the basis functions) and four length-scale parameters (one spatial and one temporal length-scale for each resolution) were estimated, for a total of nine parameters.\n\npIDE &lt;- 7\npFRK &lt;- 9\n\nThe total number of data points used to fit the two models is\n\nm &lt;- length(radar_obs)\n\nWe now find the AIC and BIC for both models.\n\n## Initialize table\nCriteria &lt;- data.frame(AIC = c(0, 0), BIC = c(0, 0),\n                         row.names = c(\"FRK\", \"IDE\"))\n\n## Compute criteria\nCriteria[\"FRK\", \"AIC\"] &lt;- -2*loglikFRK + 2*pFRK\nCriteria[\"IDE\", \"AIC\"] &lt;- -2*loglikIDE + 2*pIDE\nCriteria[\"FRK\", \"BIC\"] &lt;- -2*loglikFRK + pFRK*log(m)\nCriteria[\"IDE\", \"BIC\"] &lt;- -2*loglikIDE + pIDE*log(m)\nCriteria\n\n      AIC   BIC\nFRK 65992 66057\nIDE 45626 45676\n\n\nBoth the AIC and BIC are much smaller for the IDE model than for the FRK model. When the difference in the criteria is so large (in this case around 10,000), it is safe to conclude that one model is a much better representation of the data. Combined with the other visualizations and diagnostics, we can conclude that the IDE model is preferable to the FRK model for modeling and predicting with the Sydney radar data set.\nAs a final note, as discussed in Section 6.4.4, the AIC and BIC are not really appropriate for model selection in the presence of dependent random effects as the effective number of parameters in such settings is more than the number of parameters describing the fixed effects and covariance functions, and less than this number plus the number of basis-function coefficients (due to dependence; e.g., Hodges & Sargent (2001), Overholser & Xu (2014)). Excluding the number of basis functions (i.e., the number of random effects) when computing the AIC and BIC clearly results in optimistic criteria; other measures such as the conditional AIC (e.g., Overholser & Xu (2014)) or the DIC, WAIC, and PPL are more suitable for such problems (see Section 6.4.4).\n\n\n\n\nBenjamini, Y., & Hochberg, Y. (1995). Controlling the false discovery rate: A practical and powerful approach to multiple testing. Journal of the Royal Statistical Society, Series B, 57, 289–300.\n\n\nBerliner, L. M., Milliff, R. F., & Wikle, C. K. (2003). Bayesian hierarchical modeling of air-sea interaction. Journal of Geophysical Research: Oceans, 108(C4).\n\n\nBranstator, G., Mai, A., & Baumhefner, D. (1993). Identification of highly predictable flow elements for spatial filtering of medium- and extended-range numerical forecasts. Monthly Weather Review, 121(6), 1786–1802.\n\n\nBriggs, W. M., & Levine, R. A. (1997). Wavelets and field forecast verification. Monthly Weather Review, 125(6), 1329–1341.\n\n\nBröcker, J., & Smith, L. A. (2007). Scoring probabilistic forecasts: The importance of being proper. Weather and Forecasting, 22(2), 382–388.\n\n\nBrown, B. G., Gilleland, E., & Ebert, E. E. (2012). Forecasts of spatial fields. In I. T. Jolliffe & D. B. Stephenson (Eds.), Forecast verification: A practitioner’s guide in atmospheric science (2nd ed., pp. 95–117). John Wiley & Sons.\n\n\nCarroll, S. S., & Cressie, N. (1996). A comparison of geostatistical methodologies used to estimate snow water equivalent. Water Resources Bulletin, 32(2), 267–278.\n\n\nCarvalho, A. (2016). An overview of applications of proper scoring rules. Decision Analysis, 13(4), 223–242.\n\n\nCongdon, P. (2006). Bayesian model choice based on monte carlo estimates of posterior model probabilities. Computational Statistics & Data Analysis, 50(2), 346–357.\n\n\nCook, R. D. (1977). Detection of influential observation in linear regression. Technometrics, 19(1), 15–18.\n\n\nCressie, N. (1993). Statistics for spatial data (revised). John Wiley & Sons.\n\n\nCressie, N., & Wikle, C. K. (2011). Statistics for spatio-temporal data. John Wiley & Sons.\n\n\nEbert, E. E., & McBride, J. L. (2000). Verification of precipitation in weather systems: Determination of systematic errors. Journal of Hydrology, 239(1-4), 179–202.\n\n\nGelfand, A. E., & Ghosh, S. K. (1998). Model choice: A minimum posterior predictive loss approach. Biometrika, 85(1), 1–11.\n\n\nGelman, A., Carlin, J. B., Stern, H. S., Dunson, D. B., Vehtari, A., & Rubin, D. B. (2014). Bayesian data analysis (3rd ed.). Chapman & Hall/CRC.\n\n\nGilleland, E., Ahijevych, D. A., Brown, B. G., & Ebert, E. E. (2010). Verifying forecasts spatially. Bulletin of the American Meteorological Society, 91(10), 1365–1376.\n\n\nGneiting, T., & Katzfuss, M. (2014). Probabilistic forecasting. Annual Review of Statistics and Its Application, 1, 125–151.\n\n\nGneiting, T., & Raftery, A. E. (2007). Strictly proper scoring rules, prediction, and estimation. Journal of the American Statistical Association, 102(477), 359–378.\n\n\nHaslett, J. (1999). A simple derivation of deletion diagnostic results for the general linear model with correlated errors. Journal of the Royal Statistical Society, Series B, 61(3), 603–609.\n\n\nHastie, T., Tibshirani, R., & Friedman, J. (2009). The elements of statistical learning (2nd ed.). Springer.\n\n\nHodges, J. S., & Sargent, D. J. (2001). Counting degrees of freedom in hierarchical and other richly-parameterised models. Biometrika, 88, 367–379.\n\n\nHoeting, J. A., Madigan, D., Raftery, A. E., & Volinsky, C. T. (1999). Bayesian model averaging: A tutorial. Statistical Science, 382–401.\n\n\nHooten, M. B., & Hobbs, N. (2015). A guide to bayesian model selection for ecologists. Ecological Monographs, 85(1), 3–28.\n\n\nJordan, A., Krüger, F., & Lerch, S. (2017). Evaluation of probabilistic forecasts with the scoringRules package. EGU General Assembly Conference Abstracts, 19, 3295.\n\n\nKang, E. L., Liu, D., & Cressie, N. (2009). Statistical analysis of small-area data based on independence, spatial, non-hierarchical, and hierarchical models. Computational Statistics & Data Analysis, 53(8), 3016–3032.\n\n\nKornak, J., Irwin, M. E., & Cressie, N. (2006). Spatial point process models of defensive strategies: Detecting changes. Statistical Inference for Stochastic Processes, 9(1), 31–46.\n\n\nLaud, P. W., & Ibrahim, J. G. (1995). Predictive model selection. Journal of the Royal Statistical Society, Series B, 247–262.\n\n\nLivezey, R. E., & Chen, W. Y. (1983). Statistical field significance and its determination by monte carlo techniques. Monthly Weather Review, 111(1), 46–59.\n\n\nMicheas, A. C., Fox, N. I., Lack, S. A., & Wikle, C. K. (2007). Cell identification and verification of QPF ensembles using shape analysis techniques. Journal of Hydrology, 343(3-4), 105–116.\n\n\nMurphy, A. H. (1993). What is a good forecast? An essay on the nature of goodness in weather forecasting. Weather and Forecasting, 8(2), 281–293.\n\n\nNakazono, Y. (2013). Strategic behavior of federal open market committee board members: Evidence from members’ forecasts. Journal of Economic Behavior & Organization, 93, 62–70.\n\n\nOverholser, R., & Xu, R. (2014). Effective degrees of freedom and its application to conditional AIC for linear mixed-effects models with correlated error structures. Journal of Multivariate Analysis, 132, 160–170.\n\n\nScheuerer, M., & Hamill, T. M. (2015). Variogram-based proper scoring rules for probabilistic forecasts of multivariate quantities. Monthly Weather Review, 143(4), 1321–1334.\n\n\nShen, X., Huang, H.-C., & Cressie, N. (2002). Nonparametric hypothesis testing for a spatial signal. Journal of the American Statistical Association, 97(460), 1122–1140.\n\n\nSpiegelhalter, D. J., Best, N. G., Carlin, B. P., & Linde, A. van der. (2002). Bayesian measures of model complexity and fit. Journal of the Royal Statistical Society, Series B, 64(4), 583–639.\n\n\nStanford, J. L., & Ziemke, J. R. (1994). Field (MAP) statistics. In J. Stanford & S. B. Vardeman (Eds.), Statistical methods for physical science (pp. 457–479). Academic Press.\n\n\nVer Hoef, J. M., & Boveng, P. L. (2015). Iterating on a single model is a viable alternative to multimodel inference. Journal of Wildlife Management, 79(5), 719–729.\n\n\nVon Storch, H., & Zwiers, F. W. (2002). Statistical analysis in climate research. Cambridge University Press.\n\n\nWeigel, A. P. (2012). Ensemble forecasts. In I. T. Jolliffe & D. B. Stephenson (Eds.), Forecast verification: A practitioner’s guide in atmospheric science (pp. 141–166). John Wiley & Sons.\n\n\nWilks, D. S. (2011). Statistical methods in the atmospheric sciences (3rd ed.). Academic Press.\n\n\nZhang, J., Craigmile, P. F., & Cressie, N. (2008). Loss function approaches to predict a spatial quantile and its exceedance region. Technometrics, 50(2), 216–227.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Evaluating Spatio-Temporal Statistical Models</span>"
    ]
  },
  {
    "objectID": "ChapterPergimus.html",
    "href": "ChapterPergimus.html",
    "title": "Pergimus (Epilogue)",
    "section": "",
    "text": "These are our last words in the book, but that doesn’t mean this is the end. This epilogical chapter’s title is meant to convey an open-endedness to our project. Our Latin sources tell us that pergimus means “let’s go forward” or “let’s continue to progress,” derived from the verb pergere. Here’s an opportunity for you, the reader, to move beyond the previous six chapters and develop your own statistical approaches to the analysis of spatio-temporal data. You now have a sense for the motivations, main concepts, and practicalities behind spatio-temporal statistics, and the R Labs have given you an important “hands-on” perspective.\nWe hope you’ve seen enough to want more than what is in our book. A stepping-off point for more theory and methods might be in the pages of Chapters 6–9 of Cressie & Wikle (2011); and you can find more and more applications in the literature, most recently where the spatio-temporal models fitted are non-Gaussian, nonlinear, and multivariate. We expect that by the time our book comes out, new applications and software for spatio-temporal statistics will have appeared, and we hope you’ll be motivated yourself to contribute.\nWe’ve tried to emphasize that spatio-temporal data are ubiquitous in the real, complex, messy world, and making sense of them depends on accounting for spatio-temporal dependencies. In the past, it’s been difficult to handle the complexity of such data, the hidden processes behind them, and the sheer size of many of the data sets. Yet the principles of good statistical practice still apply – they’re just a bit more involved! We should still explore our data through visualization and quantitative summaries; we should still try to build parsimonious models; we should add complexity to our models only when necessary; and we still need to evaluate our inferences through simulation and (cross-)validation. Then, after making all necessary modifications to the model, we go through the modeling–evaluation cycle again!\nThere are several challenges that are particular to spatio-temporal statistics. The obvious one is how to accommodate the complex dependencies that are typically present in spatio-temporal data. This is often exacerbated by the curse of dimensionality – that is, we may have a lot of data and/or are interested in predicting at a lot of locations in space and time. It’s often worse when we’re data-rich in one of the dimensions (e.g., space) but data-poor in the other (e.g., time, or vice versa). These challenges can be met by focusing on parsimonious parameterizations, for example when parameterizing spatio-temporal covariance functions in the descriptive approach or propagator matrices in the dynamic approach. In the latter case, using mechanistic processes to motivate parsimonious dynamic models has proven very useful.\nIn both cases, a very effective strategy is to treat scientifically interpretable parameters as random processes (e.g., spatial stochastic processes) at a lower level in a hierarchical statistical model. We’ve also seen that if we’re not careful about how our models are parameterized, we can run into serious computational roadblocks. One of the most helpful solutions comes through basis-function expansions, where the modeling effort is typically redirected towards specifying multivariate-time-series models for the random coefficients of the basis functions.\nFinally, we’ve presented some approaches to model evaluation (checking, validation, and selection) for models fitted to spatio-temporal data. However, this is very much an open area of research, and there’s no “one way” to go about it. Nor should there be: using the analogy of a medical professional trying to evaluate a patient’s health status, such evaluation comes from running a battery of diagnostics.\nWe’ve entered an interesting time where statistical applications are increasingly using machine-learning methods to answer all sorts of questions. All the rage at the time of writing are “deep learning” methods based on deep models, which are quite complicated but, as noted earlier, essentially hierarchical. Statistical and machine-learning versions of these models share many things in common, such as requiring a lot of training data and prior information, substantial regularization (smoothing), and high-performance computing. The biggest difference to date is that machine-learning methods don’t always provide estimates of uncertainty or account for uncertainties in inputs and outputs. In the near future, we expect there will be substantially more cross-fertilization between these two paradigms, leading to new avenues of research and development in spatio-temporal modeling. This is an interesting and exciting place to be, at the intersection of statistics and the data-oriented disciplines in science, technology, engineering, and mathematics (STEM) that loosely define “data science.”\nWe believe that the statistical methods presented in this book provide a good practical foundation for much of spatio-temporal statistics, although there are many things that we didn’t cover – not because they are less important, but mainly because of space and time limitations (pun intended!). For example, some of the topics on our “should’ve but didn’t” list are:\n\nspatio-temporal point processes\nspatio-temporal random sets\ncontinuous-time spatio-temporal processes\nspatio-temporal extremes\nmultivariate spatio-temporal processes\nspatio-temporal design of sampling networks\nspatio-temporal change-of-support (resolution, alignment, scale)\nsmall-area panel data from surveys\nmore details on estimation, computation, and implementation (especially in “big data” situations)\nmore R examples of Bayesian spatio-temporal statistical analyses.\n\nIt’s time to take a break, but let’s continue to progress… and we invite you to share your progress and check up on ours through the book’s website: https://spacetimewithr.org.\n\n\n\n\nCressie, N., & Wikle, C. K. (2011). Statistics for spatio-temporal data. John Wiley & Sons.",
    "crumbs": [
      "Pergimus (Epilogue)"
    ]
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "Auguie, B. (2016). gridExtra: Miscellaneous functions for\n“grid” graphics.\n\n\nBaddeley, A., Rubak, E., & Turner, R. (2015). Spatial point\npatterns: Methodology and applications with r. Chapman &\nHall/CRC.\n\n\nBakar, K. S., & Sahu, S. K. (2015). spTimer: Spatio-temporal\nbayesian modelling using r. Journal of Statistical Software,\n63(15), 1–32.\n\n\nBanerjee, S., Carlin, B. P., & Gelfand, A. E. (2015).\nHierarchical modeling and analysis for spatial data (second).\nChapman & Hall/CRC.\n\n\nBates, D., & Maechler, M. (2017). Matrix: Sparse and dense\nmatrix classes and methods.\n\n\nBecker, R. A., & Wilks, A. R. (2017). Maps: Draw geographical\nmaps.\n\n\nBenjamini, Y., & Hochberg, Y. (1995). Controlling the false\ndiscovery rate: A practical and powerful approach to multiple testing.\nJournal of the Royal Statistical Society, Series B,\n57, 289–300.\n\n\nBerliner, L. M. (1996). Hierarchical Bayesian time series\nmodels. In K. M. Hanson & R. N. Silver (Eds.), Maximum entropy\nand bayesian methods (pp. 15–22). Kluwer.\n\n\nBerliner, L. M., Milliff, R. F., & Wikle, C. K. (2003). Bayesian\nhierarchical modeling of air-sea interaction. Journal of Geophysical\nResearch: Oceans, 108(C4).\n\n\nBivand, R. S., Pebesma, E., & Gomez-Rubio, V. (2013). Applied\nspatial data analysis with R (second). Springer. http://www.asdar-book.org/\n\n\nBivand, R., & Lewin-Koh, N. (2017). Maptools: Tools for reading\nand handling spatial objects.\n\n\nBlangiardo, M., & Cameletti, M. (2015). Spatial and\nspatio-temporal bayesian models with R-INLA. John\nWiley & Sons.\n\n\nBox, G. E. P. (1976). Science and statistics. Journal of the\nAmerican Statistical Association, 71(356), 791–799.\n\n\nBox, G. E. P. (1979). Robustness in the strategy of scientific model\nbuilding. In R. L. Launer & G. L. Wilkinson (Eds.), Robustness\nin statistics (pp. 201–236). Academic Press.\n\n\nBranstator, G., Mai, A., & Baumhefner, D. (1993). Identification of\nhighly predictable flow elements for spatial filtering of medium- and\nextended-range numerical forecasts. Monthly Weather Review,\n121(6), 1786–1802.\n\n\nBriggs, W. M., & Levine, R. A. (1997). Wavelets and field forecast\nverification. Monthly Weather Review, 125(6),\n1329–1341.\n\n\nBröcker, J., & Smith, L. A. (2007). Scoring probabilistic forecasts:\nThe importance of being proper. Weather and Forecasting,\n22(2), 382–388.\n\n\nBrown, B. G., Gilleland, E., & Ebert, E. E. (2012). Forecasts of\nspatial fields. In I. T. Jolliffe & D. B. Stephenson (Eds.),\nForecast verification: A practitioner’s guide in atmospheric\nscience (2nd ed., pp. 95–117). John Wiley & Sons.\n\n\nCarlin, B. P., & Louis, T. A. (2010). Bayes and empirical bayes\nmethods for data analysis. Chapman & Hall/CRC.\n\n\nCarroll, S. S., & Cressie, N. (1996). A comparison of geostatistical\nmethodologies used to estimate snow water equivalent. Water\nResources Bulletin, 32(2), 267–278.\n\n\nCarvalho, A. (2016). An overview of applications of proper scoring\nrules. Decision Analysis, 13(4), 223–242.\n\n\nChristakos, G. (2017). Spatiotemporal random fields: Theory and\napplications (second). Elsevier.\n\n\nCohen, A., & Jones, R. H. (1969). Regression on a random field.\nJournal of the American Statistical Association,\n64(328), 1172–1182.\n\n\nCongdon, P. (2006). Bayesian model choice based on monte carlo estimates\nof posterior model probabilities. Computational Statistics &\nData Analysis, 50(2), 346–357.\n\n\nCook, R. D. (1977). Detection of influential observation in linear\nregression. Technometrics, 19(1), 15–18.\n\n\nCressie, N. (1990). The origins of kriging. Mathematical\nGeology, 22, 239–252.\n\n\nCressie, N. (1993). Statistics for spatial data (revised). John\nWiley & Sons.\n\n\nCressie, N., & Huang, H.-C. (1999). Classes of nonseparable,\nspatio-temporal stationary covariance functions. Journal of the\nAmerican Statistical Association, 94(448), 1330–1339.\n\n\nCressie, N., Shi, T., & Kang, E. L. (2010). Fixed rank filtering for\nspatio-temporal data. Journal of Computational and Graphical\nStatistics, 19(3), 724–745.\n\n\nCressie, N., & Wikle, C. K. (2011). Statistics for\nspatio-temporal data. John Wiley & Sons.\n\n\nCrujeiras, R. M., Fernández-Casal, R., & González-Manteiga, W.\n(2010). Nonparametric test for separability of spatio-temporal\nprocesses. Environmetrics, 21(3-4), 382–399.\n\n\nDahl, D. B. (2016). Xtable: Export tables to LaTeX or HTML.\n\n\nDelmonico, R. (2017). The philosophy of fractals.\n\n\nDiggle, P. J. (2013). Statistical analysis of spatial and\nspatio-temporal point patterns. Chapman & Hall/CRC.\n\n\nDiggle, P. J., & Ribeiro Jr., P. J. (2007). Model-based\ngeostatistics. Springer.\n\n\nDouc, R., Moulines, E., & Stoffer, D. (2014). Nonlinear time\nseries: Theory, methods and applications with R\nexamples. Chapman & Hall/CRC.\n\n\nEbert, E. E., & McBride, J. L. (2000). Verification of precipitation\nin weather systems: Determination of systematic errors. Journal of\nHydrology, 239(1-4), 179–202.\n\n\nFinley, A. O., Banerjee, S., & Carlin, B. P. (2007). spBayes: An r\npackage for univariate and multivariate hierarchical point-referenced\nspatial models. Journal of Statistical Software,\n19(4), 1–24.\n\n\nFisher, R. A. (1935). The design of experiments (8th ed.).\nEdinburgh: Oliver; Boyd.\n\n\nGamerman, D., & Lopes, H. F. (2006). Markov chain monte carlo:\nStochastic simulation for bayesian inference (2nd ed.). Chapman\n& Hall/CRC.\n\n\nGelfand, A. E., & Ghosh, S. K. (1998). Model choice: A minimum\nposterior predictive loss approach. Biometrika, 85(1),\n1–11.\n\n\nGelman, A., Carlin, J. B., Stern, H. S., Dunson, D. B., Vehtari, A.,\n& Rubin, D. B. (2014). Bayesian data analysis (3rd ed.).\nChapman & Hall/CRC.\n\n\nGenton, M. G., Castruccio, S., Crippa, P., Dutta, S., Huser, R., Sun,\nY., & Vettori, S. (2015). Visuanimation in statistics.\nStat, 4(1), 81–96.\n\n\nGilleland, E. (2018). SpatialVx: Spatial forecast verification.\n\n\nGilleland, E., Ahijevych, D. A., Brown, B. G., & Ebert, E. E.\n(2010). Verifying forecasts spatially. Bulletin of the American\nMeteorological Society, 91(10), 1365–1376.\n\n\nGneiting, T., & Katzfuss, M. (2014). Probabilistic forecasting.\nAnnual Review of Statistics and Its Application, 1,\n125–151.\n\n\nGneiting, T., & Raftery, A. E. (2007). Strictly proper scoring\nrules, prediction, and estimation. Journal of the American\nStatistical Association, 102(477), 359–378.\n\n\nGonzález, I., & Déjean, S. (2012). CCA: Canonical correlation\nanalysis.\n\n\nGoodfellow, I., Bengio, Y., Courville, A., & Bengio, Y. (2016).\nDeep learning. MIT Press.\n\n\nGoulet, V., Dutang, C., Maechler, M., Firth, D., Shapira, M., &\nStadelmann, M. (2017). Expm: Matrix exponential, log,\n“etc”.\n\n\nGräler, B., Pebesma, E., & Heuvelink, G. (2016). Spatio-temporal\ninterpolation using gstat. R Journal, 8, 204–218. https://journal.r-project.org/archive/2016-1/na-pebesma-heuvelink.pdf\n\n\nHanks, E. M., Schliep, E. M., Hooten, M. B., & Hoeting, J. A.\n(2015). Restricted spatial regression in practice: Geostatistical\nmodels, confounding, and robustness under model misspecification.\nEnvironmetrics, 26(4), 243–254.\n\n\nHarvey, A. C. (1993). Time series models (2nd ed.). MIT Press.\n\n\nHaslett, J. (1999). A simple derivation of deletion diagnostic results\nfor the general linear model with correlated errors. Journal of the\nRoyal Statistical Society, Series B, 61(3), 603–609.\n\n\nHastie, T., Tibshirani, R., & Friedman, J. (2009). The elements\nof statistical learning (2nd ed.). Springer.\n\n\nHenebry, G. M. (1995). Spatial model error analysis using\nautocorrelation indices. Ecological Modelling, 82(1),\n75–91.\n\n\nHenry, L., & Wickham, H. (2017). Purrr: Functional programming\ntools.\n\n\nHlavac, M. (2015). Stargazer: Well-formatted regression and summary\nstatistics tables. Harvard University.\n\n\nHodges, J. S., & Reich, B. J. (2010). Adding spatially-correlated\nerrors can mess up the fixed effect you love. American\nStatistician, 64(4), 325–334.\n\n\nHodges, J. S., & Sargent, D. J. (2001). Counting degrees of freedom\nin hierarchical and other richly-parameterised models.\nBiometrika, 88, 367–379.\n\n\nHoeting, J. A., Madigan, D., Raftery, A. E., & Volinsky, C. T.\n(1999). Bayesian model averaging: A tutorial. Statistical\nScience, 382–401.\n\n\nHooten, M. B., & Hobbs, N. (2015). A guide to bayesian model\nselection for ecologists. Ecological Monographs,\n85(1), 3–28.\n\n\nHotelling, H. (1936). Relations between two sets of variates.\nBiometrika, 28(3/4), 321–377.\n\n\nHovmöller, E. (1949). The trough-and-ridge diagram. Tellus,\n1(2), 62–66.\n\n\nHughes, J., & Haran, M. (2013). Dimension reduction and alleviation\nof confounding for spatial generalized linear mixed models. Journal\nof the Royal Statistical Society, Series B, 75(1),\n139–159.\n\n\nJaeger, H. (2007). Echo state network. Scholarpedia,\n2(9), 2330.\n\n\nJames, G., Witten, D., Hastie, T., & Tibshirani, R. (2013). An\nintroduction to statistical learning. Springer.\n\n\nJohnson, R. A., & Wichern, D. W. (1992). Applied multivariate\nstatistical analysis. In Prentice Hall (3rd ed.). Prentice\nHall.\n\n\nJordan, A., Krueger, F., & Lerch, S. (2017). scoringRules:\nScoring rules for parametric and simulated distribution forecasts.\n\n\nJordan, A., Krüger, F., & Lerch, S. (2017). Evaluation of\nprobabilistic forecasts with the scoringRules package. EGU General\nAssembly Conference Abstracts, 19, 3295.\n\n\nKahle, D., & Wickham, H. (2013). Ggmap: Spatial visualization with\nggplot2. R Journal, 5(1), 144–161. http://journal.r-project.org/archive/2013-1/kahle-wickham.pdf\n\n\nKalman, R. E. (1960). A new approach to linear filtering and prediction\nproblems. Journal of Basic Engineering, 82, 35–45.\n\n\nKang, E. L., Liu, D., & Cressie, N. (2009). Statistical analysis of\nsmall-area data based on independence, spatial, non-hierarchical, and\nhierarchical models. Computational Statistics & Data\nAnalysis, 53(8), 3016–3032.\n\n\nKendall, M. G., & Stuart, A. (1969). The advanced theory of\nstatistics (3rd ed., Vol. 1). Hafner.\n\n\nKornak, J., Irwin, M. E., & Cressie, N. (2006). Spatial point\nprocess models of defensive strategies: Detecting changes.\nStatistical Inference for Stochastic Processes, 9(1),\n31–46.\n\n\nKrainski, E. T., Gómez-Rubio, V., Bakka, H., Lenzi, A., Castro-Camilo,\nD., Simpson, D., Lindgren, F., & Rue, H. (2019). Advanced\nspatial modeling with stochastic partial differential equations using r\nand INLA. Chapman; Hall/CRC.\n\n\nKuhnert, P. (2014). Editorial: Physical-statistical modelling.\nEnvironmetrics, 25, 201–202.\n\n\nKutner, H., M, Nachtsheim, C. J., & Neter, J. (2004). Applied\nmultiple regression models. McGraw-Hill.\n\n\nLaird, N. M., & Ware, J. H. (1982). Random-effects models for\nlongitudinal data. Biometrics, 963–974.\n\n\nLamigueiro, O. P. (2018). Displaying time series, spatial, and\nspace-time data with r (2nd ed.). Chapman; Hall/CRC.\n\n\nLaud, P. W., & Ibrahim, J. G. (1995). Predictive model selection.\nJournal of the Royal Statistical Society, Series B, 247–262.\n\n\nLe, N. D., & Zidek, J. V. (2006). Statistical analysis of\nenvironmental space-time processes. Springer.\n\n\nLindgren, F., & Rue, H. (2015). Bayesian spatial modelling with\nR-INLA. Journal of Statistical\nSoftware, 63(19), 1–25. http://www.jstatsoft.org/v63/i19/\n\n\nLindgren, F., Rue, H., & Lindström, J. (2011). An explicit link\nbetween gaussian fields and gaussian markov random fields: The\nstochastic partial differential equation approach. Journal of the\nRoyal Statistical Society, Series B, 73(4), 423–498.\n\n\nLindstrom, J., Szpiro, A., Sampson, P. D., Bergen, S., & Oron, A. P.\n(2013). SpatioTemporal: Spatio-temporal model estimation.\n\n\nLivezey, R. E., & Chen, W. Y. (1983). Statistical field significance\nand its determination by monte carlo techniques. Monthly Weather\nReview, 111(1), 46–59.\n\n\nLucchesi, L. R., & Wikle, C. K. (2017). Visualizing uncertainty in\nareal data with bivariate choropleth maps, map pixelation, and glyph\nrotation. Stat, 6, 292–302.\n\n\nLukoševičius, M. (2012). A practical guide to applying echo state\nnetworks. In G. Montavon, G. B. Orr, & K.-R. Müller (Eds.),\nNeural networks: Tricks of the trade (2nd ed., pp. 659–686).\nSpringer.\n\n\nLukoševičius, M., & Jaeger, H. (2009). Reservoir computing\napproaches to recurrent neural network training. Computer Science\nReview, 3(3), 127–149.\n\n\nLumley, T. (2017). Leaps: Regression subset selection.\n\n\nLütkepohl, H. (2005). New introduction to multiple time series\nanalysis. Springer.\n\n\nMateu, J., & Müller, W. G. (2013). Spatio-temporal design:\nAdvances in efficient data acquisition. John Wiley & Sons.\n\n\nMcCullagh, P., & Nelder, J. A. (1989). Generalized linear\nmodels. Cambridge University Press.\n\n\nMcCulloch, C. E., & Searle, S. R. (2001). Generalized, linear,\nand mixed models. John Wiley & Sons.\n\n\nMcDermott, P. L., & Wikle, C. K. (2016). A model-based approach for\nanalog spatio-temporal dynamic forecasting. Environmetrics,\n27(2), 70–82.\n\n\nMcDermott, P. L., & Wikle, C. K. (2017). An ensemble quadratic echo\nstate network for non-linear spatio-temporal forecasting. Stat,\n6(1), 315–330.\n\n\nMcDermott, P. L., Wikle, C. K., & Millspaugh, J. (2018). A\nhierarchical spatiotemporal analog forecasting model for count data.\nEcology and Evolution, 8(1), 790–800.\n\n\nMicheas, A. C., Fox, N. I., Lack, S. A., & Wikle, C. K. (2007). Cell\nidentification and verification of QPF ensembles using shape analysis\ntechniques. Journal of Hydrology, 343(3-4), 105–116.\n\n\nMilliff, R. F., Bonazzi, A., Wikle, C. K., Pinardi, N., & Berliner,\nL. M. (2011). Ocean ensemble forecasting. Part i: Ensemble mediterranean\nwinds from a bayesian hierarchical model. Quarterly Journal of the\nRoyal Meteorological Society, 137(657), 858–878.\n\n\nMonahan, A. H., Fyfe, J. C., Ambaum, M. H., Stephenson, D. B., &\nNorth, G. R. (2009). Empirical orthogonal functions: The medium is the\nmessage. Journal of Climate, 22(24), 6501–6514.\n\n\nMontero, J.-M., Fernández-Avilés, G., & Mateu, J. (2015).\nSpatial and spatio-temporal geostatistical modeling and\nkriging. John Wiley & Sons.\n\n\nMorrison, P., & Morrison, P. (1982). Powers of ten: About the\nrelative size of things in the universe. Scientific American\nLibrary, distributed by WH Freeman.\n\n\nMurphy, A. H. (1993). What is a good forecast? An essay on the nature of\ngoodness in weather forecasting. Weather and Forecasting,\n8(2), 281–293.\n\n\nNakazono, Y. (2013). Strategic behavior of federal open market committee\nboard members: Evidence from members’ forecasts. Journal of Economic\nBehavior & Organization, 93, 62–70.\n\n\nNCAR – Research Applications Laboratory. (2015). Verification:\nWeather forecast verification utilities.\n\n\nNeuwirth, E. (2014). RColorBrewer: ColorBrewer palettes.\n\n\nNychka, D., Furrer, R., Paige, J., & Sain, S. (2015). Fields:\nTools for spatial data.\n\n\nO’Hara-Wild, M. (2017). Ggquiver: Quiver plots for ’ggplot2’.\n\n\nObled, Ch., & Creutin, J. D. (1986). Some developments in the use of\nempirical orthogonal functions for mapping meteorological fields.\nJournal of Climate and Applied Meteorology, 25(9),\n1189–1204.\n\n\nOverholser, R., & Xu, R. (2014). Effective degrees of freedom and\nits application to conditional AIC for linear mixed-effects\nmodels with correlated error structures. Journal of Multivariate\nAnalysis, 132, 160–170.\n\n\nParadis, E., Claude, J., & Strimmer, K. (2004). APE:\nAnalyses of phylogenetics and evolution in R language.\nBioinformatics, 20, 289–290.\n\n\nPatterson, H. D., & Thompson, R. (1971). Recovery of inter-block\ninformation when block sizes are unequal. Biometrika,\n58(3), 545–554.\n\n\nPebesma, E. (2012). spacetime:\nSpatio-temporal data in R. Journal of Statistical\nSoftware, 51(7), 1–30. http://www.jstatsoft.org/v51/i07/\n\n\nPinheiro, J., Bates, D., DebRoy, S., Sarkar, D., & R Core Team.\n(2017). nlme: Linear and nonlinear mixed\neffects models.\n\n\nPrado, R., & West, M. (2010). Time series: Modeling,\ncomputation, and inference. Chapman & Hall/CRC.\n\n\nR Core Team. (2018). R: A language and environment for\nstatistical computing. R Foundation for Statistical\nComputing. https://www.R-project.org/\n\n\nRasmussen, C. E., & Williams, C. K. I. (2006). Gaussian\nprocesses for machine learning. MIT Press.\n\n\nRESSTE Network et al. (2017). Analyzing spatio-temporal data with r:\nEverything you always wanted to know – but were afraid to ask.\nJournal de La Société Française de Statistique, 158,\n124–158.\n\n\nRobinson, D., & Hayes, A. (2018). Broom: Convert statistical\nanalysis objects into tidy tibbles. https://CRAN.R-project.org/package=broom\n\n\nRue, H., & Held, L. (2005). Gaussian markov random fields:\nTheory and applications. Chapman & Hall/CRC.\n\n\nRue, H., Martino, S., & Chopin, N. (2009). Approximate bayesian\ninference for latent gaussian models by using integrated nested laplace\napproximations. Journal of the Royal Statistical Society, Series\nB, 71(2), 319–392.\n\n\nSarkar, D. (2008). Lattice: Multivariate data visualization with\nr. Springer. http://lmdvr.r-forge.r-project.org\n\n\nSchabenberger, O., & Gotway, C. A. (2005). Statistical methods\nfor spatial data analysis. Chapman & Hall/CRC.\n\n\nScheuerer, M., & Hamill, T. M. (2015). Variogram-based proper\nscoring rules for probabilistic forecasts of multivariate quantities.\nMonthly Weather Review, 143(4), 1321–1334.\n\n\nSchott, J. R. (2017). Matrix analysis for statistics (3rd ed.).\nJohn Wiley & Sons.\n\n\nSearle, S. R. (1982). Matrix algebra useful for statistics.\nJohn Wiley & Sons.\n\n\nShaddick, G., & Zidek, J. V. (2015). Spatio-temporal methods in\nenvironmental epidemiology. Chapman & Hall/CRC.\n\n\nShen, X., Huang, H.-C., & Cressie, N. (2002). Nonparametric\nhypothesis testing for a spatial signal. Journal of the American\nStatistical Association, 97(460), 1122–1140.\n\n\nSherman, M. (2011). Spatial statistics and spatio-temporal data:\nCovariance functions and directional properties. John Wiley &\nSons.\n\n\nShumway, R. H., & Stoffer, D. S. (1982). An approach to time series\nsmoothing and forecasting using the EM algorithm. Journal of Time\nSeries Analysis, 3(4), 253–264.\n\n\nShumway, R. H., & Stoffer, D. S. (2006). Time series analysis\nand its applications with r examples (2nd ed.). Springer.\n\n\nSiegert, S. (2017). SpecsVerification: Forecast verification\nroutines for ensemble forecasts of weather and climate.\n\n\nSimpson, D., Rue, H., Riebler, A., Martins, T. G., & Sørbye, S. H.\n(2017). Penalising model component complexity: A principled, practical\napproach to constructing priors. Statistical Science,\n32(1), 1–28.\n\n\nSpiegelhalter, D. J., Best, N. G., Carlin, B. P., & Linde, A. van\nder. (2002). Bayesian measures of model complexity and fit. Journal\nof the Royal Statistical Society, Series B, 64(4),\n583–639.\n\n\nStanford, J. L., & Ziemke, J. R. (1994). Field (MAP) statistics. In\nJ. Stanford & S. B. Vardeman (Eds.), Statistical methods for\nphysical science (pp. 457–479). Academic Press.\n\n\nStevens, B., Duan, J., McWilliams, J. C., Münnich, M., & Neelin, J.\nD. (2002). Entrainment, rayleigh friction, and boundary layer winds over\nthe tropical pacific. Journal of Climate, 15(1),\n30–44.\n\n\nTobler, W. R. (1970). A computer movie simulating urban growth in the\nDetroit region. Economic Geography,\n46(sup1), 234–240.\n\n\nVer Hoef, J. M., & Boveng, P. L. (2015). Iterating on a single model\nis a viable alternative to multimodel inference. Journal of Wildlife\nManagement, 79(5), 719–729.\n\n\nVerbeke, G., & Molenberghs, G. (2009). Linear mixed models for\nlongitudinal data. Springer.\n\n\nVon Storch, H., & Zwiers, F. W. (2002). Statistical analysis in\nclimate research. Cambridge University Press.\n\n\nWaller, L. A., & Gotway, C. A. (2004). Applied spatial\nstatistics for public health data. John Wiley & Sons.\n\n\nWeigel, A. P. (2012). Ensemble forecasts. In I. T. Jolliffe & D. B.\nStephenson (Eds.), Forecast verification: A practitioner’s guide in\natmospheric science (pp. 141–166). John Wiley & Sons.\n\n\nWickham, H. (2011). The split-apply-combine strategy for data analysis.\nJournal of Statistical Software, 40(1), 1–29. http://www.jstatsoft.org/v40/i01/\n\n\nWickham, H. (2016). ggplot2: Elegant graphics for data analysis\n(2nd ed.). Springer.\n\n\nWickham, H., & Chang, W. (2017). Devtools: Tools to make\ndeveloping r packages easier.\n\n\nWickham, H., Francois, R., Henry, L., & Müller, K. (2017).\nDplyr: A grammar of data manipulation.\n\n\nWickham, H., & Grolemund, G. (2016). R for data science: Import,\ntidy, transform, visualize, and model data. O’Reilly Media.\n\n\nWickham, H., & Henry, L. (2017). Tidyr: Easily tidy data with\n“spread()” and “gather()” functions.\n\n\nWikle, C. K., & Hooten, M. B. (2016). Hierarchical agent-based\nspatio-temporal dynamic models for discrete-valued data. In R. A. Davis,\nS. H. Holan, R. Lund, & N. Ravishanker (Eds.), Handbook of\ndiscrete-valued time series. Chapman & Hall/CRC.\n\n\nWilks, D. S. (2011). Statistical methods in the atmospheric\nsciences (3rd ed.). Academic Press.\n\n\nWood, S. N. (2017). Generalized additive models: An introduction\nwith r (2nd ed.). Chapman & Hall/CRC.\n\n\nWu, C.-T., Gumpertz, M. L., & Boos, D. D. (2001). Comparison of GEE,\nMINQUE, ML, and REML estimating equations for normally distributed data.\nAmerican Statistician, 55(2), 125–130.\n\n\nXie, Y. (2013). animation: An R\npackage for creating animations and demonstrating statistical methods.\nJournal of Statistical Software, 53(1), 1–27. http://www.jstatsoft.org/v53/i01/\n\n\nXie, Y. (2015). Dynamic documents with r and knitr (second).\nChapman & Hall/CRC.\n\n\nXu, K., Wikle, C. K., & Fox, N. I. (2005). A kernel-based\nspatio-temporal dynamical model for nowcasting weather radar\nreflectivities. Journal of the American Statistical\nAssociation, 100(472), 1133–1144.\n\n\nZammit-Mangion, A. (2018a). FRK: Fixed rank kriging.\n\n\nZammit-Mangion, A. (2018b). IDE: Integro-difference equation\nspatio-temporal models.\n\n\nZammit-Mangion, A. (2018c). STRbook: Supplementary package for book\non ST modelling with r.\n\n\nZammit-Mangion, A., & Huang, H.-C. (2015). EFDR: Wavelet-based\nenhanced FDR for signal detection in noisy images. https://CRAN.R-project.org/package=EFDR\n\n\nZeileis, A., & Hothorn, T. (2002). Diagnostic checking in regression\nrelationships. R News, 2(3), 7–10. https://CRAN.R-project.org/doc/Rnews/\n\n\nZhang, J., Craigmile, P. F., & Cressie, N. (2008). Loss function\napproaches to predict a spatial quantile and its exceedance region.\nTechnometrics, 50(2), 216–227.",
    "crumbs": [
      "References"
    ]
  },
  {
    "objectID": "ChapterAppendixA.html",
    "href": "ChapterAppendixA.html",
    "title": "Appendix A — Some Useful Matrix-Algebra Definitions and Properties",
    "section": "",
    "text": "For the sake of completeness, we provide some definitions and properties of vectors and matrices that are needed to understand many of the formulas and equations in this book. Readers who are already familiar with matrix algebra can skip this section. Readers who would like more detail than the bare minimum presented here can find them in books on matrix algebra or multivariate statistics (e.g., Johnson & Wichern (1992), Schott (2017)).\nVectors and matrices. In this book we denote a vector (a column of numbers) by a bold letter (Latin or Greek); for example,\n\\[\n{\\mathbf{a}} = \\left[\\begin{array}{c} a_1 \\\\ a_2\\\\ \\vdots \\\\ a_p \\end{array} \\right]\n\\]\nrepresents a \\(p\\)-dimensional vector, and \\({\\mathbf{a}}' = [a_1, a_2, \\ldots, a_p]\\) or \\((a_1, a_2,\\ldots,a_p)\\) is its \\(p\\)-dimensional transpose.\nWe also denote a matrix (an array of numbers) by bold upper-case letters (Latin or Greek); for example,\n\\[\n{\\mathbf{A}} = \\left[ \\begin{array}{cccc}\na_{11} & a_{12} & \\cdots & a_{1n} \\\\\na_{21} & a_{22} & \\cdots & a_{2n} \\\\\n\\vdots & \\vdots & \\; & \\vdots \\\\\na_{p1} & a_{p2} & \\cdots & a_{pn} \\\\\n\\end{array}\\right]\n\\]\nis a \\(p \\times n\\) matrix, and \\(a_{k \\ell}\\) corresponds to the element in the \\(k\\)th row and \\(\\ell\\)th column; sometimes it is also written as \\(\\{a_{k \\ell}\\}\\). The matrix transpose, \\(\\mathbf{A}'\\), is then an \\(n \\times p\\) matrix given by\n\\[\n{\\mathbf{A}}' = \\left[ \\begin{array}{cccc}\na_{11} & a_{21} & \\cdots & a_{p1} \\\\\na_{12} & a_{22} & \\cdots & a_{p2} \\\\\n\\vdots & \\vdots & \\; & \\vdots \\\\\na_{1n} & a_{2n} & \\cdots & a_{pn} \\\\\n\\end{array}\\right].\n\\]\nWe often consider a special matrix known as the identity matrix, denoted \\(\\mathbf{I}_n\\), which is an \\(n \\times n\\) diagonal matrix with ones along the main diagonal (i.e., \\(a_{ii}=1\\) for \\(i=1,\\ldots,n\\)) and zeros for all of the off-diagonal elements (i.e., \\(a_{ij} = 0, \\mbox{for } i \\neq j\\)). It is sometimes the case that the dimensional subscript (in this case, \\(n\\)) is left off if the context is clear.\nFinally, note that a vector can be thought of as a special case of a \\(p \\times n\\) matrix, where either \\(p=1\\) or \\(n=1\\).\nMatrix addition. Matrix addition is defined for two matrices that have the same dimension. Then, given \\(p \\times n\\) matrices \\(\\mathbf{A}\\) and \\(\\mathbf{B}\\), with elements \\(\\{a_{k \\ell}\\}\\) and \\(\\{b_{k \\ell}\\}\\) for \\(k=1,\\ldots,p\\) and \\(\\ell = 1,\\ldots,n\\), respectively, the elements of the matrix sum, \\(\\mathbf{C}= \\{c_{k \\ell}\\} = \\mathbf{A}+ \\mathbf{B}\\), are given by\n\\[\nc_{k \\ell} = a_{k \\ell} + b_{k \\ell}, \\quad k=1,\\ldots,p;\\ \\ell = 1,\\ldots,n.\n\\]\nScalar multiplication. Consider an arbitrary scalar, \\(c\\), and the \\(p \\times n\\) matrix \\(\\mathbf{A}\\). Scalar multiplication by a matrix then gives a new matrix in which each element of the matrix \\(\\mathbf{A}\\) is multiplied individually by the scalar \\(c\\). Specifically, \\(c \\mathbf{A}= \\mathbf{A}c = \\mathbf{G}\\), where each element of \\(\\mathbf{G}= \\{g_{k \\ell}\\}\\) is given by \\(g_{k \\ell} = c a_{k \\ell}\\), for \\(k=1,\\ldots,p\\) and \\(\\ell = 1,\\ldots,n\\).\nMatrix subtraction. As with matrix addition, matrix subtraction is defined for two matrices that have the same dimension. Consider the two \\(p \\times n\\) matrices \\(\\mathbf{A}\\) and \\(\\mathbf{B}\\), with elements \\(\\{a_{k \\ell}\\}\\) and \\(\\{b_{k \\ell}\\}\\), for \\(k=1,\\ldots,p\\) and \\(\\ell = 1,\\ldots,n\\), respectively. The matrix difference between \\(\\mathbf{A}\\) and \\(\\mathbf{B}\\) is then given by \\[\n\\mathbf{C}= \\{c_{k \\ell}\\} = \\mathbf{A}- \\mathbf{B}= \\mathbf{A}+ (-1)\\mathbf{B},\n\\] where it can be seen that the elements of \\(\\mathbf{C}\\) are given by \\(c_{k \\ell} = a_{k \\ell} - b_{k \\ell}\\), for \\(k=1,\\ldots,p\\) and \\(\\ell = 1,\\ldots,n\\). Thus, matrix subtraction is just a combination of matrix addition and scalar multiplication (by \\(- 1\\)).\nMatrix multiplication. The product of the \\(p \\times n\\) matrix \\(\\mathbf{A}\\) and \\(n \\times m\\) matrix \\(\\mathbf{B}\\) is given by the \\(p \\times m\\) matrix \\(\\mathbf{C}\\), where \\(\\mathbf{C}= \\{c_{kj}\\} = \\mathbf{A}\\mathbf{B}\\), with \\[\nc_{kj} = \\sum_{\\ell = 1}^n a_{k \\ell} b_{\\ell j}, \\quad k=1,\\ldots,p;\\ j=1,\\ldots,m.\n\\] Thus, for the matrix product \\(\\mathbf{A}\\mathbf{B}\\) to exist, the number of columns in \\(\\mathbf{A}\\) must equal the number of rows in \\(\\mathbf{B}\\); so \\(\\mathbf{C}\\) always has the number of rows that are in \\(\\mathbf{A}\\) and the number of columns that are in \\(\\mathbf{B}\\).\nOrthogonal matrix. A square \\(p \\times p\\) matrix \\(\\mathbf{A}\\) is said to be orthogonal if \\(\\mathbf{A}\\mathbf{A}' = \\mathbf{A}' \\mathbf{A}= \\mathbf{I}_p\\).\nVector inner product. As a special case of matrix multiplication, consider two vectors, \\(\\mathbf{a}\\) and \\(\\mathbf{b}\\), both of length \\(p\\). The inner product of \\(\\mathbf{a}\\) and \\(\\mathbf{b}\\) is given by the scalar \\(\\mathbf{a}' \\mathbf{b}= \\mathbf{b}' \\mathbf{a}\\equiv \\sum_{k=1}^p a_k b_k\\).\nVector outer product. For another special case of matrix multiplication, consider a \\(p\\)-dimensional vector \\(\\mathbf{a}\\) and a \\(q\\)-dimensional vector \\(\\mathbf{b}\\). The outer product \\(\\mathbf{a}\\mathbf{b}'\\) is given by the \\(p \\times q\\) matrix \\[\n\\mathbf{a}\\mathbf{b}' \\equiv\n\\left[ \\begin{array}{cccc}\na_{1} b_{1} & a_{1} b_2 & \\cdots & a_1 b_q \\\\\na_{2} b_1 & a_{2} b_2 & \\cdots & a_2 b_q \\\\\n\\vdots & \\vdots & \\; & \\vdots \\\\\na_{p} b_1 & a_{p} b_2 & \\cdots & a_{p} b_q \\\\\n\\end{array}\\right].\n\\] Note that (in general) \\(\\mathbf{a}\\mathbf{b}' \\neq \\mathbf{b}' \\mathbf{a}\\).\nKronecker product. Consider two matrices, an \\(n_a \\times m_a\\) matrix, \\(\\mathbf{A}\\), and an \\(n_b \\times m_b\\) matrix, \\(\\mathbf{B}\\). The Kronecker product of \\(\\mathbf{A}\\) and \\(\\mathbf{B}\\) is given by the \\(n_a n_b \\times m_a m_b\\) matrix \\(\\mathbf{A}\\otimes \\mathbf{B}\\) defined as \\[\n\\mathbf{A}\\otimes \\mathbf{B}\\equiv \\left[\\begin{array}{ccc}\na_{11} \\mathbf{B}& \\cdots & a_{1 m_a} \\mathbf{B}\\\\\n\\vdots & \\vdots & \\vdots \\\\\na_{n_a 1} \\mathbf{B}& \\cdots & a_{n_a m_a} \\mathbf{B}\n\\end{array}\\right].\n\\] If \\(\\mathbf{A}\\) is \\(n_a \\times n_a\\) and \\(\\mathbf{B}\\) is \\(n_b \\times n_b\\), the inverse and determinant of the Kronecker product can be expressed in terms of the Kronecker product of the inverses and determinants of the individual matrices, respectively: \\[\n(\\mathbf{A}\\otimes \\mathbf{B})^{-1} = \\mathbf{A}^{-1} \\otimes \\mathbf{B}^{-1},\n\\] \\[\n|\\mathbf{A}\\otimes \\mathbf{B}| = |\\mathbf{A}|^{n_b} \\;  |\\mathbf{B}|^{n_a}.\n\\]\nEuclidean norm. Consider the \\(p\\)-dimensional real-valued vector \\(\\mathbf{a}= [a_1,a_2,\\ldots,a_p]'\\). The Euclidean norm is simply the Euclidean distance in \\(p\\)-dimensional space, given by\n\\[\n||\\mathbf{a}|| \\equiv  \\sqrt{\\mathbf{a}' \\mathbf{a}} \\equiv \\sqrt{\\sum_{k=1}^p a^2_{k}}.\n\\]\nSymmetric matrix. A matrix \\(\\mathbf{A}\\) is said to be symmetric if \\(\\mathbf{A}' = \\mathbf{A}\\).\nDiagonal matrix. Consider the \\(p \\times p\\) matrix \\(\\mathbf{A}\\). The (main) diagonal elements of this matrix are given by the vector \\([a_{11},a_{22},\\ldots,a_{pp}]'\\). Sometimes it is helpful to use a shorthand notation to construct a matrix with specific elements of a vector on the main diagonal and zeros for all other elements. For example,\n\\[\n\\textrm{diag}(b_1,b_2,\\ldots,b_q) \\equiv\n\\left[ \\begin{array}{ccccc}\nb_{1}  & 0  & 0 & \\cdots & 0\\\\\n  0 & b_2 & 0 &\\cdots & 0 \\\\\n  \\vdots & \\vdots & \\ddots & \\vdots & \\vdots \\\\\n   0 & 0 & 0 &\\cdots &  b_q \\\\\n\\end{array}\\right].\n\\]\nTrace of a matrix. Let \\(\\mathbf{A}\\) be a \\(p \\times p\\) square matrix. We then define the trace of this matrix, denoted \\(\\textrm{trace}(\\mathbf{A})\\) (or \\(\\text{tr}(\\mathbf{A})\\)) as the sum of the diagonal elements of \\(\\mathbf{A}\\); that is,\n\\[\n\\textrm{trace}(\\mathbf{A}) = \\sum_{k=1}^p a_{kk}.\n\\]\nNon-negative-definite and positive-definite matrices. Consider a \\(p \\times p\\) symmetric and real-valued matrix, \\(\\mathbf{A}\\). If, for any non-zero real-valued vector \\(\\mathbf{x}\\), the scalar given by the quadratic form \\(\\mathbf{x}' \\mathbf{A}\\mathbf{x}\\) is non-negative, we say \\(\\mathbf{A}\\) is a non-negative-definite matrix. Similarly, if \\(\\mathbf{x}' \\mathbf{A}\\mathbf{x}\\) is strictly positive for any \\(\\mathbf{x}\\neq {\\mathbf{0}}\\), we say that \\(\\mathbf{A}\\) is a positive-definite matrix.\nMatrix inverse. Consider the \\(p \\times p\\) square matrix, \\(\\mathbf{A}\\). If it exists, the matrix \\(\\mathbf{B}\\) such that \\(\\mathbf{A}\\mathbf{B}= \\mathbf{B}\\mathbf{A}= \\mathbf{I}_p\\) is known as the inverse matrix of \\(\\mathbf{A}\\), and it is denoted by \\(\\mathbf{A}^{-1}\\). Thus, \\(\\mathbf{A}^{-1} \\mathbf{A}= \\mathbf{A}\\mathbf{A}^{-1} = \\mathbf{I}_p\\). If the inverse exists, we say that the matrix is invertible. Not every square matrix has an inverse, but every positive-definite matrix is invertible (and, the inverse matrix is also positive-definite).\nMatrix square root. Let \\(\\mathbf{A}\\) be a \\(p \\times p\\) positive-definite matrix. Then there exists a matrix \\(\\mathbf{B}\\) such that \\(\\mathbf{A}= \\mathbf{B}\\mathbf{B}\\equiv \\mathbf{B}^2\\) and we say that \\(\\mathbf{B}\\) is the matrix square root of \\(\\mathbf{A}\\) and denote it by \\(\\mathbf{A}^{1/2}\\). The matrix square root of a positive-definite matrix is also positive-definite and we can write the inverse matrix as \\(\\mathbf{A}^{-1} = \\mathbf{A}^{-1/2} \\mathbf{A}^{-1/2}\\), where \\(\\mathbf{A}^{-1/2}\\) is the inverse of \\(\\mathbf{A}^{1/2}\\).\nSpectral decomposition. Let \\(\\mathbf{A}\\) be a \\(p \\times p\\) symmetric matrix of real values. This matrix can be decomposed as \\[\n\\mathbf{A}= \\sum_{k=1}^p \\lambda_k \\boldsymbol{\\phi}_k \\boldsymbol{\\phi}_k' = \\boldsymbol{\\Phi}\\boldsymbol{\\Lambda}\\boldsymbol{\\Phi}',\n\\] where \\(\\boldsymbol{\\Lambda}= \\textrm{diag}(\\lambda_1,\\ldots,\\lambda_p)\\), \\(\\boldsymbol{\\Phi}= [\\boldsymbol{\\phi}_1,\\ldots,\\boldsymbol{\\phi}_p]\\), and \\(\\{\\lambda_k\\}\\) are called the eigenvalues that are associated with the eigenvectors, \\(\\{\\boldsymbol{\\phi}_k\\}\\), \\(k=1,\\ldots,p\\), which are orthogonal (i.e., \\(\\boldsymbol{\\Phi}\\boldsymbol{\\Phi}' = \\boldsymbol{\\Phi}' \\boldsymbol{\\Phi}= \\mathbf{I}_p\\)). Note that for a symmetric non-negative-definite matrix \\(\\mathbf{A}\\), \\(\\lambda_k \\ge 0\\), and for a symmetric positive-definite matrix \\(\\mathbf{A}\\), \\(\\lambda_k &gt; 0\\), for all \\(k = 1,\\ldots,p\\). The matrix square root and its inverse can be written as \\(\\mathbf{A}^{1/2} = \\boldsymbol{\\Phi}\\textrm{diag}(\\lambda_1^{1/2},\\ldots,\\lambda_p^{1/2}) \\boldsymbol{\\Phi}'\\) and \\(\\mathbf{A}^{-1/2} = \\boldsymbol{\\Phi}\\textrm{diag}(\\lambda_1^{-1/2},\\ldots,\\lambda_p^{-1/2}) \\boldsymbol{\\Phi}'\\), respectively.\nSingular value decomposition (SVD). Let \\(\\mathbf{A}\\) be a \\(p \\times n\\) matrix of real values. Then the matrix \\(\\mathbf{A}\\) can be decomposed as \\(\\mathbf{A}= \\mathbf{U}\\mathbf{D}\\mathbf{V}'\\), where \\(\\mathbf{U}\\) and \\(\\mathbf{V}\\) are \\(p \\times p\\) and \\(n \\times n\\) orthogonal matrices, respectively. In addition, the \\(p \\times n\\) matrix \\(\\mathbf{D}\\) contains all zeros except for the \\((k,k)\\)th non-negative elements, \\(\\{d_k: \\; k=1,2,\\ldots,\\min(p,n)\\}\\), which are known as singular values.\n\n\n\n\nJohnson, R. A., & Wichern, D. W. (1992). Applied multivariate statistical analysis. In Prentice Hall (3rd ed.). Prentice Hall.\n\n\nSchott, J. R. (2017). Matrix analysis for statistics (3rd ed.). John Wiley & Sons.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>Some Useful Matrix-Algebra Definitions and Properties</span>"
    ]
  },
  {
    "objectID": "ChapterAppendixB.html",
    "href": "ChapterAppendixB.html",
    "title": "Appendix B — General Smoothing Kernels",
    "section": "",
    "text": "Consider data \\(\\{Z_i: i=1,\\ldots,m\\}\\), which we can write as a vector, \\(\\mathbf{Z}= (Z_1,\\dots,Z_m)'\\). Now, a homogeneously linear (smoothing) predictor for \\(\\mathbf{Z}\\) can always be written as \\(\\widehat{\\mathbf{Z}} = \\mathbf{H}\\mathbf{Z}\\), where the \\(i\\)th row of the \\(m \\times m\\) matrix \\(\\mathbf{H}\\), sometimes referred to as the influence matrix, corresponds to smoothing weights for the prediction, \\(\\widehat{Z_i}\\); that is,\n\\[\n\\widehat{Z}_i = \\sum_{j=1}^m h_{ij} Z_j,\n\\]\nwhere \\(h_{ij}\\) corresponds to the \\((i,j)\\)th element of \\(\\mathbf{H}\\) and, by definition, the elements of \\(\\mathbf{H}\\) do not depend on \\(\\mathbf{Z}\\). Note that both the kernel and regression predictors given in Section 3.1 and Section 3.2, respectively, are linear predictors of this form. In the case of the kernel predictors, \\(h_{ij}\\) corresponds to the kernel evaluated at location \\(i\\) and \\(j\\). For the regression case, \\(\\mathbf{H}= \\mathbf{X}(\\mathbf{X}' \\mathbf{X})^{-1} \\mathbf{X}'\\) (sometimes called the “hat” matrix in books on regression). The difference is that, in general, under a kernel model, \\(\\mathbf{H}\\) gives more weight to locations that are near to each other, whereas standard regression matrices do not necessarily do so, although so-called local linear regression approaches do (see, for example, James et al. (2013)).\nThere are several useful properties of the general linear smoothing matrix, \\(\\mathbf{H}\\), used in the linear predictor. First, as we have noted, if one has \\(m\\) observations but they are statistically dependent, then there are effectively fewer than \\(m\\) degrees of freedom (e.g., some of the information is redundant due to the dependence). Specifically, the effective degrees of freedom in the sample of \\(m\\) observations are given by the trace of the matrix \\(\\mathbf{H}\\),\n\\[\ndf_{\\mathrm{eff}} = \\textrm{tr}(\\mathbf{H}) = \\sum_{i=1}^m h_{ii}.\n\\]\nAnother important property of linear predictors of this form is that we can obtain the LOOCV estimate (see Note 3.1) without actually having to refit the model. That is, in the case of evaluating the MSPE, the LOOCV statistic is given by\n\\[\nCV_{(m)} = \\frac{1}{m} \\sum_{i=1}^m (Z_i - \\widehat{Z}_i^{(-i)})^2 = \\frac{1}{m} \\sum_{i=1}^m \\left(\\frac{Z_i - \\widehat{Z}_i}{1 - h_{ii}}  \\right)^2,\n\\tag{B.1}\\]\nand the so-called generalized cross-validation statistic is given by replacing the denominator in the right-hand side of Equation B.1 by \\((1 - \\textrm{tr}(\\mathbf{H})/m)\\).\nIn cases where regularization is considered in the context of the linear predictor (e.g., when we wish to shrink the parameters toward zero by using, for example, a ridge regression (\\(L_2\\)-norm) penalty; see Note 3.4), we can write \\(\\mathbf{H}= \\mathbf{X}(\\mathbf{X}' \\mathbf{X}+ \\mathbf{R})^{-1} \\mathbf{X}'\\) (with \\(\\mathbf{R}= \\lambda \\mathbf{I}\\) in the ridge-regression case), and the effective degrees of freedom and LOOCV properties are still valid (see James et al., 2013). As discussed in Note 3.4, a lasso (\\(L_1\\)-norm) penalty can also be used for regularization, but the smoothing kernel has no closed form in this case.\n\n\n\n\nJames, G., Witten, D., Hastie, T., & Tibshirani, R. (2013). An introduction to statistical learning. Springer.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>B</span>  <span class='chapter-title'>General Smoothing\\index{smoothing} Kernels</span>"
    ]
  },
  {
    "objectID": "ChapterAppendixC.html",
    "href": "ChapterAppendixC.html",
    "title": "Appendix C — Estimation and Prediction for Dynamic Spatio-Temporal Models",
    "section": "",
    "text": "C.1 Estimation in Vector Autoregressive Spatio-Temporal Models via the Method of Moments\nEstimation and prediction for linear dynamic spatio-temporal models (DSTMs) with Gaussian errors can sometimes be done using methods developed for state-space models (when there are many more temporal observations than spatial locations). In particular, after conditioning on parameter estimates, the hidden (state) process can be predicted using a Kalman filter or smoother, and the parameters might be estimated using an expectation-maximization (EM) algorithm or a Markov chain Monte Carlo (MCMC) algorithm. This appendix illustrates, first, a method-of-moments estimation approach that is common in vector autoregression modeling in time series, and second, a detailed description of parameter estimation and prediction of the process in linear DSTMs with Gaussian errors using the Kalman filter, Kalman smoother, and the EM algorithm.\nIn traditional vector autoregressive (VAR) time-series applications, the autoregressive process is assumed to correspond directly to the data-generating process (i.e., there is no separate data model and process model). In the spatio-temporal context this implies a model such as\n\\[\n\\mathbf{Z}_t = \\mathbf{M}\\mathbf{Z}_{t-1} + \\boldsymbol{\\eta}_t,\\quad \\boldsymbol{\\eta}_t \\sim \\; Gau(\\mathbf{0},\\mathbf{C}_{\\eta}),\n\\tag{C.1}\\]\nfor \\(t=1,\\ldots,T\\), where we assume that \\(\\mathbf{Z}_0\\) is known and recall that \\(\\mathbf{Z}_t = (Z_t(\\mathbf{s}_1),\\ldots,Z_t(\\mathbf{s}_m))'\\). Estimation of the matrices \\(\\mathbf{M}\\) and \\(\\mathbf{C}_{\\eta}\\) can be obtained via maximum likelihood, least squares, or the method of moments (see Lütkepohl (2005), Chapter 3). We illustrate the latter here.\nFor simplicity, we assume \\(\\{\\mathbf{Z}_t\\}\\) has mean zero and is second-order stationary in time. If we post-multiply both sides of Equation C.1 by \\(\\mathbf{Z}'_{t-1}\\) and take the expectation, we get,\n\\[\nE(\\mathbf{Z}_t \\mathbf{Z}'_{t-1}) =  \\mathbf{M}E(\\mathbf{Z}_{t-1} \\mathbf{Z}'_{t-1}),\n\\]\nwhich we write as\n\\[\n\\mathbf{C}_z^{(1)}  =  \\mathbf{M}\\mathbf{C}_z^{(0)}.\n\\tag{C.2}\\]\nRecall from Chapter 2 that \\(\\mathbf{C}_z^{(\\tau)}\\) is the lag-\\(\\tau\\) spatial covariance matrix for \\(\\{\\mathbf{Z}_t\\}\\). Now, Equation C.2 implies that\n\\[\n\\mathbf{M}= \\mathbf{C}_z^{(1)} (\\mathbf{C}_z^{(0)})^{-1}.\n\\tag{C.3}\\]\nSimilarly, if we post-multiply Equation C.1 by \\(\\mathbf{Z}'_t\\) and take expectations, we can show that\n\\[\n\\mathbf{C}_{\\eta} = \\mathbf{C}_z^{(0)} - \\mathbf{M}\\mathbf{C}_z^{(1)'} = \\mathbf{C}_z^{(0)} - \\mathbf{C}_z^{(1)} (\\mathbf{C}_z^{(0)})^{-1} \\mathbf{C}_z^{(1)'}.\n\\tag{C.4}\\]\nIt follows that the method-of-moments estimators (where empirical moments are equated with theoretical moments) of Equation C.3 and Equation C.4 are given by\n\\[\n\\widehat{\\mathbf{M}} =  \\widehat{\\mathbf{C}}_z^{(1)} (\\widehat{\\mathbf{C}}_z^{(0)})^{-1},\n\\tag{C.5}\\]\n\\[\n\\widehat{\\mathbf{C}}_\\eta =  \\widehat{\\mathbf{C}}_z^{(0)} - \\widehat{\\mathbf{C}}_z^{(1)} (\\widehat{\\mathbf{C}}_z^{(0)})^{-1} \\widehat{\\mathbf{C}}_z^{(1)'}.\n\\tag{C.6}\\]\nIn Equation C.6, the empirical lag-\\(\\tau\\) covariance matrices, \\(\\widehat{\\mathbf{C}}_z^{(\\tau)}\\), are calculated as shown in Equation 2.4. Note that \\(T\\) needs to be larger than the dimension of \\(\\mathbf{Z}_t\\) to ensure that \\(\\widehat{\\mathbf{C}}_z^{(0)}\\) is invertible.\nAs we have said throughout this book, we prefer to consider DSTMs that have a separate data and process model. Estimation for these models is described below in Section C.2. So, what is the benefit of the method-of-moments approach in the context of DSTMs? In cases where the signal-to-noise ratio is high, the estimates given by Equation C.5 and Equation C.6 can provide reasonable estimates for exploratory data analysis. We illustrate an example using method-of-moments estimation in Lab 5.3. Specifically, assume that we project the spatial-mean-centered data onto orthogonal basis functions, \\(\\boldsymbol{\\Phi}\\): \\(\\boldsymbol{\\alpha}_t = \\boldsymbol{\\Phi}' (\\mathbf{Z}_t - \\widehat{\\boldsymbol{\\mu}})\\). We then assume that the projected data come from the model, \\(\\boldsymbol{\\alpha}_t = \\mathbf{M}\\boldsymbol{\\alpha}_{t-\\tau} + \\boldsymbol{\\eta}_t\\), and we obtain estimates \\(\\widehat{\\mathbf{M}}\\) and \\(\\widehat{\\mathbf{C}}_\\eta\\) based on the projected data. One can then produce forecasts such as \\(\\widehat{\\boldsymbol{\\alpha}}_{T+\\tau} = \\widehat{\\mathbf{M}} \\widehat{\\boldsymbol{\\alpha}}_T\\), with estimated forecast covariance matrix, \\(\\widehat{\\mathbf{C}}_{\\alpha} = \\widehat{\\mathbf{M}} \\widehat{\\mathbf{C}}_\\alpha^{(0)} \\widehat{\\mathbf{M}}' + \\widehat{\\mathbf{C}}_\\eta\\), where \\(\\widehat{\\mathbf{C}}_\\alpha^{(0)}\\) is the empirical estimate of \\(E(\\boldsymbol{\\alpha}_t \\boldsymbol{\\alpha}_t')\\). To obtain a forecast for \\(\\widehat{\\mathbf{Z}}_{T+\\tau}\\), one would have to multiply the forecast \\(\\widehat{\\boldsymbol{\\alpha}}_{T+\\tau}\\) by the basis-function matrix and add back the spatial mean: \\(\\widehat{\\mathbf{Z}}_{T+\\tau} = \\widehat{\\boldsymbol{\\mu}} + \\boldsymbol{\\Phi}\\widehat{\\boldsymbol{\\alpha}}_{T+\\tau}\\). The forecast covariance matrix is then approximated by \\(\\widehat{\\mathbf{C}}_{Z} = \\boldsymbol{\\Phi}\\widehat{\\mathbf{C}}_{\\alpha} \\boldsymbol{\\Phi}'\\), where we have ignored the truncation and measurement error when projecting onto the basis functions. Although this procedure is somewhat ad hoc, it is simple and can give a quick forecast. More importantly, the parameter estimates in this procedure would be used as starting values in the state-space EM algorithm described in Section C.2. This is demonstrated in the second portion of Lab 5.3.\nFor completeness, note that when one makes the assumption that the initial spatial data vector \\(\\mathbf{Z}_0\\) is known, it can be shown that, conditional on \\(\\mathbf{Z}_0\\), maximum likelihood, least squares, and method-of-moments estimation all give equivalent estimates, \\(\\widehat{\\mathbf{M}}\\) and \\(\\widehat{\\mathbf{C}}_\\eta\\) (see, for example, Harvey, 1993, Section 7.4).",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>C</span>  <span class='chapter-title'>Estimation and Prediction for Dynamic Spatio-Temporal Models</span>"
    ]
  },
  {
    "objectID": "ChapterAppendixC.html#sec-estimationLDSTM",
    "href": "ChapterAppendixC.html#sec-estimationLDSTM",
    "title": "Appendix C — Estimation and Prediction for Dynamic Spatio-Temporal Models",
    "section": "C.2 Prediction and Estimation in Fully Parameterized Linear DSTMs",
    "text": "C.2 Prediction and Estimation in Fully Parameterized Linear DSTMs\nTraditionally, from the data model,\n\\[\n\\mathbf{Z}_t =  \\mathbf{H}_t \\mathbf{Y}_t + \\boldsymbol{\\varepsilon}_t,\\quad \\boldsymbol{\\varepsilon}_t \\sim \\; Gau(\\mathbf{0},\\mathbf{C}_{\\epsilon,t}),\n\\tag{C.7}\\]\nand from the process model,\n\\[\n\\mathbf{Y}_t = \\mathbf{M}\\mathbf{Y}_{t-1} + \\boldsymbol{\\eta}_t,\\quad \\boldsymbol{\\eta}_t \\sim \\; Gau(\\mathbf{0},\\mathbf{C}_{\\eta}),\n\\tag{C.8}\\]\nwe obtain a hierarchical model (HM). Note that we have assumed here that there is no additive offset in the data model and that the process has mean zero to simplify the exposition. Next, we can perform prediction on the hidden process via the Kalman filter and Kalman smoother if the parameter matrices are all known. In practice, these are not known, and estimates are sometimes used in their place, which is an empirical hierarchical model (EHM) approach. Note that although in general \\(\\mathbf{M}\\) could depend on time (and hence would be written as \\(\\mathbf{M}_t\\)), we consider the simpler time-invariant case here.\n\nSequential Prediction of the Process via Kalman Filtering and Smoothing\nIn Chapter 1 we discussed the notions of smoothing, filtering, and forecasting. Before we show the filtering and smoothing distributions and algorithms, we need to define some notation and terms. In particular, let \\(\\mathbf{w}_{c:d} \\equiv \\{\\mathbf{w}_c,\\ldots,\\mathbf{w}_d\\}\\), for the generic vector \\(\\mathbf{w}_t\\) at times \\(t \\in \\{c, c+1,\\ldots,d-1,d\\}\\). Then we define the forecasting distribution to be the distribution of \\(\\mathbf{Y}_t\\) given all of the observations that occur before time \\(t\\), namely, \\([\\mathbf{Y}_t | \\mathbf{Z}_{1:t-1}]\\). We also define the filtering distribution to be the distribution of \\(\\mathbf{Y}_t\\) given all of the observations up to and including time \\(t\\), namely, \\([\\mathbf{Y}_t | \\mathbf{Z}_{1:t}]\\). Finally, we define the smoothing distribution to be the distribution of \\(\\mathbf{Y}_t\\) given all the observations before, including, and after time \\(t\\), namely, \\([\\mathbf{Y}_t | \\mathbf{Z}_{1:T}]\\), for \\(1 \\leq t \\leq T\\).\nThe forecasting distribution is of most interest when one would like to predict the process one time step into the future; the filtering distribution is typically most useful when one seeks to “filter out” observation error from the true process as data come along sequentially (e.g., in real time); and the smoothing distribution is most useful when one retrospectively wants to smooth out the observation errors for any time in the entire observation period. Now, consider the following notation for the conditional expectations of the forecast and filtering distributions, respectively: \\(\\mathbf{Y}_{t | t-1} \\equiv E[\\mathbf{Y}_t | \\mathbf{Z}_{1:t-1}]\\) and \\(\\mathbf{Y}_{t:t} \\equiv E[\\mathbf{Y}_t | \\mathbf{Z}_{1:t}]\\). Similarly, define the conditional covariance matrices for the forecast error and filtering error distributions, respectively, as: \\(\\mathbf{P}_{t|t-1} \\equiv E[(\\mathbf{Y}_t - \\mathbf{Y}_{t|t-1})(\\mathbf{Y}_t - \\mathbf{Y}_{t|t-1})' | \\mathbf{Z}_{1:t-1}]\\) and \\(\\mathbf{P}_{t|t} \\equiv E[(\\mathbf{Y}_t - \\mathbf{Y}_{t|t})(\\mathbf{Y}_t - \\mathbf{Y}_{t|t})' | \\mathbf{Z}_{1:t}]\\).\nIn the case of linear Gaussian data models and process models given by Equation C.7 and Equation C.8, the forecast and filtering distributions can be found analytically by using standard conditional expectation/variance relationships and Bayes’ Rule, respectively. In particular, the forecast and filtering distributions are denoted, respectively, by\n\\[\n\\mathbf{Y}_t | \\mathbf{Z}_{1:t-1} \\sim \\; Gau(\\mathbf{Y}_{t | t-1},\\mathbf{P}_{t | t-1}),\n\\]\nand\n\\[\n\\mathbf{Y}_t | \\mathbf{Z}_{1:t} \\sim \\; Gau(\\mathbf{Y}_{t | t},\\mathbf{P}_{t | t}),\n\\]\nand they can be found through the famous Kalman filter algorithm given in Note C.1. Thus, given the initial conditions \\(\\mathbf{Y}_{0 | 0} \\equiv \\boldsymbol{\\mu}_0\\) and \\(\\mathbf{P}_{0 | 0} \\equiv \\mathbf{C}_0\\) and the parameter matrices, \\(\\{\\mathbf{H}_t\\}_{t=1}^T\\), \\(\\{\\mathbf{C}_{\\epsilon,t}\\}_{t=1}^T\\), \\(\\mathbf{M}\\), and \\(\\mathbf{C}_{\\eta}\\), one can iterate sequentially between the forecast and filtering steps to obtain these distributions for all times \\(t=1,\\ldots,T\\).\n\n\n\n\n\n\nNote C.1: Kalman Filter\n\n\n\n\nSet initial conditions: \\(\\mathbf{Y}_{0 | 0} = \\boldsymbol{\\mu}_0\\) and \\(\\mathbf{P}_{0 | 0} = \\mathbf{C}_0\\)\nfor \\(t = 1\\) to \\(T\\) do\n\nForecast distribution step:\n\nObtain \\(\\mathbf{Y}_{t | t-1} =  \\mathbf{M}\\mathbf{Y}_{t-1 | t-1}\\)\nObtain \\(\\mathbf{P}_{t | t-1} = \\mathbf{C}_{\\eta} + \\mathbf{M}\\mathbf{P}_{t-1 | t-1} \\mathbf{M}'\\)\n\nFiltering distribution step:\n\nObtain the Kalman gain, \\(\\mathbf{K}_t \\equiv \\mathbf{P}_{t | t-1} \\mathbf{H}'_t (\\mathbf{H}_t \\mathbf{P}_{t | t-1} \\mathbf{H}'_t + \\mathbf{C}_{\\epsilon,t})^{-1}\\)\nObtain \\(\\mathbf{Y}_{t|t} = \\mathbf{Y}_{t | t-1} + \\mathbf{K}_t (\\mathbf{Z}_t - \\mathbf{H}_t \\mathbf{Y}_{t | t-1})\\)\nObtain \\(\\mathbf{P}_{t | t} = (\\mathbf{I}- \\mathbf{K}_t \\mathbf{H}_t)\\mathbf{P}_{t | t-1}\\)\n\n\nend for\n\n\n\nRecall that the smoothing distribution considers the distribution of the process at time \\(t\\) given all of the observations regardless of whether they come before, during, or after time \\(t\\). This smoothing distribution is denoted by\n\\[\n\\mathbf{Y}_t | \\mathbf{Z}_{1:T} \\sim \\; Gau(\\mathbf{Y}_{t | T}, \\mathbf{P}_{t | T})\n\\]\nand, if one saves the results from the Kalman filter, this can be obtained for all \\(t\\) by the Kalman smoother algorithm (also known as the Rauch–Tung–Striebel smoother) given in Note C.2.\n\n\n\n\n\n\nNote C.2: Kalman Smoother\n\n\n\n\nObtain \\(\\{\\mathbf{Y}_{t|t-1}, \\mathbf{P}_{t|t-1}\\}_{t=1}^T\\) and \\(\\{\\mathbf{Y}_{t|t}, \\mathbf{P}_{t|t}\\}_{t=0}^T\\) from the Kalman filter algorithm (Note C.1).\nfor \\(t = T-1\\) down to \\(0\\) do\n\nObtain \\(\\mathbf{J}_t \\equiv \\mathbf{P}_{t|t} \\; \\mathbf{M}' \\; \\mathbf{P}_{t+1|t}^{-1}\\).\nObtain \\(\\mathbf{Y}_{t|T} = \\mathbf{Y}_{t|t} + \\mathbf{J}_t (\\mathbf{Y}_{t+1|T} - \\mathbf{Y}_{t+1|t})\\).\nObtain \\(\\mathbf{P}_{t|T} = \\mathbf{P}_{t|t} + \\mathbf{J}_t (\\mathbf{P}_{t+1|T} - \\mathbf{P}_{t+1 | t})\\mathbf{J}'_t\\).\n\nend for\n\n\n\n\n\nParameter Estimation via the EM Algorithm\nThe state-space approach discussed above in terms of the Kalman filter and smoother makes the assumption that the parameter matrices in the data and process models are known. This is unrealistic in most cases, and one must use the data to estimate these parameters; that is, the HM being used is an EHM. One of the most popular (and effective) ways to do this in the state-space time-series case is through the EM algorithm (recall the general EM algorithm presented in Note 4.4).\nThe state-space version of the EM algorithm, originally developed by Shumway & Stoffer (1982), denotes by \\(\\mathbf{Z}_{1:T}\\) the observations and the unobservable latent process, and by \\(\\mathbf{Y}_{0:T}\\) the “missing data.” Denote the parameters by \\(\\boldsymbol{\\Theta}\\equiv \\{\\boldsymbol{\\mu}_0, \\mathbf{C}_0, \\mathbf{C}_\\eta, \\mathbf{C}_{\\epsilon},\\mathbf{M}\\}\\), where we assume typically that the observation matrices, \\(\\{\\mathbf{H}_t\\}\\), are all known. We assume that the initial distribution is given by \\(\\mathbf{Y}_{0|0} \\sim \\; Gau(\\boldsymbol{\\mu}_0, \\mathbf{C}_0)\\), and we further assume here (for simplicity) that \\(\\mathbf{C}_{\\epsilon}\\) corresponds to the \\(m \\times m\\) measurement-error covariance matrix for all possible observation locations (thus, \\(\\mathbf{C}_{\\epsilon,t} = \\mathbf{C}_{\\epsilon}\\), for all \\(t\\), so \\(m_t = m\\) and we assume no missing observations at each time point). The EM algorithm is then based on the complete-data likelihood given by \\([\\mathbf{Z}_{1:T}, \\mathbf{Y}_{0:T} | \\boldsymbol{\\Theta}] = \\left(\\prod_{t=1}^T [\\mathbf{Z}_t | \\mathbf{Y}_t]\\right) \\left(\\prod_{t=1}^T [\\mathbf{Y}_t | \\mathbf{Y}_{t-1}]\\right) [\\mathbf{Y}_0]\\), which again makes use of the conditional independencies in the data model and the Markov property of the process model. The EM algorithm for a linear DSTM, presented in Note C.3, makes use of the Kalman smoother algorithm to evaluate both the E-step and the M-step. Note that, in addition to running the Kalman smoother at each iteration of the algorithm, we also have to obtain the so-called “lagged-one smoother” variance-covariance matrix, \\(\\mathbf{P}_{t,t-1|T} \\equiv E((\\mathbf{Y}_t - \\mathbf{Y}_{t|T})(\\mathbf{Y}_{t-1} - \\mathbf{Y}_{t-1|T})' | \\mathbf{Z}_{1:T})\\), for \\(t=T,T-1,\\ldots.\\) This is accomplished by the so-called lag-one covariance smoother, which is part of the algorithm in Note C.3. Convergence can be assessed by considering parameter changes and/or changes to the log complete-data likelihood (i.e., see Equation C.9 in Note C.3). Typically, in the linear DSTM case, one considers the latter because there are a large number of parameters. An example of using the EM algorithm for a linear DSTM is given in Lab 5.3.\n\n\n\n\n\n\nNote C.3: Linear DSTM EM Algorithm\n\n\n\n\nChoose initial condition covariance matrix, \\(\\mathbf{C}_0\\)\nChoose starting values: \\(\\widehat{\\boldsymbol{\\Theta}}^{(0)} =\\{\\widehat{\\boldsymbol{\\mu}}_0^{(0)}, \\widehat{\\mathbf{C}}_{\\eta}^{(0)}, \\widehat{\\mathbf{C}}_{\\epsilon}^{(0)}, \\widehat{\\mathbf{M}}^{(0)} \\}\\)\nrepeat \\(i=1,2,\\ldots\\)\n\n\nE-step:\n\nUse \\(\\widehat{\\boldsymbol{\\Theta}}^{(i-1)}\\) in the Kalman smoother (Note C.2) to obtain \\(\\{\\mathbf{Y}_{t|T}^{(i-1)}, \\mathbf{P}_{t|T}^{(i-1)}\\}\\)\nUse Kalman smoother output to obtain the lag-one covariance smoother estimates\nCalculate \\(\\mathbf{P}^{(i-1)}_{T,T-1|T} = (\\mathbf{I}- \\mathbf{K}^{(i-1)}_T \\mathbf{H}_T)\\mathbf{M}^{(i-1)} \\mathbf{P}^{(i-1)}_{T-1|T-1}\\)\nfor \\(t=T,T-1,\\ldots,2\\) do \\[\n\\mathbf{P}^{(i-1)}_{t-1,t-2|T}  =  \\mathbf{P}^{(i-1)}_{t-1|t-1} \\mathbf{J}^{(i-1)'}_{t-2} + \\mathbf{J}^{(i-1)'}_{t-1}(\\mathbf{P}^{(i-1)}_{t,t-1|T} - \\mathbf{M}^{(i-1)} \\mathbf{P}^{(i-1)}_{t-1|t-1})\\mathbf{J}^{(i-1)'}_{t-2}\n\\]\nend for\nCalculate \\(\\mathbf{S}_{00} \\equiv \\sum_{t=1}^T (\\mathbf{P}^{(i-1)}_{t-1 | T} + \\mathbf{Y}^{(i-1)}_{t-1|T} \\mathbf{Y}^{(i-1)'}_{t-1|T})\\)\nCalculate \\(\\mathbf{S}_{11} \\equiv \\sum_{t=1}^T (\\mathbf{P}^{(i-1)}_{t | T} + \\mathbf{Y}^{(i-1)}_{t|T} \\mathbf{Y}^{(i-1)'}_{t|T})\\)\nCalculate \\(\\mathbf{S}_{10} \\equiv \\sum_{t=1}^T (\\mathbf{P}^{(i-1)}_{t,t-1 | T} + \\mathbf{Y}^{(i-1)}_{t|T} \\mathbf{Y}^{(i-1)'}_{t-1|T})\\)\n\nM-step:\n\nUpdate: \\(\\widehat{\\boldsymbol{\\mu}}_0^{(i)} = \\mathbf{Y}^{(i-1)}_{0|T}\\)\nUpdate: \\(\\widehat{\\mathbf{M}}^{(i)} = \\mathbf{S}_{10}\\mathbf{S}_{00}^{-1}\\)\nUpdate: \\(\\widehat{\\mathbf{C}}_{\\eta}^{(i)} = (1/T)(\\mathbf{S}_{11} - \\mathbf{S}_{10}\\mathbf{S}_{00}^{-1} \\mathbf{S}_{10}')\\)\nUpdate: \\[\n\\widehat{\\mathbf{C}}_{\\epsilon}^{(i)}  =  \\frac1T \\sum_{t=1}^T ((\\mathbf{Z}_t - \\mathbf{H}_t \\mathbf{Y}^{(i-1)}_{t|T})(\\mathbf{Z}_t - \\mathbf{H}_t \\mathbf{Y}^{(i-1)}_{t|T})' + \\mathbf{H}_t \\mathbf{P}^{(i-1)}_{t|T} \\mathbf{H}'_t)\n\\]\n\nuntil convergence (typically, based on differences in \\(-2\\ln(L(\\boldsymbol{\\Theta}| \\mathbf{Z}_{1:T},\\mathbf{Y}_{0:T}))\\) as calculated in Equation C.9:\n\n\\[\n\\small\n\\begin{aligned}\n-2 \\ln(L(\\boldsymbol{\\Theta}^{(i)} | \\mathbf{Z}_{1:T},\\mathbf{Y}^{(i)}_{0:T}))  =  \\ln(|\\widehat{\\mathbf{C}}^{(i)}_0|) + (\\mathbf{Y}^{(i)}_{0|T} - \\widehat{\\boldsymbol{\\mu}}^{(i)}_0)'\\widehat{\\mathbf{C}}_0^{-1{(i)}}(\\mathbf{Y}^{(i)}_{0|T} - \\widehat{\\boldsymbol{\\mu}}^{(i)}_0) \\\\\n+  T \\ln(|\\widehat{\\mathbf{C}}^{(i)}_{\\eta}|) + \\sum_{t=1}^T (\\mathbf{Y}^{(i)}_{t|T} - \\widehat{\\mathbf{M}}^{(i)} \\mathbf{Y}^{(i)}_{t-1|T})'\\widehat{\\mathbf{C}}_{\\eta}^{-1{(i)}}(\\mathbf{Y}^{(i)}_{t|T} - \\widehat{\\mathbf{M}}^{(i)} \\mathbf{Y}^{(i)}_{t-1|T}) \\\\\n  +   T \\ln(|\\widehat{\\mathbf{C}}^{(i)}_{\\epsilon}|) + \\sum_{t=1}^T (\\mathbf{Z}_t - \\mathbf{H}_t \\mathbf{Y}^{(i)}_{t|T})'\\widehat{\\mathbf{C}}_{\\epsilon}^{-1{(i)}}(\\mathbf{Z}_t - \\mathbf{H}_t \\mathbf{Y}^{(i)}_{t|T}).\n\\end{aligned}\n\\tag{C.9}\\]\n\n\nUncertainty estimates are less easily obtained for the parameter estimates than they are for the state-process estimates, but they can be obtained through considering the inverse of the associated asymptotic information matrix or by parametric bootstrap methods. Unfortunately, obtaining uncertainty estimates even for the state-process estimates is not often done in practice and, as discussed in the comments motivating DSTMs in Section 5.2.3, it can be problematic because of the potential for explosive behavior by some of the transition matrices whose parameters are within the joint confidence region.\nMore flexible inference for DSTMs can be accomplished by the fully hierarchical Bayesian hierarchical model (BHM); see Section 4.5.2 as well as (Cressie & Wikle, 2011, Chapter 8). These BHM implementations are often problem-specific, and they are often best implemented directly in R or in a so-called probabilistic programming language (e.g., Stan, WinBugs, JAGS). For an example, see the Gibbs sampler MCMC algorithm (corresponding to a BHM) to predict Mediterranean surface winds implemented in Appendix E.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>C</span>  <span class='chapter-title'>Estimation and Prediction for Dynamic Spatio-Temporal Models</span>"
    ]
  },
  {
    "objectID": "ChapterAppendixC.html#sec-Est_and_pred",
    "href": "ChapterAppendixC.html#sec-Est_and_pred",
    "title": "Appendix C — Estimation and Prediction for Dynamic Spatio-Temporal Models",
    "section": "C.3 Estimation for Non-Gaussian and Nonlinear DSTMs",
    "text": "C.3 Estimation for Non-Gaussian and Nonlinear DSTMs\nIn principle, the filtering and smoothing methods presented in Section C.2 can be generalized to the setting of non-Gaussian and nonlinear DSTMs (e.g., particle filters and smoothers, ensemble Kalman filters; see Chapter 8 of Cressie & Wikle (2011)). However, in the high-dimensional settings with deep BHMs with complicated parameter-dependence structures, one typically has to consider fully Bayesian implementations. As mentioned above, these implementations are often programmed “from scratch” rather than from particular R packages. As an example, see the BHM based on a linear DSTM with Gaussian error given in Appendix E.\n\n\n\n\nCressie, N., & Wikle, C. K. (2011). Statistics for spatio-temporal data. John Wiley & Sons.\n\n\nHarvey, A. C. (1993). Time series models (2nd ed.). MIT Press.\n\n\nLütkepohl, H. (2005). New introduction to multiple time series analysis. Springer.\n\n\nShumway, R. H., & Stoffer, D. S. (1982). An approach to time series smoothing and forecasting using the EM algorithm. Journal of Time Series Analysis, 3(4), 253–264.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>C</span>  <span class='chapter-title'>Estimation and Prediction for Dynamic Spatio-Temporal Models</span>"
    ]
  },
  {
    "objectID": "ChapterAppendixD.html",
    "href": "ChapterAppendixD.html",
    "title": "Appendix D — Mechanistically Motivated Dynamic Spatio-Temporal Models",
    "section": "",
    "text": "D.1 Example of a Process Model Motivated by a PDE: Finite Differences\nAs discussed in Section 5.3, it can be quite useful to parameterize DSTMs by considering transition matrices that are motivated by a mechanistic model. Here, we show the details of how one can do this with a partial differential equation (PDE) and an integro-difference equation (IDE).\nConsider the case where the parameters \\(a\\), \\(b\\), \\(u\\), and \\(v\\) in Equation 5.14 vary with space and denote the two-dimensional spatial location by the vector \\(\\mathbf{s}= (x,y)'\\). Then the PDE is\n\\[\n\\small\n\\frac{\\partial Y}{\\partial t} = \\frac{\\partial}{\\partial x}\\left(a(x,y) \\frac{\\partial Y}{\\partial x}\\right) + \\frac{\\partial}{\\partial y}\\left(b(x,y) \\frac{\\partial Y}{\\partial y}\\right) + u(x,y) \\frac{\\partial Y}{\\partial x} + v(x,y) \\frac{\\partial Y}{\\partial y}.\n\\tag{D.1}\\]\nIf we consider this process on a regular two-dimensional grid and employ a standard centered finite difference in space and a forward difference in time for Equation D.1, we obtain a lagged nearest-neighbor relationship given by\n\\[\n\\small\n\\begin{aligned}\nY_t(x,y) =~~  &\\theta_{p,1}(x,y) Y_{t-\\Delta_t}(x,y) + \\theta_{p,2}(x,y) Y_{t-\\Delta_t}(x+\\Delta_x,y) \\nonumber \\\\\n&\\; + \\; \\theta_{p,3}(x,y) Y_{t-\\Delta_t}(x-\\Delta_x,y)   + \\theta_{p,4}(x,y) Y_{t-\\Delta_t}(x,y + \\Delta_y) \\nonumber \\\\\n&\\; + \\; \\theta_{p,5}(x,y) Y_{t-\\Delta_t}(x,y-\\Delta_y),\n\\end{aligned}\n\\tag{D.2}\\]\nwhere \\(\\Delta_t\\) is a time-discretization constant, \\(\\Delta_x\\) and \\(\\Delta_y\\) are spatial-discretization constants, and the \\(\\theta\\)s are defined as\n\\[\\begin{align*}\n\\small \\theta_{p,1}(x,y) &= \\left[\\frac{-2 a(x,y) \\Delta_t}{\\Delta^2_x} + \\frac{-2 b(x,y) \\Delta_t}{\\Delta^2_y}\\right] + 1,  \\\\\n\\theta_{p,2}(x,y) &= \\frac{a(x+\\Delta_x,y) \\Delta_t}{4 \\Delta_x^2} - \\frac{a(x-\\Delta_x,y) \\Delta_t}{4 \\Delta_x^2} +  \\frac{a(x,y) \\Delta_t}{ \\Delta_x^2} + \\frac{u(x,y) \\Delta_t}{2 \\Delta_x}, \\\\\n\\theta_{p,3}(x,y) &= \\frac{-a(x+\\Delta_x,y) \\Delta_t}{4 \\Delta_x^2} + \\frac{a(x-\\Delta_x,y) \\Delta_t}{4 \\Delta_x^2} +  \\frac{a(x,y) \\Delta_t}{ \\Delta_x^2} - \\frac{u(x,y) \\Delta_t}{2 \\Delta_x}, \\\\\n\\theta_{p,4}(x,y) &= \\frac{b(x,y+\\Delta_y) \\Delta_t}{4 \\Delta_y^2} - \\frac{b(x,y-\\Delta_y) \\Delta_t}{4 \\Delta_y^2} +  \\frac{b(x,y) \\Delta_t}{ \\Delta_y^2} + \\frac{v(x,y) \\Delta_t}{2 \\Delta_y}, \\\\\n\\theta_{p,5}(x,y) &= \\frac{-b(x,y+\\Delta_y) \\Delta_t}{4 \\Delta_y^2} + \\frac{b(x,y-\\Delta_y) \\Delta_t}{4 \\Delta_y^2} +  \\frac{b(x,y) \\Delta_t}{ \\Delta_y^2} - \\frac{v(x,y) \\Delta_t}{2 \\Delta_y}.\n\\end{align*}\\]\nThus, we see that the finite differences suggest that the neighbors of location \\((x,y)\\) at the previous time (i.e., locations \\((x-\\Delta_x,y)\\), \\((x+\\Delta_x,y)\\), \\((x,y-\\Delta_y)\\), and \\((x, y + \\Delta_y)\\)), as well as the location \\((x,y)\\) itself, play a role in the transition from one time to the next. Note the role of the spatially varying parameters. Let \\(\\mathbf{Y}_t\\) be the process evaluated at all interior grid points at time \\(t\\), and assume the process is defined to be 0 on the boundary (for ease of presentation). Then one can write Equation D.2 as \\(\\mathbf{Y}_t = \\mathbf{M}\\mathbf{Y}_{t-\\Delta_t}\\), where \\(\\mathbf{M}\\) is parameterized with the elements of \\(\\{\\theta_{p,i}(x,y),\\ i=1,\\ldots,5\\}\\). Assume first for simplicity that there is no advection (i.e., \\(u(x,y) = 0\\), \\(v(x,y) = 0\\) for all locations) and the diffusion coefficients in the \\(x\\)- and \\(y\\)-directions are equal (i.e., \\(a(x,y) = b(x,y)\\)). Then, it can be shown that the transition operator \\(\\mathbf{M}\\) is still asymmetric if the diffusion coefficients vary with space. If the diffusion coefficients are constant in space and equal (i.e., \\(a=b\\)), then transition-operator asymmetry is only due to the advection component.\nThe type of diffusion represented in Equation D.1 is typically called “Fickian” diffusion. Similar finite-difference discretizations of other diffusion representations (e.g., so-called “ecological diffusion,” \\(\\nabla^2 (a(x,y) Y)\\)) lead to different formulations of the parameters \\(\\boldsymbol{\\theta}_p\\) (in terms of the coefficients \\(a(x,y)\\)), but they still correspond to a five-diagonal sparse transition operator \\(\\mathbf{M}\\). Thus, in the context of a linear DSTM process model, we typically allow the parameters \\(\\boldsymbol{\\theta}_p\\) to be spatially explicit random processes with the possible addition of covariates, rather than model the specific diffusion equation coefficients (e.g., \\(a(x,y)\\), \\(b(x,y)\\), \\(u(x,y)\\), and \\(v(x,y)\\) in Equation D.1) directly. (Although, one can certainly do this, and the different diffusions correspond to different scientific interpretations.) A last point to make is that different types of finite-difference discretizations lead to different parameterizations (e.g., a higher-order spatial discretization leads to larger neighborhoods, and higher-order temporal differences lead to higher-order Markov models.)",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>D</span>  <span class='chapter-title'>Mechanistically Motivated Dynamic Spatio-Temporal Models</span>"
    ]
  },
  {
    "objectID": "ChapterAppendixD.html#sec-PDEdecomp",
    "href": "ChapterAppendixD.html#sec-PDEdecomp",
    "title": "Appendix D — Mechanistically Motivated Dynamic Spatio-Temporal Models",
    "section": "D.2 Example of a Process Model Motivated by a PDE: Spectral",
    "text": "D.2 Example of a Process Model Motivated by a PDE: Spectral\nThis section considers a natural basis-function (i.e., spectral) approach to motivating a DSTM from a mechanistic model.\nConsider a simple one-dimensional spatial version of the advection-diffusion PDE in Equation 5.14, and denote the spatial index by \\(x\\),\n\\[\n\\frac{\\partial Y(x,t)}{\\partial t} =   a \\frac{\\partial^2 Y(x,t)}{\\partial x^2} + b \\frac{\\partial Y(x,t)}{\\partial x},\n\\tag{D.3}\\]\nwhere in this example the advection and diffusion coefficients (\\(b\\) and \\(a\\), respectively) are assumed to be constant. Now consider the solution as a superposition of Fourier functions (i.e., sines and cosines),\n\\[\nY_t(x) = \\sum_{j=1}^J [\\alpha_{j,t}(1) \\cos(\\omega_j x) +\\alpha_{j,t}(2) \\sin(\\omega_j x)],\n\\tag{D.4}\\]\nwhere \\(\\omega_j = 2 \\pi j/\\vert D_x\\vert\\) is the spatial frequency of a sinusoid with spatial wave number \\(j=1,\\dots ,J\\) in the spatial domain \\({D_x}\\) (and \\(|D_x|\\) corresponds to the length of the spatial domain). For simplicity of exposition, we do not include a constant term in the expansion Equation D.4. For the \\(n\\) spatial locations of interest and \\(n_\\alpha\\) Fourier basis functions, we let \\(\\mathbf{Y}_t = \\boldsymbol{\\Phi}\\boldsymbol{\\alpha}_t\\), where \\(\\mathbf{Y}_t\\) is an \\(n\\)-dimensional vector, \\(\\boldsymbol{\\Phi}\\) is an \\(n \\times n_\\alpha\\) matrix consisting of Fourier basis functions, and \\(\\boldsymbol{\\alpha}_t\\) contains the associated \\(n_\\alpha\\) expansion coefficients, where \\(n_\\alpha = 2 J\\).\nThe deterministic solution of Equation D.3 gives formulas for \\(\\{\\alpha_{j,t}(1), \\alpha_{j,t}(2)\\}\\), which are exponentially decaying sinusoids in time:\n\\[\n\\begin{aligned}\n\\alpha_{j,t}(1) & =  \\exp(-a \\omega_j^2 t) \\sin(b \\omega_j t), \\\\\n\\alpha_{j,t}(2) & =  \\exp(-a \\omega_j^2 t) \\cos(b \\omega_j t),\\quad j=1,\\dots ,J.\n\\end{aligned}\n\\]\nIn this case, the time evolution is given by,\n\\[\n{\\boldsymbol{\\alpha}}_{j,t+\\Delta_t} = {\\mathbf{M}}_j {\\boldsymbol{\\alpha}}_{j,t},\\quad j=1,\\dots ,J,\n\\]\nwhere \\(\\boldsymbol{\\alpha}_{j,t} \\equiv (\\alpha_{j,t}(1) \\; \\alpha_{j,t}(2))'\\) and\n\\[\n{\\mathbf{M}}_j = \\left[ \\begin{array}{rr} e^{-a \\omega_j^2 \\Delta_t}\\cos \\{\\omega_j \\Delta_t\\} & e^{-a \\omega_j^2 \\Delta_t}\\sin\\{\\omega_j \\Delta_t\\} \\\\\n-e^{-a \\omega_j^2 \\Delta_t}\\sin\\{\\omega_j \\Delta_t\\} & e^{-a \\omega_j^2 \\Delta_t}\\cos\\{\\omega_j \\Delta_t\\} \\end{array} \\right].\n\\]\nThis motivates the \\(2J\\)-dimensional linear DSTM process model,\n\\[\n{\\boldsymbol{\\alpha}}_{t} = \\mathbf{M}_\\alpha {\\boldsymbol{\\alpha}}_{t-1} + {\\boldsymbol{\\eta}}_{t}\\,,\n\\]\nwhere \\({\\boldsymbol{\\alpha}}_t \\equiv ({\\boldsymbol{\\alpha}}'_{1,t} \\; \\ldots \\; {\\boldsymbol{\\alpha}}'_{J,t})'\\) for \\(\\boldsymbol{\\alpha}_{j,t} = (\\alpha_{j,t}(1),a_{j,t}(2))'\\), \\(\\boldsymbol{\\eta}_t = (\\boldsymbol{\\eta}'_{1,t},\\ldots,\\boldsymbol{\\eta}'_{J,t})'\\) for \\(\\boldsymbol{\\eta}_{j,t} = (\\eta_{j,t}(1),\\eta_{j,t}(2))'\\), \\(\\mathbf{M}_\\alpha\\) is a \\(2J \\times 2J\\) block diagonal matrix with blocks \\(\\{{\\mathbf{M}}_j\\), \\(j=1,\\ldots,J\\}\\), and we have assumed that \\(\\Delta_t=1\\). This then suggests block-diagonal parameterizations where the \\(2 \\times 2\\) coefficients associated with each set of Fourier functions are unknown and must be estimated (e.g., via a Bayesian hierarchical model). The result is a very sparse representation for the transition matrix, \\(\\mathbf{M}_\\alpha\\), when \\(J\\) is large.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>D</span>  <span class='chapter-title'>Mechanistically Motivated Dynamic Spatio-Temporal Models</span>"
    ]
  },
  {
    "objectID": "ChapterAppendixD.html#sec-IDEdecomp",
    "href": "ChapterAppendixD.html#sec-IDEdecomp",
    "title": "Appendix D — Mechanistically Motivated Dynamic Spatio-Temporal Models",
    "section": "D.3 Example of a Process Model Motivated by an IDE",
    "text": "D.3 Example of a Process Model Motivated by an IDE\nThe stochastic IDE framework discussed in Chapter 5 naturally motivates a DSTM process model. Consider a decomposition similar to Equation 5.24, where we let the process \\(\\{ \\widetilde{Y}_t(\\mathbf{s}): \\mathbf{s}\\in D_s\\}\\) be decomposed as\n\\[\n\\widetilde{Y}_t(\\mathbf{s})=\\mathbf{x}_t(\\mathbf{s})' \\boldsymbol{\\beta}+ Y_t(\\mathbf{s})+\\nu_t(\\mathbf{s})\\,,\n\\]\nwhere \\(\\{Y_t(\\mathbf{s}): \\mathbf{s}\\in D_s\\}\\) is assumed to be a dynamical process, and \\(\\nu_t (\\mathbf{s})\\) is a non-dynamical process in the sense that it does not exhibit Markovian temporal dependence. Now, we assume that \\(\\{ Y_t(\\mathbf{s})\\}\\) follows a stochastic IDE model as in Equation 5.9. That is, for \\(\\mathbf{s}\\in D_s\\),\n\\[\nY_t(\\mathbf{s}) = \\int_{D_s} m(\\mathbf{s},\\mathbf{x}; \\boldsymbol{\\theta}_p) \\; Y_{t-1}(\\mathbf{x}) \\textrm{d}\\mathbf{x}+\\eta_t (\\mathbf{s})\\,,\n\\tag{D.5}\\]\nwhere \\(m(\\mathbf{s},\\mathbf{x}; \\boldsymbol{\\theta}_p)\\) is the transition kernel over the domain \\(D_s\\), and \\(\\boldsymbol{\\theta}_p\\) are kernel parameters.\nAs in Equation 5.15, we assume that the dynamical process can be expanded in terms of \\(n_\\alpha\\) basis functions, \\(\\{\\phi_i (\\mathbf{s})\\colon i=1,\\dots ,n_\\alpha\\}\\). That is,\n\\[\nY_t (\\mathbf{s}) = \\sum^{n_\\alpha}_{i=1} \\phi_i(\\mathbf{s})  \\alpha_{i,t} \\,.\n\\tag{D.6}\\]\nNow, we can also expand the transition kernel in terms of these basis functions (although we could use different basis functions in general; see, for example, Cressie & Wikle (2011), Chapter 7):\n\\[\nm(\\mathbf{s},\\mathbf{x}; \\boldsymbol{\\theta}_p) = \\sum^{n_\\alpha}_{j=1}  \\phi_j(\\mathbf{x}) b_j(\\mathbf{s}; \\boldsymbol{\\theta}_p).\n\\tag{D.7}\\]\nSubstituting Equation D.6 and Equation D.7 into Equation D.5 and, for the sake of simplicity, adding the assumption that the basis functions are orthonormal,\n\\[\n\\int_{D_s}\\phi_i (\\mathbf{x})\\phi_j(\\mathbf{x}) \\textrm{d}\\mathbf{x}= \\left\\{ \\begin{array}{ll}\n1,&i=j,\\\\\n0,&i\\not= j, \\end{array}\\right.\n\\]\nwe can show that\n\\[\n\\begin{aligned}\nY_{t}(\\mathbf{s}) &=  \\sum^{n_\\alpha}_{i=1} b_i(\\mathbf{s}; \\boldsymbol{\\theta}_p)\\alpha_{i,t-1} + \\eta_t\n(\\mathbf{s})\\\\\n&=   \\; \\mathbf{b}'(\\mathbf{s}; \\boldsymbol{\\theta}_p)\\boldsymbol{\\alpha}_{t-1} +\\eta_t(\\mathbf{s})\\,,\n\\end{aligned}\n\\]\nwhere \\(\\mathbf{b}(\\mathbf{s}; \\boldsymbol{\\theta}_p)\\equiv (b_1(\\mathbf{s}; \\boldsymbol{\\theta}_p),\\dots ,b_{n_\\alpha}(\\mathbf{s}; \\boldsymbol{\\theta}_p))'\\) and \\(\\boldsymbol{\\alpha}_t \\equiv (\\alpha_{1,t},\\ldots ,\\alpha_{n_\\alpha,t})'\\). Note also that \\(\\mathbf{Y}_t = \\boldsymbol{\\Phi}\\boldsymbol{\\alpha}_t\\), where \\(\\boldsymbol{\\Phi}\\) is an \\(n \\times n_\\alpha\\) basis-function matrix,\n\\[\n\\boldsymbol{\\Phi}\\equiv\\left(\\begin{array}{c}\n\\boldsymbol{\\phi}(\\mathbf{s}_1)'\\\\ \\vdots\\\\ \\boldsymbol{\\phi}(\\mathbf{s}_n)'\\end{array}\\right)\\,,\n\\]\nand \\(\\boldsymbol{\\phi}(\\mathbf{s}_i) \\equiv (\\phi_1(\\mathbf{s}_i),\\dots ,\\phi_{n_\\alpha}(\\mathbf{s}_i))'\\), for \\(i=1,\\dots ,n\\). Now, define the \\(n\\times n_\\alpha\\) matrix\n\\[\n\\mathbf{B}\\equiv \\left(\\begin{array}{c}\n\\mathbf{b}(\\mathbf{s}_1;\\boldsymbol{\\theta}_p)'\\\\ \\vdots\\\\ \\mathbf{b}(\\mathbf{s}_n;\\boldsymbol{\\theta}_p)'\\end{array}\\right)\\,.\n\\]\nThen, for all \\(n\\) process locations, we can write\n\\[\n\\begin{aligned}\n\\boldsymbol{\\alpha}_t &= (\\boldsymbol{\\Phi}'\\boldsymbol{\\Phi})^{-1}\\boldsymbol{\\Phi}'\\mathbf{B}\\boldsymbol{\\alpha}_{t-1} +\n(\\boldsymbol{\\Phi}'\\boldsymbol{\\Phi})^{-1}\\boldsymbol{\\Phi}'\\boldsymbol{\\eta}_t \\\\\n&= \\boldsymbol{\\Phi}'\\mathbf{B}\\boldsymbol{\\alpha}_{t-1} +\n\\boldsymbol{\\Phi}'\\boldsymbol{\\eta}_t\\\\\n&\\equiv \\mathbf{M}_\\alpha\\boldsymbol{\\alpha}_{t-1} +\\tilde{\\boldsymbol{\\eta}}_t,\n\\end{aligned}\n\\]\nwhere the second equality is due to orthonormality (i.e., \\(\\boldsymbol{\\Phi}' \\boldsymbol{\\Phi}= \\mathbf{I}\\)), \\(\\tilde{\\boldsymbol{\\eta}}_t \\equiv \\boldsymbol{\\Phi}' {\\boldsymbol{\\eta}}_t\\) is the \\(n_\\alpha\\)-dimensional noise process, and the \\(n_\\alpha \\times n_\\alpha\\) propagator matrix is given by \\(\\mathbf{M}_\\alpha \\equiv \\boldsymbol{\\Phi}'\\mathbf{B}\\).\nThe truncated expansion Equation D.6 leads to a lower-dimensional dynamical process (since \\(n_\\alpha \\ll n\\)). In principle, we still have to estimate the \\(n \\times n_\\alpha\\) matrix \\(\\mathbf{B}\\) and the covariance matrix associated with \\(\\tilde{\\boldsymbol{\\eta}}_t\\). However, the IDE formulation allows the kernel \\(m(\\mathbf{s},\\mathbf{x}; \\boldsymbol{\\theta}_p)\\) to be parameterized parsimoniously. In some cases, one can select \\(\\{\\phi_j(\\mathbf{s})\\}\\) to ensure that the expansion coefficients for the kernel can be specified analytically in terms of its parameters. For example, letting \\(\\{\\phi_j(\\cdot)\\}\\) in Equation D.7 be Fourier basis functions allows one to parameterize the kernel in terms of its characteristic function. This can facilitate a BHM parameterization that allows kernel asymmetry and scale parameters to vary in space.\nLab 5.1 gives an introduction to the implementation of the IDE in one-dimensional space. Lab 5.2 then gives an implementation of a DSTM motivated by the stochastic IDE model to generate nowcasts for weather radar images.\n\n\n\n\nCressie, N., & Wikle, C. K. (2011). Statistics for spatio-temporal data. John Wiley & Sons.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>D</span>  <span class='chapter-title'>Mechanistically Motivated Dynamic Spatio-Temporal Models</span>"
    ]
  },
  {
    "objectID": "ChapterAppendixE.html",
    "href": "ChapterAppendixE.html",
    "title": "Appendix E — Case Study: Physical-Statistical Bayesian Hierarchical Model for Predicting Mediterranean Surface Winds",
    "section": "",
    "text": "In this section we present a specific and detailed example of how to develop a physically motivated bivariate spatio-temporal model for the purpose of predicting near-surface wind fields in the Mediterranean Sea. The implementation of this model in R is given below.\nConsider a simple analytical model for the surface wind known as the Rayleigh friction equations (e.g., Stevens et al., 2002):\n\\[\\begin{align*}\n\\frac{\\partial u}{\\partial t} &= f v - \\frac{1}{\\rho_0} \\frac{\\partial P}{\\partial x} - \\gamma u, \\\\\n\\frac{\\partial v}{\\partial t} &= - f u - \\frac{1}{\\rho_0} \\frac{\\partial P}{\\partial y} - \\gamma v,\n\\end{align*}\\]\nwhere \\(u\\) and \\(v\\) are the east–west and north–south components of the wind, respectively (recall that winds are vectors with a magnitude and direction that can be decomposed into \\(x\\) (east–west) and \\(y\\) (north–south) coordinates); \\(f\\) is the Coriolis parameter; \\(\\rho_0\\) is a reference atmospheric density; \\(P\\) is the sea-level pressure; and \\(\\gamma\\) is the Rayleigh friction parameter. Note that \\(u\\), \\(v\\), and \\(P\\) are functions of time and space. As in Section D.1, simple forward differencing in time with \\(\\Delta_t = 1\\), and centered differencing in space, give the analogous discretized form of these equations:\n\\[\n\\small\nu_{t+1}(i,j) =  u_t(i,j) + \\Delta_t \\left\\{ f v_t(i,j) -  \\frac{1}{\\rho_0}\\left(\\frac{P_t(i+1,j) - P_t(i-1,j)}{2 \\Delta_x}\\right) - \\gamma u_t(i,j)\\right\\},\n\\tag{E.1}\\]\n\\[\n\\small\nv_{t+1}(i,j) = v_t(i,j) +   \\Delta_t\\left\\{  - f u_t(i,j) -\n  \\frac{1}{\\rho_0}\\left(\\frac{P_t(i,j+1) - P_t(i,j-1)}{2 \\Delta_y}\\right) - \\gamma v_t(i,j)\\right\\},  \n\\tag{E.2}\\]\nwhere \\(u_t(i,j)\\), \\(v_t(i,j)\\), and \\(P_t(i,j)\\) are discretized wind components and pressure, respectively, at grid location \\((i,j)\\) and time \\(t\\), and \\(\\Delta_x\\), and \\(\\Delta_y\\) are the \\(x\\)-, and \\(y\\)-discretization constants, respectively. Note that this is a multivariate linear system, with each component of the wind conditioned on the past values of that component, the other component, and the difference (gradient) in pressure.\nNow a simple statistical process model based on these equations can be written in vector form as:\n\\[\n\\mathbf{u}_{t+1} = \\theta_{uu} \\mathbf{u}_t + \\theta_{uv} \\mathbf{v}_t + \\theta_{up} \\mathbf{D}_x \\mathbf{P}_t  + \\boldsymbol{\\eta}_{u,t},\n\\tag{E.3}\\]\n\\[\n\\mathbf{v}_{t+1} = \\theta_{vv} \\mathbf{v}_t + \\theta_{vu} \\mathbf{u}_t + \\theta_{vp} \\mathbf{D}_y \\mathbf{P}_t  + \\boldsymbol{\\eta}_{v,t},  \\tag{E.4}\\]\nfor \\(t=1,\\ldots,T-1\\), where \\(\\mathbf{v}_t\\) and \\(\\mathbf{u}_t\\) are \\(n_g\\)-dimensional \\((n_g = n_x \\times n_y)\\) vectors of the discretized \\(u\\) and \\(v\\) components, \\(n_x\\) and \\(n_y\\) being the number of grid locations in the \\(x\\)- and \\(y\\)-directions on the prediction grid; \\(\\mathbf{P}_t\\) is an \\(n_e = (n_x + 2) \\times (n_y + 2)\\)-dimensional vector of surface pressure values on an expanded grid (which in our example will come from data); \\(\\mathbf{D}_x\\) and \\(\\mathbf{D}_y\\) are \\(n_g \\times n_e\\) matrix operators that give the centered difference in the \\(x\\) and \\(y\\) directions, respectively; and \\(\\boldsymbol{\\eta}_{u,t} \\sim iid \\; Gau(0,\\sigma^2_u \\mathbf{I})\\) and \\(\\boldsymbol{\\eta}_{v,t} \\sim  iid \\; Gau(0, \\sigma^2_v \\mathbf{I})\\) are residual error processes. Although we could specify the \\(\\theta\\)-values in Equation E.3 and Equation E.4 according to the discretization constants and \\(f\\) and \\(\\gamma\\) in Equation E.1 and Equation E.2, we instead allow them to be unknown and random here (see below) and include the additive error terms to adapt to the data and to reflect the fact that the Rayleigh friction equations are a pretty rough approximation for reality. More complicated versions of this model are given in Milliff et al. (2011) and Cressie & Wikle (2011, Chapter 9) to account for spatio-temporal dependent errors as well as a random pressure process.\nWe have two sources of data on Mediterranean surface winds (as described in Section 2.1): gridded analysis wind and pressure data from the European Center for Medium Range Weather Forecasting (ECMWF) (these observations are complete in space and time); and higher-resolution satellite observations of near-surface winds over the ocean from the polar-orbiting QuikSCAT scatterometer (these observations are irregular in space and time). The BHM is given by a data model, a process model, and a parameter model. For modeling the Mediterranean wind data, these are defined as follows.\nData model. For \\(t=1,\\ldots,T\\), assume \\[\n\\mathbf{E}_{u,t} | \\mathbf{u}_t, \\sigma^2_e \\sim indep. \\; Gau(\\mathbf{H}_{e} \\mathbf{u}_t, \\sigma^2_e \\mathbf{I}),\n\\tag{E.5}\\] \\[\n\\mathbf{E}_{v,t} | \\mathbf{v}_t, \\sigma^2_e \\sim indep. \\; Gau(\\mathbf{H}_{e} \\mathbf{v}_t, \\sigma^2_e \\mathbf{I}),\n\\tag{E.6}\\] \\[\n\\mathbf{S}_{u,t} | \\mathbf{u}_t, \\sigma^2_s \\sim indep. \\; Gau(\\mathbf{H}_{s,t} \\mathbf{u}_t, \\sigma^2_s \\mathbf{I}),\n\\tag{E.7}\\] \\[\n\\mathbf{S}_{v,t} | \\mathbf{v}_t, \\sigma^2_s \\sim indep. \\; Gau(\\mathbf{H}_{s,t} \\mathbf{v}_t, \\sigma^2_s \\mathbf{I}),\n\\tag{E.8}\\] where \\(\\mathbf{E}_{u,t}, \\mathbf{E}_{v,t}\\) are \\(n_e\\)-vectors of ECMWF observations at time \\(t\\), with associated \\(n_e \\times n_g\\) incidence matrix \\(\\mathbf{H}_e\\); and \\(\\mathbf{S}_{u,t}, \\mathbf{S}_{v,t}\\) are \\(n_{s,t}\\)-dimensional vectors of QuikSCAT observations at time \\(t\\), with associated \\(n_{s,t} \\times n_g\\) incidence matrices, \\(\\mathbf{H}_{s,t}\\). (Note that there are different numbers of QuikSCAT observations at each time, and there can be times for which there are no QuikSCAT observations.)\nProcess model. For \\(t=1,\\ldots,T-1\\), assume\n\\[\n\\mathbf{u}_{t+1} | \\mathbf{u}_t, \\mathbf{v}_t, \\mathbf{P}_t, \\theta_{uu}, \\theta_{uv}, \\theta_{up}, \\sigma^2_u \\sim {indep.} \\; {Gau}(\\theta_{uu} \\mathbf{u}_t + \\theta_{uv} \\mathbf{v}_t + \\theta_{up} \\mathbf{D}_x \\mathbf{P}_t , \\sigma^2_u \\mathbf{I}),~~~~~\n\\tag{E.9}\\]\n\\[\n\\mathbf{v}_{t+1} | \\mathbf{v}_t, \\mathbf{u}_t, \\mathbf{P}_t, \\theta_{vv}, \\theta_{vu}, \\theta_{vp}, \\sigma^2_v \\sim {indep.} \\; {Gau}(\\theta_{vv} \\mathbf{v}_t + \\theta_{vu} \\mathbf{u}_t + \\theta_{vp} \\mathbf{D}_y \\mathbf{P}_t , \\sigma^2_v \\mathbf{I}),~~~~~\n\\tag{E.10}\\]\nwhere we have pressure observations, \\(\\mathbf{P}_t\\), from the ECMWF data within the Mediterranean wind data. We also need to specify the process’s initial conditions at time \\(t=1\\). Assume\n\\[\n\\mathbf{u}_1 | \\boldsymbol{\\mu}_{u,1}, \\sigma^2_{u,1} \\sim \\; {Gau}(\\boldsymbol{\\mu}_{u,1},\\sigma^2_{u,1} \\mathbf{I}),\n\\tag{E.11}\\]\n\\[\n\\mathbf{v}_1 | \\boldsymbol{\\mu}_{v,1}, \\sigma^2_{v,1} \\sim \\; {Gau}(\\boldsymbol{\\mu}_{v,1},\\sigma^2_{v,1} \\mathbf{I}).\n\\tag{E.12}\\]\nParameter model. All of the process-model parameters are assumed to be independent and their distributions are given by\n\\[\n\\theta_{ab} | \\mu_{ab}, \\sigma^2_{ab} \\sim \\; {Gau}(\\mu_{ab}, \\sigma^2_{ab}),\n\\tag{E.13}\\]\nfor \\(ab = \\{uu,vv,uv,vu,up,vp\\}\\). Further,\n\\[\n\\sigma^2_a | q_a, r_a \\sim \\; {IG}(q_a,r_a),\n\\tag{E.14}\\]\nfor \\(a = \\{u, v\\}\\) (where \\({IG}(q_a, r_a)\\) is the inverse gamma distribution with shape parameter \\(q_a\\) and rate parameter \\(r_a\\)).\nHyperparameters (fixed and specified). The following hyperparameters are specified based on scientific assumptions or to correspond to “vague” prior distributions: \\(\\sigma^2_e,\\) \\(\\sigma^2_s,\\) \\(\\{\\mu_{ab},\\) \\(\\sigma^2_{ab}:ab = uu,vv,uv,vu,up,vp\\}\\), \\(\\{q_a, r_a: a= u,v\\}\\), \\(\\boldsymbol{\\mu}_{u,1}\\), \\(\\boldsymbol{\\mu}_{v,1}\\), \\(\\sigma^2_{u,1}\\), \\(\\sigma^2_{v,1}\\). Specific values are given in the R example that follows.\nGibbs sampler. The BHM presented above is amenable to a Gibbs sampler MCMC implementation because all the full conditional distributions are available in closed form (see Cressie & Wikle, 2011, Chapter 8, for details on how to derive full conditional distributions for spatio-temporal models). Recall from Note 4.5 that the Gibbs sampler simply cycles through the full conditional distributions, sampling each variable given the most recent samples. The Gibbs sampler for the BHM of the Mediterranean winds data is outlined in Note E.1, where the equation numbers correspond to the full conditional distributions presented in the next section.\n\n\n\n\n\n\nNote E.1: Gibbs Sampler for BHM of Mediterranean winds data set\n\n\n\nSelect hyperparameters: \\(\\sigma^2_e\\), \\(\\sigma^2_s\\), \\(\\{\\mu_{ab}, \\sigma^2_{ab}:ab = uu,vv,uv,vu,up,vp \\}\\), \\(\\{q_a, r_a: a=u,v\\}\\), \\(\\boldsymbol{\\mu}_{u,1}\\), \\(\\boldsymbol{\\mu}_{v,1}\\), \\(\\sigma^2_{u,1}\\), \\(\\sigma^2_{v,1}\\)\nSelect initial values: \\(\\{\\mathbf{u}^{(0)}_t: \\, t=2,\\ldots,T \\}\\), \\(\\{\\mathbf{v}^{(0)}_t: \\, t=1,\\ldots,T \\}\\), \\(\\theta_{uu}^{(0)}\\), \\(\\theta_{vv}^{(0)}\\), \\(\\theta_{uv}^{(0)}\\), \\(\\theta_{vu}^{(0)}\\), \\(\\theta_{up}^{(0)}\\), \\(\\theta_{vp}^{(0)}\\), \\(\\sigma_u^{2(0)}\\), \\(\\sigma_v^{2(0)}\\)\nfor \\(\\; \\; i=1,2,\\ldots, N_{\\mathrm{gibbs}}\\) do\n\nusing Equation E.15, sample from\n\\[\n\\small\n\\mathbf{u}^{(i)}_1 | \\mathbf{v}^{(i-1)}_1, \\mathbf{u}^{(i-1)}_2, \\mathbf{v}^{(i-1)}_2, \\theta_{vu}^{(i-1)}, \\theta_{uu}^{(i-1)}, \\theta_{uv}^{(i-1)}, \\theta_{vv}^{(i-1)}, \\theta_{vp}^{(i-1)}, \\theta_{up}^{(i-1)}, \\sigma_u^{2(i-1)}, \\sigma_v^{2(i-1)}\n\\]\nusing Equation E.16 for \\(t=2,\\ldots,T-1\\), sample from\n\\[\\begin{eqnarray*}\n\\; & \\; & \\; \\mathbf{u}^{(i)}_t | \\mathbf{u}^{(i)}_{t-1}, \\mathbf{u}^{(i-1)}_{t+1}, \\mathbf{v}^{(i-1)}_{t-1}, \\mathbf{v}^{(i-1)}_t, \\mathbf{v}^{(i-1)}_{t+1}, \\theta_{vu}^{(i-1)}, \\theta_{uu}^{(i-1)}, \\theta_{uv}^{(i-1)}, \\theta_{vv}^{(i-1)}, \\nonumber \\\\\n\\; & \\; & \\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;  \\theta_{vp}^{(i-1)}, \\theta_{up}^{(i-1)},  \\sigma_u^{2(i-1)}, \\sigma_v^{2(i-1)}\n\\end{eqnarray*}\\]\nusing Equation E.17, sample from\n\\[\n\\mathbf{u}^{(i)}_T | \\mathbf{u}^{(i)}_{T-1}, \\mathbf{v}^{(i-1)}_{T-1},  \\theta_{uu}^{(i-1)}, \\theta_{uv}^{(i-1)},  \\theta_{up}^{(i-1)},  \\sigma_u^{2(i-1)}\n\\]\nusing Equation E.18, sample from\n\\[\\small\n\\mathbf{v}^{(i)}_1 | \\mathbf{u}^{(i)}_1, \\mathbf{u}^{(i)}_2, \\mathbf{v}^{(i-1)}_2, \\theta_{uv}^{(i-1)}, \\theta_{uu}^{(i-1)}, \\theta_{vu}^{(i-1)}, \\theta_{vv}^{(i-1)}, \\theta_{up}^{(i-1)}, \\theta_{vp}^{(i-1)}, \\sigma_u^{2(i-1)}, \\sigma_v^{2(i-1)}\n\\]\nusing Equation E.19 for \\(t=2,\\ldots,T-1\\), sample from\n\\[\\small\n\\mathbf{v}^{(i)}_t | \\mathbf{u}^{(i)}_t, \\mathbf{u}^{(i)}_{t-1}, \\mathbf{u}^{(i)}_{t+1}, \\mathbf{v}^{(i)}_{t-1},  \\mathbf{v}^{(i-1)}_{t+1}, \\theta_{vu}^{(i-1)}, \\theta_{uu}^{(i-1)}, \\theta_{uv}^{(i-1)}, \\theta_{vv}^{(i-1)}, \\theta_{vp}^{(i-1)}, \\theta_{up}^{(i-1)},  \\sigma_u^{2(i-1)}, \\sigma_v^{2(i-1)}\n\\]\nusing Equation E.20, sample from\n\\[\n\\mathbf{v}^{(i)}_T | \\mathbf{u}^{(i)}_{T-1}, \\mathbf{v}^{(i)}_{T-1},  \\theta_{vv}^{(i-1)}, \\theta_{vu}^{(i-1)},  \\theta_{vp}^{(i-1)},  \\sigma_v^{2(i-1)}\n\\]\nusing Equation E.21, sample from\n\\[\n\\theta_{uu}^{(i)} | \\{\\mathbf{u}^{(i)}_t: t=1,\\ldots,T\\}, \\{\\mathbf{v}^{(i)}_t: t=1,\\ldots,T\\}, \\theta_{uv}^{(i-1)}, \\theta_{up}^{(i-1)}, \\sigma_u^{2(i-1)}\n\\]\nusing Equation E.22, sample from\n\\[\n\\theta_{vv}^{(i)} | \\{\\mathbf{u}^{(i)}_t: t=1,\\ldots,T\\}, \\{\\mathbf{v}^{(i)}_t: t=1,\\ldots,T\\}, \\theta_{vu}^{(i-1)}, \\theta_{vp}^{(i-1)}, \\sigma_v^{2(i-1)}\n\\]\nusing Equation E.23, sample from\n\\[\n\\theta_{uv}^{(i)} | \\{\\mathbf{u}^{(i)}_t: t=1,\\ldots,T\\}, \\{\\mathbf{v}^{(i)}_t: t=1,\\ldots,T\\}, \\theta_{uu}^{(i)}, \\theta_{up}^{(i-1)}, \\sigma_u^{2(i-1)}\n\\]\nusing Equation E.24, sample from\n\\[\n\\theta_{vu}^{(i)} | \\{\\mathbf{u}^{(i)}_t: t=1,\\ldots,T\\}, \\{\\mathbf{v}^{(i)}_t: t=1,\\ldots,T\\}, \\theta_{vv}^{(i)}, \\theta_{vp}^{(i-1)}, \\sigma_v^{2(i-1)}\n\\]\nusing Equation E.25, sample from\n\\[\n\\theta_{up}^{(i)} | \\{\\mathbf{u}^{(i)}_t: t=1,\\ldots,T\\}, \\{\\mathbf{v}^{(i)}_t: t=1,\\ldots,T\\}, \\theta_{uu}^{(i)}, \\theta_{uv}^{(i)}, \\sigma_u^{2(i-1)}\n\\]\nusing Equation E.26, sample from\n\\[\n\\theta_{vp}^{(i)} | \\{\\mathbf{u}^{(i)}_t: t=1,\\ldots,T\\}, \\{\\mathbf{v}^{(i)}_t: t=1,\\ldots,T\\}, \\theta_{vv}^{(i)}, \\theta_{vu}^{(i)}, \\sigma_v^{2(i-1)}\n\\]\nusing Equation E.27, sample from\n\\[\n\\sigma_u^{2(i)} | \\{\\mathbf{u}^{(i)}_t: t=1,\\ldots,T\\}, \\{\\mathbf{v}^{(i)}_t: t=1,\\ldots,T\\}, \\theta_{uu}^{(i)}, \\theta_{uv}^{(i)}, \\theta_{up}^{(i)}\n\\]\nusing Equation E.28, sample from\n\\[\n\\sigma_v^{2(i)} | \\{\\mathbf{u}^{(i)}_t: t=1,\\ldots,T\\}, \\{\\mathbf{v}^{(i)}_t: t=1,\\ldots,T\\}, \\theta_{vv}^{(i)}, \\theta_{vu}^{(i)}, \\theta_{vp}^{(i)}\n\\]\n\nend for\n\n\nFull Conditional Distributions. Readers be warned that this material is very technical! The full conditional distributions for the Gibbs sampler presented in Note E.1 are included here for advanced readers. For more examples in the spatio-temporal context, see (Cressie & Wikle, 2011, Chapter 8), and for other examples see Gelman et al. (2014). In the representation to follow, \\([equation\\ number]_t\\) corresponds to the distribution associated with the equation number above, where the variable on the left side of the conditioning symbol is given at time \\(t\\). When referring to the parameter model, Equation E.13 and Equation E.14, the notation \\([equation\\ number]_{ab}\\) and \\([equation\\ number]_a\\) correspond to the specific parameter distribution given by \\(ab = \\{uu, vv, uv, vu, up, vp\\}\\) or \\(a = \\{u, v\\}\\), respectively.\n\nFull conditional distribution for \\(\\mathbf{u}_1\\):\n\n\n\\([\\mathbf{u}_1 | \\cdot] \\propto [\\) Equation E.5 \\(]_1 \\times [\\) Equation E.7 \\(]_1 \\times [\\) Equation E.9 \\(]_2 \\times [\\) Equation E.10 \\(]_2 \\times [\\) Equation E.11 \\(]\\)\n\n\\[\n  \\mathbf{u}_1 | \\cdot \\sim Gau(\\mathbf{A}_{u,1} \\mathbf{b}_{u,1}, \\mathbf{A}_{u,1})\n   \\tag{E.15}\\]\nwhere\n\\[\n  \\begin{aligned}\n  \\mathbf{A}_{u,1}\\equiv &\\left( \\mathbf{H}_e' \\mathbf{H}_e/\\sigma^2_e + \\mathbf{H}_{s,1}' \\mathbf{H}_{s,1}/\\sigma^2_s + \\theta_{vu}^2 \\, \\mathbf{I}/\\sigma^2_v + \\theta_{uu}^2 \\, \\mathbf{I}/\\sigma^2_u +  \\mathbf{I}/\\sigma^2_{u,1} \\right)^{-1},\\\\\n  \\mathbf{b}_{u,1}\\equiv &\\left(\\mathbf{E}_{u,1}' \\mathbf{H}_e/\\sigma^2_e + \\mathbf{S}_{u,1}' \\mathbf{H}_{s,1}/\\sigma^2_s +  (\\mathbf{v}_2 - \\mathbf{c}_{v,1})' \\theta_{vu}/\\sigma^2_v \\right.\\\\\n  &~~~\\left.+ (\\mathbf{u}_2 - \\mathbf{c}_{u,1})' \\theta_{uu}/\\sigma^2_u + \\boldsymbol{\\mu}_{u,1}'/\\sigma^2_{u,1}   \\right)',\n  \\end{aligned}\n  \\]\nwith\n\\[\n  \\mathbf{c}_{u,1} \\equiv \\theta_{uv} \\mathbf{v}_1  + \\theta_{up} \\mathbf{D}_x \\mathbf{P}_1,\n  \\]\n\\[\n  \\mathbf{c}_{v,1} \\equiv \\theta_{vv} \\mathbf{v}_1  + \\theta_{vp} \\mathbf{D}_y \\mathbf{P}_1.\n  \\]\n\nFull conditional distribution for \\(\\mathbf{u}_t, t=2,\\ldots,T-1\\):\n\n\n\\([\\mathbf{u}_t | \\cdot] \\propto [\\) Equation E.5 \\(]_t \\times [\\) Equation E.7 \\(]_t \\times [\\) Equation E.10 \\(]_{t+1} \\times [\\) Equation E.9 \\(]_{t+1} \\times [\\) Equation E.9 \\(]_{t}\\)\n\n\\[\n  \\mathbf{u}_t | \\cdot \\sim Gau(\\mathbf{A}_{u,t} \\mathbf{b}_{u,t}, \\mathbf{A}_{u,t})\n   \\tag{E.16}\\]\nwhere\n\\[\n  \\mathbf{A}_{u,t} \\equiv \\left( \\mathbf{H}_e' \\mathbf{H}_e/\\sigma^2_e + \\mathbf{H}_{s,t}' \\mathbf{H}_{s,t}/\\sigma^2_s + \\theta_{vu}^2 \\, \\mathbf{I}/\\sigma^2_v + \\theta_{uu}^2 \\, \\mathbf{I}/\\sigma^2_u + \\mathbf{I}/\\sigma^2_{u}  \\right)^{-1},\n  \\]\n\\[\n  \\begin{aligned}\n  \\mathbf{b}_{u,t} \\equiv & (\\mathbf{E}_{u,t}' \\mathbf{H}_e/\\sigma^2_e + \\mathbf{S}_{u,t}' \\mathbf{H}_{s,t}/\\sigma^2_s + (\\mathbf{v}_{t+1} - \\mathbf{c}_{v,t})' \\theta_{vu}/\\sigma^2_v  + (\\mathbf{u}_{t+1} - \\mathbf{c}_{u,t})' \\theta_{uu}/\\sigma^2_u. \\\\\n  & ~~~ +  (\\mathbf{c}_{u,t-1} + \\theta_{uu} \\mathbf{u}_{t-1})'/\\sigma^2_u  )',\n  \\end{aligned}\n  \\]\nwith\n\\[\n  \\mathbf{c}_{v,t} \\equiv \\theta_{vv} \\mathbf{v}_t  + \\theta_{vp} \\mathbf{D}_y \\mathbf{P}_t,\n  \\]\n\\[\n  \\mathbf{c}_{u,t} \\equiv \\theta_{uv} \\mathbf{v}_t + \\theta_{up} \\mathbf{D}_x \\mathbf{P}_t,\n  \\]\n\\[\n  \\mathbf{c}_{u,t-1} \\equiv \\theta_{uv} \\mathbf{v}_{t-1} + \\theta_{up} \\mathbf{D}_x \\mathbf{P}_{t-1}.\n  \\]\n\nFull conditional distribution for \\(\\mathbf{u}_T\\):\n\n\n\\([\\mathbf{u}_T | \\cdot] \\propto [\\) Equation E.5 \\(]_T \\times [\\) Equation E.7 \\(]_T \\times [\\) Equation E.9 \\(]_{T}\\)\n\n\\[\n\\mathbf{u}_T | \\cdot \\sim Gau(\\mathbf{A}_{u,T} \\mathbf{b}_{u,T}, \\mathbf{A}_{u,T})\n\\tag{E.17}\\]\nwhere\n\\[\\begin{align*}\n\\mathbf{A}_{u,T} &\\equiv \\left( \\mathbf{H}_e' \\mathbf{H}_e/\\sigma^2_e + \\mathbf{H}_{s,T}' \\mathbf{H}_{s,T}/\\sigma^2_s + \\mathbf{I}/ \\sigma^2_{u}  \\right)^{-1}, \\\\\n\\mathbf{b}_{u,T} &\\equiv (\\mathbf{E}_{u,T}' \\mathbf{H}_e/\\sigma^2_e + \\mathbf{S}_{u,T}' \\mathbf{H}_{s,T}/\\sigma^2_s +   (\\mathbf{c}_{u,T-1} + \\theta_{uu} \\mathbf{u}_{T-1})'/\\sigma^2_u  )',  \n\\end{align*}\\]\nwith\n\\[\n\\mathbf{c}_{u,T-1} \\equiv \\theta_{uv} \\mathbf{v}_{T-1} + \\theta_{up} \\mathbf{D}_x \\mathbf{P}_{T-1}.\n\\]\n\nFull conditional distribution for \\(\\mathbf{v}_1\\):\n\n\n\\([\\mathbf{v}_1 | \\cdot] \\propto [\\) Equation E.6 \\(]_1 \\times [\\) Equation E.8 \\(]_1 \\times [\\) Equation E.10 \\(]_2 \\times [\\) Equation E.9 \\(]_2 \\times [\\) Equation E.12 \\(]\\)\n\n\\[\n\\mathbf{v}_1 | \\cdot \\sim Gau(\\mathbf{A}_{v,1} \\mathbf{b}_{v,1}, \\mathbf{A}_{v,1})\n\\tag{E.18}\\]\nwhere\n\\[\\begin{align*}\n\\mathbf{A}_{v,1} &\\equiv \\left( \\mathbf{H}_e' \\mathbf{H}_e/\\sigma^2_e + \\mathbf{H}_{s,1}' \\mathbf{H}_{s,1}/\\sigma^2_s + \\theta_{uv}^2 \\, \\mathbf{I}/\\sigma^2_u + \\theta_{vv}^2 \\, \\mathbf{I}/\\sigma^2_v +   \\, \\mathbf{I}/\\sigma^2_{v,1}  \\right)^{-1}, \\\\\n\\mathbf{b}_{v,1} &\\equiv \\left(\\mathbf{E}_{v,1}' \\mathbf{H}_e/\\sigma^2_e + \\mathbf{S}_{v,1}' \\mathbf{H}_{s,1}/\\sigma^2_s +  (\\mathbf{u}_2 - \\mathbf{c}_{u,1})' \\theta_{uv}/\\sigma^2_u  \\right. \\\\\n&~~~~ \\left. + (\\mathbf{v}_2 - \\mathbf{c}_{v,1})' \\theta_{vv}/\\sigma^2_v + \\boldsymbol{\\mu}_{v,1}'/\\sigma^2_{v,1}   \\right)',  \n\\end{align*}\\]\nwith\n\\[\\begin{align*}\n\\mathbf{c}_{v,1} &\\equiv \\theta_{vu} \\mathbf{u}_1  + \\theta_{vp} \\mathbf{D}_y \\mathbf{P}_1, \\\\\n\\mathbf{c}_{u,1} &\\equiv \\theta_{uu} \\mathbf{u}_1  + \\theta_{up} \\mathbf{D}_x \\mathbf{P}_1.\n\\end{align*}\\]\n\nFull conditional distribution for \\(\\mathbf{v}_t,\\ t=2,\\ldots,T-1\\):\n\n\n\\([\\mathbf{v}_t | \\cdot] \\propto [\\) Equation E.6 \\(]_t \\times [\\) Equation E.8 \\(]_t \\times [\\) Equation E.9 \\(]_{t+1} \\times [\\) Equation E.10 \\(]_{t+1} \\times [\\) Equation E.10 \\(]_{t}\\)\n\n\\[\n\\mathbf{v}_t | \\cdot \\sim Gau(\\mathbf{A}_{v,t} \\mathbf{b}_{v,t}, \\mathbf{A}_{v,t})\n\\tag{E.19}\\]\nwhere\n\\[\\begin{align*}\n\\mathbf{A}_{v,t} \\equiv &\\left( \\mathbf{H}_e' \\mathbf{H}_e/\\sigma^2_e + \\mathbf{H}_{s,t}' \\mathbf{H}_{s,t}/\\sigma^2_s + \\theta_{uv}^2 \\, \\mathbf{I}/\\sigma^2_u + \\theta_{vv}^2 \\, \\mathbf{I}/\\sigma^2_v +  \\, \\mathbf{I}/\\sigma^2_{v}  \\right)^{-1}, \\\\\n\\mathbf{b}_{v,t} \\equiv &(\\mathbf{E}_{v,t}' \\mathbf{H}_e/\\sigma^2_e + \\mathbf{S}_{v,t}' \\mathbf{H}_{s,t}/\\sigma^2_s +  (\\mathbf{u}_{t+1} - \\mathbf{c}_{u,t})' \\theta_{uv}/\\sigma^2_u  + (\\mathbf{v}_{t+1} - \\mathbf{c}_{v,t})' \\theta_{vv}/\\sigma^2_v \\\\ &+  (\\mathbf{c}_{v,t-1} + \\theta_{vv} \\mathbf{v}_{t-1})'/\\sigma^2_v  )',  \n\\end{align*}\\]\nwith\n\\[\\begin{align*}\n\\mathbf{c}_{u,t} &\\equiv \\theta_{uu} \\mathbf{u}_t  + \\theta_{up} \\mathbf{D}_x \\mathbf{P}_t,\\\\\n\\mathbf{c}_{v,t} &\\equiv \\theta_{vu} \\mathbf{u}_t + \\theta_{vp} \\mathbf{D}_y \\mathbf{P}_t,\\\\\n\\mathbf{c}_{v,t-1} &\\equiv \\theta_{vu} \\mathbf{u}_{t-1} + \\theta_{vp} \\mathbf{D}_y \\mathbf{P}_{t-1}.\n\\end{align*}\\]\n\nFull conditional distribution for \\(\\mathbf{v}_T\\):\n\n\n\\([\\mathbf{v}_T | \\cdot] \\propto [\\) Equation E.6 \\(]_T \\times [\\) Equation E.8 \\(]_T \\times [\\) Equation E.10 \\(]_{T}\\)\n\n\\[\n\\mathbf{v}_T | \\cdot \\sim Gau(\\mathbf{A}_{v,T} \\mathbf{b}_{v,T}, \\mathbf{A}_{v,T})\n\\tag{E.20}\\]\nwhere\n\\[\n\\mathbf{A}_{v,T} \\equiv \\left( \\mathbf{H}_e' \\mathbf{H}_e/\\sigma^2_e + \\mathbf{H}_{s,T}' \\mathbf{H}_{s,T}/\\sigma^2_s +  \\, \\mathbf{I}/\\sigma^2_{v}  \\right)^{-1},\n\\]\n\\[\n\\mathbf{b}_{v,T} \\equiv (\\mathbf{E}_{v,T}' \\mathbf{H}_e/\\sigma^2_e + \\mathbf{S}_{v,T}' \\mathbf{H}_{s,T}/\\sigma^2_s +   (\\mathbf{c}_{v,T-1} + \\theta_{vv} \\mathbf{v}_{T-1})'/\\sigma^2_v  )',\n\\]\nwith\n\\[\n\\mathbf{c}_{v,T-1} \\equiv \\theta_{vu} \\mathbf{u}_{T-1} + \\theta_{vp} \\mathbf{D}_y \\mathbf{P}_{T-1}.\n\\]\n\nFull conditional distribution for \\(\\theta_{uu}\\):\n\n\n\\([\\theta_{uu} | \\cdot] \\propto \\prod_{t=1}^{T-1} [\\) Equation E.9 \\(]_{t+1} \\times [\\) Equation E.13 \\(]_{(uu)}\\)\n\n\\[\n\\theta_{uu} | \\cdot \\sim Gau(A_{uu} b_{uu}, A_{uu})\n\\tag{E.21}\\]\nwhere\n\\[\nA_{uu} \\equiv \\left( \\sum_{t=1}^{T-1} \\mathbf{u}_t' \\mathbf{u}_t /\\sigma^2_{u} + 1/\\sigma^2_{uu} \\right)^{-1},\n\\]\n\\[\nb_{uu} \\equiv  \\sum_{t=1}^{T-1} (\\mathbf{u}_{t+1} - \\mathbf{k}_{v,t})' \\mathbf{u}_t /\\sigma^2_u + \\mu_{uu}/\\sigma^2_{uu},\n\\]\nwith\n\\[\n\\mathbf{k}_{v,t} \\equiv \\theta_{uv} \\mathbf{v}_t + \\theta_{up} \\mathbf{D}_x \\mathbf{P}_t.\n\\]\n\nFull conditional distribution for \\(\\theta_{vv}\\):\n\n\n\\([\\theta_{vv} | \\cdot] \\propto \\prod_{t=1}^{T-1} [\\) Equation E.10 \\(]_{t+1} \\times [\\) Equation E.13 \\(]_{(vv)}\\)\n\n\\[\n\\theta_{vv} | \\cdot \\sim Gau(A_{vv} b_{vv}, A_{vv})\n\\tag{E.22}\\]\nwhere\n\\[\nA_{vv} \\equiv \\left( \\sum_{t=1}^{T-1} \\mathbf{v}_t' \\mathbf{v}_t /\\sigma^2_{v} + 1/\\sigma^2_{vv} \\right)^{-1},\n\\]\n\\[\nb_{vv} \\equiv  \\sum_{t=1}^{T-1} (\\mathbf{v}_{t+1} - \\mathbf{k}_{u,t})' \\mathbf{v}_t /\\sigma^2_v + \\mu_{vv}/\\sigma^2_{vv},\n\\]\nwith\n\\[\n\\mathbf{k}_{u,t} \\equiv \\theta_{vu} \\mathbf{u}_t + \\theta_{vp} \\mathbf{D}_y \\mathbf{P}_t.\n\\]\n\nFull conditional distribution for \\(\\theta_{uv}\\):\n\n\n\\([\\theta_{uv} | \\cdot] \\propto \\prod_{t=1}^{T-1} [\\) Equation E.9 \\(]_{t+1} \\times [\\) Equation E.13 \\(]_{(uv)}\\)\n\n\\[\n\\theta_{uv} | \\cdot \\sim Gau(A_{uv} b_{uv}, A_{uv})\n\\tag{E.23}\\]\nwhere\n\\[\nA_{uv} \\equiv \\left( \\sum_{t=1}^{T-1} \\mathbf{v}_t' \\mathbf{v}_t /\\sigma^2_{u} + 1/\\sigma^2_{uv} \\right)^{-1},\n\\]\n\\[\nb_{uv} \\equiv  \\sum_{t=1}^{T-1} (\\mathbf{u}_{t+1} - \\mathbf{k}_{u,t})' \\mathbf{v}_t /\\sigma^2_u + \\mu_{uv}/\\sigma^2_{uv},\n\\]\nwith\n\\[\n\\mathbf{k}_{u,t} \\equiv \\theta_{uu} \\mathbf{u}_t + \\theta_{up} \\mathbf{D}_x \\mathbf{P}_t.\n\\]\n\nFull conditional distribution for \\(\\theta_{vu}\\):\n\n\n\\([\\theta_{vu} | \\cdot] \\propto \\prod_{t=1}^{T-1} [\\) Equation E.10 \\(]_{t+1} \\times [\\) Equation E.13 \\(]_{(vu)}\\)\n\n\\[\n\\theta_{vu} | \\cdot \\sim Gau(A_{vu} b_{vu}, A_{vu})\n\\tag{E.24}\\]\nwhere\n\\[\nA_{vu} \\equiv \\left( \\sum_{t=1}^{T-1} \\mathbf{u}_t' \\mathbf{u}_t /\\sigma^2_{v} + 1/\\sigma^2_{vu} \\right)^{-1},\n\\]\n\\[\nb_{vu} \\equiv  \\sum_{t=1}^{T-1} (\\mathbf{v}_{t+1} - \\mathbf{k}_{v,t})' \\mathbf{u}_t /\\sigma^2_v + \\mu_{vu}/\\sigma^2_{vu},\n\\]\nwith\n\\[\n\\mathbf{k}_{v,t} \\equiv \\theta_{vv} \\mathbf{v}_t + \\theta_{vp} \\mathbf{D}_y \\mathbf{P}_t.\n\\]\n\nFull conditional distribution for \\(\\theta_{up}\\):\n\n\n\\([\\theta_{up} | \\cdot] \\propto \\prod_{t=1}^{T-1} [\\) Equation E.9 \\(]_{t+1} \\times [\\) Equation E.13 \\(]_{(up)}\\)\n\n\\[\n\\theta_{up} | \\cdot \\sim Gau(A_{up} b_{up}, A_{up})\n\\tag{E.25}\\]\nwhere\n\\[\nA_{up} \\equiv \\left( \\sum_{t=1}^{T-1} (\\mathbf{D}_x \\mathbf{P}_t)'  (\\mathbf{D}_x \\mathbf{P}_t) /\\sigma^2_{u} + 1/\\sigma^2_{up} \\right)^{-1},\n\\] \\[\nb_{up} \\equiv  \\sum_{t=1}^{T-1} (\\mathbf{u}_{t+1} - \\mathbf{k}_{u,t})' \\mathbf{D}_x \\mathbf{P}_t /\\sigma^2_u + \\mu_{up}/\\sigma^2_{up},\n\\]\nwith\n\\[\n\\mathbf{k}_{u,t} \\equiv \\theta_{uu} \\mathbf{u}_t + \\theta_{uv} \\mathbf{v}_t.\n\\]\n\nFull conditional distribution for \\(\\theta_{vp}\\):\n\n\n\\([\\theta_{vp} | \\cdot] \\propto \\prod_{t=1}^{T-1} [\\) Equation E.10 \\(]_{t+1} \\times [\\) Equation E.13 \\(]_{(vp)}\\)\n\n\\[\n\\theta_{vp} | \\cdot \\sim Gau(A_{vp} b_{vp}, A_{vp})\n\\tag{E.26}\\]\nwhere\n\\[\nA_{vp} \\equiv \\left( \\sum_{t=1}^{T-1} (\\mathbf{D}_y \\mathbf{P}_t)'  (\\mathbf{D}_y \\mathbf{P}_t) /\\sigma^2_{v} + 1/\\sigma^2_{vp} \\right)^{-1},\n\\] \\[\nb_{vp} \\equiv  \\sum_{t=1}^{T-1} (\\mathbf{v}_{t+1} - \\mathbf{k}_{v,t})' \\mathbf{D}_y \\mathbf{P}_t /\\sigma^2_v + \\mu_{vp}/\\sigma^2_{vp},\n\\]\nwith\n\\[\n\\mathbf{k}_{v,t} \\equiv \\theta_{vv} \\mathbf{u}_t + \\theta_{vu} \\mathbf{u}_t.\n\\]\n\nFull conditional distribution for \\(\\sigma^2_u\\):\n\n\n\\([\\sigma^2_u | \\cdot] \\propto \\prod_{t=1}^{T-1} [\\) Equation E.9 \\(]_{t+1} \\times [\\) Equation E.14 \\(]_{(u)}\\)\n\n\\[\n\\sigma^2_u | \\cdot \\sim IG(q_{\\mathrm{new},u},r_{\\mathrm{new},u})\n\\tag{E.27}\\]\nwhere\n\\[\nq_{\\mathrm{new},u} = q_u + (T-1)n_g /2,  \n\\] \\[\nr_{\\mathrm{new},u} = \\left(\\frac{1}{r_u} + \\frac{1}{2} \\sum_{t=1}^{T-1} (\\mathbf{u}_{t+1} - \\mathbf{k}_{u,t})' (\\mathbf{u}_{t+1} - \\mathbf{k}_{u,t})  \\right)^{-1},\n\\]\nwith\n\\[\n\\mathbf{k}_{u,t} \\equiv \\theta_{uu} \\mathbf{u}_t + \\theta_{uv} \\mathbf{v}_t + \\theta_{up} \\mathbf{D}_x \\mathbf{P}_t.\n\\]\n\nFull conditional distribution for \\(\\sigma^2_v\\):\n\n\n\\([\\sigma^2_v | \\cdot] \\propto \\prod_{t=1}^{T-1} [\\) Equation E.10 \\(]_{t+1} \\times [\\) Equation E.14 \\(]_{(v)}\\)\n\n\\[\n\\sigma^2_v | \\cdot \\sim IG(q_{\\mathrm{new},v},r_{\\mathrm{new},v})\n\\tag{E.28}\\]\nwhere\n\\[\nq_{\\mathrm{new},v} = q_v + (T-1)n_g /2,  \n\\] \\[\nr_{\\mathrm{new},v} = \\left(\\frac{1}{r_v} + \\frac{1}{2} \\sum_{t=1}^{T-1} (\\mathbf{v}_{t+1} - \\mathbf{k}_{v,t})' (\\mathbf{v}_{t+1} - \\mathbf{k}_{v,t})  \\right)^{-1},\n\\]\nwith\n\\[\n\\mathbf{k}_{v,t} \\equiv \\theta_{vv} \\mathbf{v}_t + \\theta_{vu} \\mathbf{u}_t + \\theta_{vp} \\mathbf{D}_y \\mathbf{P}_t.\n\\]\n\nImplementation in R\n\nR Preliminaries\nWe will need the Matrix package because the BHM Gibbs sampler uses sparse matrices, and ggquiver and ggmap to make “quiver” plots of the wind vectors on a map of the Mediterranean region.\n\nlibrary(\"Matrix\")\nlibrary(\"ggmap\")\nlibrary(\"ggquiver\")\nlibrary(\"STRbook\")\n\nThe functions needed for this case study are provided with STRbook. Two functions designed to work for this specific application are Medwind_BHM_preproc and Medwind_BHM, which we describe in more detail below. Their purpose is to show that this realistic, complex, science-motivated spatio-temporal BHM can be analyzed in R using the dynamical approach described in Chapter 5. It is worth browsing through the code of these functions to see how it is implemented (visit https://github.com/andrewzm/STRbook).\n\n\nPreprocessing the Data and Model Setup\nThe function Medwind_BHM_preproc is a preprocessor function that takes the following as arguments:\n\nEdat: A list of four items\n\nECMWFxylocs: Data frame containing the \\((x,y)\\) coordinates on which the wind vectors and pressures are defined\nEUdat: Data frame containing the east–west (\\(u\\)) component of the ECMWF wind vector (in units of m/s) in time-wide format\nEVdat: Data frame containing the north–south (\\(v\\)) component of the ECMWF wind vector (in units of m/s) in time-wide format\nEPdat: Data frame containing the ECMWF atmospheric pressure (in pascals (Pa)) in time-wide format.\n\nSdat: A list of three items\n\nSxylocs: A list of items (one per time point) containing the spatial locations (for each time point) of the scatterometer data\nSUdat: The east–west (\\(u\\)) component of the QuikSCAT wind vector (in units of m/s)\nSVdat: The north–south (\\(v\\)) component of the QuikSCAT wind vector (in units of m/s)\n\nPredlocs: Data frame containing the \\((x,y)\\) coordinates of the spatial prediction grid.\nInparm: Other parameters for the Gibbs sampler, discussed further below.\n\nThe data objects required for this application, Edat, Sdat, and Predlocs, can be loaded from STRbook as follows:\n\ndata(\"Medwind_data\")\n\nThe data here correspond to 28 time periods from 00:00 UTC on 29 January 2005 to 18:00 UTC on 04 February 2005 (every 6 hours). The ECMWF analysis winds and pressure are on a \\(0.5^\\circ \\times 0.5^\\circ\\) grid and the QuikSCAT scatterometer winds are polar-orbiting satellite observations at a finer resolution (see the description in Milliff et al., 2011). There are typically no QuikSCAT observations in the prediction domain considered here at 00:00 UTC and 12:00 UTC. The prediction grid consists of 1035 (\\(n_y = 23\\), \\(n_x = 45\\)) grid locations with a \\(0.5^\\circ\\) spacing; this prediction grid coincides with the interior grid points of the ECMWF domain; see Section 2.1 for more details.\nThe preprocessor argument Inparm is a list of several parameters. These parameters are associated with the prediction grid, the distance to search for data near prediction-grid locations, and hyperparameters for the parameter model (prior) distributions. In particular, the item gspdeg is the prediction-grid spacing in degrees, and srad and erad correspond to the distance (in degrees) to search for ECMWF and QuikSCAT data locations, respectively, centered on a prediction-grid location. So srad = 0.5 would mean that we would identify all QuikSCAT observations within 0.25 degrees of a prediction-grid location. The values of hx and hy correspond to the average longitudinal and latitudinal spacing (in meters), respectively, between the prediction-grid locations. With regard to the fixed hyperparameters in Inparm, the variables s2e and s2s are the measurement-error variances for the ECMWF and QuikSCAT wind data (\\(\\sigma^2_e\\) and \\(\\sigma^2_s\\)), respectively, as given in the data-model equations Equation E.5–Equation E.8. These are assumed to be known (Milliff et al., 2011). The variables mu_pri and s2_pri corresponding to the normal-distribution-prior mean (\\(\\mu_{ab}\\)) and variance (\\(\\sigma^2_{ab}\\)) for the \\(\\theta\\) parameters, given in model Equation E.13, are also fixed (at \\(0\\) and \\(10^6\\), respectively, corresponding to a vague prior for \\(\\theta_{ab}\\)). IGshape and IGrate are the prior shape (\\(q_a\\)) and rate (\\(r_a\\)) parameters, respectively, for an inverse gamma prior on the process-model error variances given in Equation E.14; these are fixed at 1 and 1, respectively, corresponding to a vague prior for \\(\\sigma^2_a\\).\n\n##     parameters to control the grid, data search, and priors\n##\n  preprocInput = list(\n    gspdeg = .5,\n    srad = .5,\n    erad = .5,\n    hx = 2*19.42865*1000,\n    hy = 2*27.75*1000,\n    s2s = 1,\n    s2e = 10,\n    mu_pri = 0,\n    s2_pri = 10^6,\n    IGshape = 1,\n    IGrate = 1\n  )\n\nThe function Medwind_BHM_preproc takes the data and other parameters and then builds the data-model incidence matrices (\\(\\mathbf{H}_e\\), \\(\\mathbf{H}_{s,t}\\)) given in equations Equation E.5–Equation E.8, and the difference operator matrices (\\(\\mathbf{D}_x\\), \\(\\mathbf{D}_y\\)) given in equations Equation E.3 and Equation E.4, respectively. The returned list (denoted below as Mpre) contains the lists Mdata (data), Mgrid (grid), Mpriors (prior hyperparameters), and Mstrt (MCMC starting values). This list is used in the Gibbs sampler given in the next section. Recall that the Gibbs sampler is an MCMC algorithm that produces samples from the posterior distribution of all the “unknowns” given the data.\n\nMpre &lt;- Medwind_BHM_preproc(Edat = Edat,\n                            Sdat = Sdat,\n                            Predlocs = Predlocs,\n                            Inparm = preprocInput)\n\n\n\nRunning the Gibbs Sampler\nThis section provides the commands necessary to implement the Gibbs sampler presented in Note E.1. We specify the parameters that control the number of Gibbs sampler iterations (ngibbs), the number of burn-in samples (nburn), and the number of iterations to save in memory (nreal).\n\nGibbsInput = list(ngibbs = 10000,\n                  nburn = 1000,\n                  nreal = 10)\n\nThe Gibbs sampler for this problem takes the arguments GibbsInput, defined as above, and the output of the pre-processor, Mpre.\n\nset.seed(1) # ensure reproducibility\nMout &lt;- Medwind_BHM(GibbsInput, Mpre)\n\nThe algorithm can take quite a long time to run in order to obtain a reasonable number of iterations. For this Lab, the output can be loaded directly from STRbook, if desired, as follows.\n\ndata(\"Medwind_Gibbs_output\")\n\nThe Gibbs sampler function Medwind_BHM returns a list that includes posterior means, posterior standard deviations, and nreal realizations for the \\(u\\) and \\(v\\) wind components, as well as all of the iterations for the \\(\\theta\\) parameters and the process-model variances. Specifically, in this case, the list contains the following items:\n\nuS: posterior mean of the \\(u\\) components (1035 locations \\(\\times\\) 28 time points)\nvS: posterior mean of the \\(v\\) components (1035 locations \\(\\times\\) 28 time points)\nuSTD: posterior standard deviation of the \\(u\\) components (1035 locations \\(\\times\\) 28 time points)\nvSTD: posterior standard deviation of the \\(v\\) components (1035 locations \\(\\times\\) 28 time points)\nuSreal: nreal realizations of all 1035 \\(\\times\\) 28 locations/time points for the \\(u\\) components (list)\nvSreal: nreal realizations of all 1035 \\(\\times\\) 28 locations/time points for the \\(v\\) components (list)\ntheta_xxS (xx = uu,vv,uv,vu,up,vp): ngibbs samples for the \\(\\theta\\) parameters\ns2uS,s2vS: ngibbs samples for the variance parameters.\n\nWe reiterate that both the pre-processor and the Gibbs sampler functions are specifically designed for this BHM fitted to the Mediterranean winds data. They would need substantial modification for different BHMs fitted to different data sets using different space-time grids. Advanced readers could examine and modify the code contained in the STRbook package for their applications.\n\n\nExamining the Model Output\nIt is customary to plot the Gibbs sampler output against iteration number of the Gibbs sampler, to provide visual evidence that the samples have reasonably converged. More formal diagnostics for MCMC convergence can be found in the coda package. In Figure E.1, we plot the post burn-in samples for \\(\\theta_{up}\\) and \\(\\sigma^2_{u}\\) as a demonstration of the code and the graphics.\n\nindx &lt;- (GibbsInput$nburn+1):GibbsInput$ngibbs #plot post \"burn-in\"\n\np1 &lt;- ggplot(data.frame(indx = indx,\n                        theta_up = Mout$theta_upS[indx]),\n             aes(x = indx,y = theta_up)) + geom_line() +\n              ylab(expression(theta[up])) + theme_bw()\n\np2 &lt;- ggplot(data.frame(indx = indx, s2u = Mout$s2uS[indx]),\n             aes(x = indx,y = s2u)) + geom_line() +\n              ylab(expression(sigma[vu]^2)) + theme_bw()\n\n\n\n\n\n\n\nFigure E.1: Post burn-in trace plots for MCMC samples from the posterior distribution, plotted against iteration number indx. Top: MCMC samples for \\(\\theta_{up}\\). Bottom: MCMC samples for \\(\\sigma^2_{vu}\\).\n\n\n\nFor inference on the model’s parameters, we are usually interested in their marginal posterior distributions. In the top-left panel of Figure E.2, we plot the posterior distribution for \\(\\theta_{up}\\) and give the code below; coding for the other parameters in the process-model equations Equation E.3 and Equation E.4 proceeds in a similar fashion.\n\np1 &lt;- ggplot(data.frame(theta_up = Mout$theta_upS[indx]),\n             aes(x = theta_up)) + geom_density() +\n             geom_vline(aes(xintercept = mean(theta_up)),\n             color = \"red\", linetype = \"dashed\", size = 1) +\n             xlab(expression(theta[up])) + theme_bw()\n\n\n\n\n\n\n\nFigure E.2: The marginal posterior distributions (after applying a kernel smoother to the MCMC samples) for the process-model parameters. The red dashed line shows the posterior mean for each distribution.\n\n\n\nFinally, the main goal in this case study was to fuse the ECMWF and QuikSCAT wind observations to generate a posterior probability distribution on wind speeds. In Figure E.3, we plot the posterior mean and a posterior realization quiver plot for the winds for 06:00 UTC on 01 February 2005, using the following code.\n\nlibrary(\"rnaturalearth\")\n\n## Extract mean\nu14 &lt;- Mout$uS[, 14]              #time 14 is Feb 1, 2005 06 UTC\nv14 &lt;- Mout$vS[, 14]\n\n## Extract realization\nu14r &lt;- Mout$uSreal[[5]][, 14]    #consider the 5th realization\nv14r &lt;- Mout$vSreal[[5]][, 14]\n\n## Get map using get_map\nlat &lt;- c(34, 45)\nlong &lt;- c(-6, 16)\n\n## Create grid on which to plot\nxg &lt;- Mpre$Mgrid$Mgridxylocs[, 1]\nyg &lt;- Mpre$Mgrid$Mgridxylocs[, 2]\nc2 &lt;-  expand.grid(x = seq(long[1], long[2], 0.5),\n                   y = seq(lat[1], lat[2], 0.5))\n\n## Construct land and ocean polygons\nland &lt;- ne_countries(scale = \"medium\", returnclass = \"sf\")\nocean &lt;- ne_download(\n  scale = \"medium\",\n  type = \"ocean\",\n  category = \"physical\",\n  returnclass = \"sf\"\n)\n\n## Plot the posterior mean and realization\np1 &lt;- ggplot() + \n        geom_sf(data = ocean, fill = \"#7EC8E3\", colour = NA) +  # blue ocean\n        geom_sf(data = land,  fill = \"#F5E5D0\", colour = \"grey60\", linewidth = 0.2) +\n        geom_quiver(data = c2,\n                aes(x = xg, y = yg, u = u14, v = v14),\n                vecsize = 1.5) +\n                coord_sf(xlim = long, ylim = lat)\n\np2 &lt;-  ggplot() + \n        geom_sf(data = ocean, fill = \"#7EC8E3\", colour = NA) +  # blue ocean\n        geom_sf(data = land,  fill = \"#F5E5D0\", colour = \"grey60\", linewidth = 0.2) +\n        geom_quiver(data = c2,\n                aes(x = xg, y = yg, u = u14r, v = v14r),\n                vecsize = 1.5) +\n        coord_sf(xlim = long, ylim = lat)\n\n\n\n\n\n\n\nFigure E.3: Quiver plots derived from the posterior distribution of winds for 06:00 UTC on 01 February 2005. Top: Posterior mean. Bottom: A single realization from the posterior distribution.\n\n\n\n\n\n\n\nCressie, N., & Wikle, C. K. (2011). Statistics for spatio-temporal data. John Wiley & Sons.\n\n\nGelman, A., Carlin, J. B., Stern, H. S., Dunson, D. B., Vehtari, A., & Rubin, D. B. (2014). Bayesian data analysis (3rd ed.). Chapman & Hall/CRC.\n\n\nMilliff, R. F., Bonazzi, A., Wikle, C. K., Pinardi, N., & Berliner, L. M. (2011). Ocean ensemble forecasting. Part i: Ensemble mediterranean winds from a bayesian hierarchical model. Quarterly Journal of the Royal Meteorological Society, 137(657), 858–878.\n\n\nStevens, B., Duan, J., McWilliams, J. C., Münnich, M., & Neelin, J. D. (2002). Entrainment, rayleigh friction, and boundary layer winds over the tropical pacific. Journal of Climate, 15(1), 30–44.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>E</span>  <span class='chapter-title'>Case Study: Physical-Statistical Bayesian Hierarchical Model for Predicting Mediterranean Surface Winds</span>"
    ]
  },
  {
    "objectID": "ChapterAppendixF.html",
    "href": "ChapterAppendixF.html",
    "title": "Appendix F — Case Study: Quadratic Echo State Networks for Sea Surface Temperature Long-Lead Prediction",
    "section": "",
    "text": "Recall from Section 5.4 that recurrent neural networks (RNNs) were developed in the engineering and machine-learning literature to accommodate time-dependent cycles and sequences as well as the concept of “memory” in a neural network. But, like the statistical GQN (also discussed in Section 5.4), RNNs have an extremely high-dimensional parameter space and can be difficult to fit. In contrast, the echo state network (ESN) is a type of RNN that considers sparsely connected hidden layers that allow for sequential interactions, yet specifies (remarkably) most of the parameters (“weights”) to be randomly generated and then fixed, with only the parameters that connect the hidden layer to the response being estimated (see the overview in Jaeger (2007)). We consider a modification of the ESN in this case study. Note that although the models presented here are relatively simple to implement, the notational burden and (especially) the machine-learning jargon can take some getting used to.\nA simple representation of an ESN is given by the following hierarchical model for data vector \\(\\mathbf{Z}_t\\) (assumed to be \\(m\\)-dimensional here):\n\\[\n\\mathbf{Z}_t = g_o(\\mathbf{V}\\mathbf{h}_t),\n\\tag{F.1}\\]\n\\[\n\\mathbf{h}_t = g_h(\\mathbf{W}\\mathbf{h}_{t-1} + \\mathbf{U}\\mathbf{x}_t).\n\\tag{F.2}\\]\nIn the data model (Equation F.1) and the process model (Equation F.2), \\(\\mathbf{h}_t\\) is an \\(n\\)-dimensional vector of latent (“hidden”) states, \\(\\mathbf{x}_t\\) is a \\(p\\)-dimensional input vector, \\(\\mathbf{V}\\) is an \\(m \\times n\\) output-parameter weight matrix, \\(\\mathbf{W}\\) is an \\(n \\times n\\) hidden-process-evolution-parameter weight matrix, \\(\\mathbf{U}\\) is an \\(n \\times p\\) input-parameter weight matrix, and \\(g_o(\\cdot)\\) and \\(g_h(\\cdot)\\) are so-called “activation functions” (e.g., identity, softmax, hyperbolic tangent). The hidden-state model Equation F.2 is sometimes called a “reservoir.” This reservoir is key to this modeling framework in that the parameter weight matrices in Equation F.2, \\(\\mathbf{W}\\) and \\(\\mathbf{U}\\), are sparse (only 1–10% of the parameters are non-zero) with non-zero elements chosen at random and fixed (for details, see the example that follows). This means that only the output weights (in \\(\\mathbf{V}\\)) are estimated, substantially reducing the estimation burden. In most applications, \\(g_o(\\cdot)\\) is the identity function, and \\(\\mathbf{V}\\) can be estimated with regression methods that include regularization, such as a ridge regression or a lasso penalty (see Note 3.4). These models work surprisingly well for forecasting central tendency and classification, but they are limited for inference and uncertainty quantification. Notice that there are no error terms in this model!\nMcDermott & Wikle (2017) modified the basic ESN algorithm for use with spatio-temporal data to include quadratic nonlinear outputs, so-called “embedding inputs” (see below), and reservoir parameter uncertainty by considering an ensemble (bootstrap) sample of forecasts. Their quadratic echo state network (QESN), for \\(t=1,\\ldots,T\\), is given by:\nResponse: \\[\n{\\mathbf{Y}}_t = {\\mathbf{V}}_1 {\\mathbf{h}}_t + {\\mathbf{V}}_2 {\\mathbf{h}}^2_t + {\\boldsymbol{\\epsilon}}_t, \\quad \\textrm{for} \\quad {\\boldsymbol{\\epsilon}}_t \\; \\sim \\; Gau({\\mathbf{0}},\\sigma^2_\\epsilon \\mathbf{I});\n\\tag{F.3}\\]\nHidden state: \\[\n{\\mathbf{h}}_t = g_h\\left(\\frac{\\nu}{|\\lambda_w|}{\\mathbf{W}} {\\mathbf{h}}_{t-1} + {\\mathbf{U}}\\tilde{\\mathbf{x}}_t\\right);\n\\tag{F.4}\\]\nParameters: \\[\n{\\mathbf{W}} = [w_{i,\\ell}]_{i,\\ell}: w_{i,\\ell} = \\gamma^w_{i,\\ell} \\; Unif(-a_w,a_w) + (1 - \\gamma^w_{i,\\ell}) \\; \\delta_0,\n\\tag{F.5}\\]\n\\[\n{\\mathbf{U}} = [u_{i,j}]_{i,j}: u_{i,j} = \\gamma^u_{i,j} \\; Unif(-a_u,a_u) + (1 - \\gamma^u_{i,j}) \\; \\delta_0,\n\\tag{F.6}\\]\n\\[\n\\gamma_{i,\\ell}^w \\; \\sim \\; Bern(\\pi_w),\n\\tag{F.7}\\]\n\\[\n\\gamma_{i,j}^u \\; \\sim \\; Bern(\\pi_u)\n\\tag{F.8}\\]\nwhere \\({\\mathbf{Y}}_t\\) is the \\(n_y\\)-dimensional response vector at time \\(t\\); \\({\\mathbf{h}}_t\\) is the \\(n_h\\)-dimensional hidden-state vector; \\({\\mathbf{h}}^2_t\\) is the \\(n-h\\)-dimensional vector where the square operation is defined elementwise;\n\\[\n\\tilde{\\mathbf{x}}_t = [\\mathbf{x}'_t,\\mathbf{x}'_{t-\\tau*}, \\mathbf{x}'_{t- 2 \\tau*},\\ldots,\\mathbf{x}'_{t - m \\tau*}]'\n\\tag{F.9}\\]\nis the \\(n_{\\tilde{x}} = (m+1) n_x\\)-dimensional “embedding input” vector, containing lagged values (embeddings) of the inputs \\(\\{\\mathbf{x}_t\\}\\) for time periods \\(t- \\tau*\\) through \\(t - m \\tau*\\), where the quantity \\(\\tau*\\) is the embedding lag (a positive integer, often set equal to the forecast lead time); and \\(Bern(\\cdot)\\) denotes the Bernoulli distribution. As in the basic ESN above, \\(\\mathbf{W}\\) is the \\(n \\times n\\) hidden-process-evolution weight matrix, \\(\\mathbf{U}\\) is the \\(n \\times p\\) input weight matrix, and \\(\\mathbf{V}_1\\), \\(\\mathbf{V}_2\\) are the \\(n \\times n_h\\) linear and quadratic output weight matrices, respectively. Furthermore, \\(\\delta_0\\) is a Kronecker delta function at zero, \\(\\lambda_w\\) corresponds to the largest eigenvalue of \\({\\mathbf{W}}\\) (i.e., the “spectral radius” of \\(\\mathbf{W}\\)), and \\(\\nu\\) is a spectral-radius control parameter. The “activation function” \\(g_h(\\cdot)\\) (a hyperbolic tangent function in our application below) controls the nonlinearity of the hidden-state evolution. The only parameters that are estimated in this model are \\({\\boldsymbol{V}}_1\\), \\({\\mathbf{V}}_2\\), and \\(\\sigma^2_\\epsilon\\) from Equation F.3, for which we require a ridge-regression penalty parameter, \\(r_v\\) (see Note 3.4). Importantly, note that the matrices \\({\\mathbf{W}}\\) and \\({\\mathbf{U}}\\) are simulated from mixture distributions of small values (uniformly sampled in the range \\((-a_w,a_w)\\) and \\((-a_u,a_u)\\), respectively) with, respectively, \\((1-\\pi_w)\\) and \\((1-\\pi_u)\\) elements set equal to zero on average. After being sampled, these parameters are assumed to be fixed and known. Typically, these weight matrices are very sparse (e.g., of the order of \\(1\\)–\\(10\\%\\) non-zeros). The hyperparameters, \\(\\{\\nu, n_h, r_v, \\pi_w, \\pi_u, a_w, a_u\\}\\), are usually chosen by cross-validation.\nAs is the case in most traditional ESN applications, the QESN model does not have an explicit mechanism to quantify uncertainty in the process or in the parameters. This is a bit troubling given that the reservoir weight matrices \\(\\mathbf{W}\\) and \\(\\mathbf{U}\\) are not estimated, but are chosen at random. We would expect that the model is likely to behave differently with a different set of weight matrices. This is especially true when the number of hidden units is fairly small. Although traditional ESN models typically have a very large number of hidden units, which tends to give more stable predictions, it can be desirable to have many different forecasts using a smaller number of hidden units. This provides flexibility in that it prevents overfitting, allows the various forecasts to behave as a “committee of relatively weak learners,” and gives a more realistic sense of the prediction uncertainty for out-of-sample forecasts. Thus, we could generate an ensemble or bootstrap sample of forecasts. As shown in McDermott & Wikle (2017), this ensemble approach can be implemented straightforwardly with the QESN model using the algorithm in Note F.1.\n\n\n\n\n\n\nNote F.1: Ensemble QESN Algorithm\n\n\n\nInitialize: Select tuning parameters \\(\\{\\nu, n_h, r_v, \\pi_w, \\pi_u, a_w, a_u\\}\\) (e.g., by cross-validation with a standalone QESN)\nfor \\(k = 1\\) to \\(K\\) do\n\nSimulate \\(\\mathbf{W}^{(k)}\\), \\(\\mathbf{U}^{(k)}\\) using Equation F.5 and Equation F.6 and initialize \\({\\mathbf{h}}_1^{(k)}\\)\nCalculate \\(\\{{\\mathbf{h}}_t^{(k)}: t=2,...,T\\}\\) using Equation F.4\nUse ridge regression to estimate \\(\\mathbf{V}^{(k)}_1, \\mathbf{V}^{(k)}_2\\), and \\(\\sigma^2_\\epsilon\\)\nCalculate out-of-sample forecasts \\(\\{\\widehat{\\mathbf{Y}}^{(k)}_t: t=T+1,...,T+\\tau\\}\\), where \\(\\tau\\) is the forecast lead time (requires calculating \\(\\{\\widehat{{\\mathbf{h}}}^{(k)}_t: t=T+1,...,T+\\tau\\}\\) from Equation F.4)\n\nend for\nUse ensemble of forecasts \\(\\{\\widehat{\\mathbf{Y}}^{(k)}_t: t=T+1,...,T+\\tau; k = 1,...,K\\}\\) to calculate moments, prediction intervals, etc.\n\n\n\nImplementation in R\nIn what follows, we provide a demonstration of the ensemble QESN model applied to long-lead forecasting of sea-surface temperature using the SST data set.\n\nEnsemble QESN Model Data Preparation\nTo prepare the data, we need ggplot2, dplyr, STRbook, and tidyr.\n\nlibrary(\"ggplot2\")\nlibrary(\"dplyr\")\nlibrary(\"STRbook\")\nlibrary(\"tidyr\")\n\nThe functions needed for this case study are provided with STRbook. Our purpose here is to show that this nonlinear DSTM can be implemented in R fairly easily. If readers are interested in adapting these functions to their own applications, it is worth browsing through the functions to see how the code is implemented (visit https://github.com/andrewzm/STRbook).\nWe first load the SST data set. This time we shall use the data up to October 1996 as training data and perform out-of-sample six-month forecasts from April 1997 to July 1999.\n\ndata(\"SSTlandmask\")\ndata(\"SSTlonlat\")\ndata(\"SSTdata\")\ndelete_rows &lt;- which(SSTlandmask == 1)   # find land values\nSSTdataA &lt;- SSTdata[-delete_rows, ]      # remove land values\n\nIn this application, we shall evaluate the forecast in terms of the time series corresponding to the average of the SST anomalies in the so-called Niño 3.4 region (defined to be the region of the tropical Pacific Ocean contained by \\(5^\\circ\\)S–\\(5^\\circ\\)N, \\(170^\\circ\\)W–\\(120^\\circ\\)W).\n\n## find grid locations corresponding to Nino 3.4 region;\n## note, 190 - 240 deg E longitude corresponds\n## to 170 - 120 deg W longitude\nnino34Index &lt;- which(SSTlonlat[,2] &lt;= 5 & SSTlonlat[, 2] &gt;= -5 &\n                       SSTlonlat[,1] &gt;= 190 & SSTlonlat[, 1] &lt;= 240)\n\nThe object SSTdataA is a 2261 × 399 matrix in time-wide format. In the code below, we save the number of spatial locations in the variable nspatial. Of the 399 time points, we only need 322 for training, the number of months between (and including) January 1970 and October 1996. We define a six-month-ahead forecast by specifying tau = 6.\n\nnspatial &lt;- nrow(SSTdataA)    # number of spat. locations\nTrainLen &lt;- 322               # no. of months to Oct 1996\ntau &lt;- 6                      # forecast lead time (months)\n\nWe train the ESN on time series associated with the first ten EOFs extracted from the SST (training) data. The following code follows closely what was done in Labs 2.3 and 5.3.\n\nn &lt;- 10                               # number of EOFs to retain\nZ &lt;- t(SSTdataA[, 1:TrainLen])        # data matrix\nspat_mean &lt;- apply(SSTdataA, 1, mean)        # spatial mean\nZspat_detrend &lt;- Z - outer(rep(1, TrainLen), # detrend data\n                           spat_mean)\nZt &lt;- 1/sqrt(TrainLen - 1)*Zspat_detrend     # normalize\nE &lt;- svd(Zt)                   # SVD\nPHI &lt;- E$v[, 1:n]              # 10 EOF spatial basis functions\nTS &lt;- t(SSTdataA) %*% PHI      # project data onto basis functions\n                               # for PC time series\n\nNow we need to create the training and validation data sets. Both data sets will need input data and output data. Since we are setting up the ESN for six-months-ahead forecasting, as input we use the PC time series (see Section 2.4.3) lagged by six months with respect to the output. For example, the PC time-series values at January 1970 are inputs (\\(\\mathbf{x}_t\\)) to forecast the SST (output) in July 1970. For prediction, we consider forecasting at ten three-month intervals starting from October 1996 (we chose three-month intervals to improve the visualization, but one can forecast each month if desired).\n\n## training set\nxTrainIndex &lt;- 1:(TrainLen - tau) # training period ind. for input\nyTrainIndex &lt;- (tau+1):(TrainLen) # shifted period ind. for output\nxTrain &lt;- TS[xTrainIndex, ]       # training input time series\nyTrain &lt;- TS[yTrainIndex, ]       # training output time series\n\n## test set: forecast every three months\nxTestIndex &lt;- seq(TrainLen, by = 3, length.out = 10)\nyTestIndex &lt;- xTestIndex+tau       # test output indices\nxTest &lt;- TS[xTestIndex,]           # test input data\nyTest &lt;- TS[yTestIndex,]           # test output data\ntestLen &lt;- nrow(xTest)             # number of test cases\n\n\n\nEnsemble QESN Model Implementation\nWe first have to make some model choices and set some parameters to run the ensemble QESN model. For model details and terminology, see the description above.\n\nquadInd &lt;- TRUE  # include both quadratic and linear output terms\n# if FALSE, then include only linear terms\nensembleLen &lt;- 500  # number of ensemble members (i.e., QESN runs)\n\nThe next set of parameters can be trained by cross-validation or out-of-sample validation (see McDermott & Wikle, 2017). For simplicity, we use the values obtained in that paper (which considered a similar long-lead SST forecasting application) here. The model arguments required as input are: wWidth, which corresponds to the parameter \\(a_w\\) that specifies the range of the uniform distribution for the \\(\\mathbf{W}\\) weight matrix parameters in Equation F.5; similarly, uWidth, which corresponds to the parameter \\(a_u\\) that specifies the range of the uniform distribution for the \\(\\mathbf{U}\\) matrix in Equation F.6; piW, which corresponds to \\(\\pi_w\\) in Equation F.7, the probability of a non-zero \\(\\mathbf{W}\\) weight; piU, which corresponds to \\(\\pi_u\\), the probability of non-zero \\(\\mathbf{U}\\) weight parameter in Equation F.8; curNh, which corresponds to \\(n_h\\), the number of hidden units; curNu, which corresponds to \\(\\nu\\), the spectral radius of the \\(\\mathbf{W}\\) matrix; curM, which corresponds to \\(m\\), the number of lags (embeddings) of input vectors to use; tauEMB, which corresponds to the embedding lag (\\(\\tau*\\) in Equation F.9); and curRV, which corresponds to \\(r_v\\), the ridge-regression parameter associated with the estimation of the output matrices, \\(\\mathbf{V}_1\\) and \\(\\mathbf{V}_2\\) in Equation F.3.\n\nwWidth &lt;- .10      # W-weight matrix, uniform dist \"width\" param.\nuWidth &lt;- .10      # U-weight matrix, uniform dist \"width\" param.\npiW &lt;- .10         # sparseness parameter for W-weight matrix\npiU &lt;- .10         # sparseness parameter for U-weight matrix\ncurNh &lt;- 120       # number of hidden units\ncurNu &lt;- .35       # scaling parameter for W-weight matrix\ncurM &lt;- 4          # number of embeddings\ntauEMB &lt;- 6        # embedding lag\ncurRV &lt;- .01       # output ridge regression parameter\n\nNow we use the function createEmbedRNNData to create a data object containing responses and embedding matrix inputs (see Equation F.9) for the training and prediction data sets (note that the responses and inputs are scaled by their respective standard deviations, as is common in the ESN literature). The function takes as inputs variables defined above: curM, the number of embedding lags; tauEMB, the embedding lag; tau, the forecast lead time; yTrain, the training output time series; TS, the input time series associated with the projection of the data onto the EOFs; and xTestIndex, which identifies the indices for the input data corresponding to the test periods.\n\n## standardize and create embedding matrices\nDataObj &lt;- createEmbedRNNData(curM, tauEMB, tau, yTrain, TS,\n                              xTestIndex)\n\nThe returned object, DataObj, is a list containing inputs and training data in the format required to train the ESN. We now need to create a parameter object that contains the parameters to be used in constructing the ESN. The function we use initializes the vectors associated with: the embedding matrix \\(\\tilde{\\mathbf{x}}_t\\) in Equation F.9; the hidden state \\(\\mathbf{h}_t\\) in Equation F.4; and the ridge-regression matrix, \\(r_v {\\mathbf{I}}\\) (as defined in Note 3.4).\n\nsetParObj &lt;- setParsEESN(curRV ,curNh, n, curM, quadInd)\n\nWe save the forecasts in a three-dimensional array, with the first dimension indexing the ensemble number, the second dimension indexing the forecast time point, and the third dimension indexing the EOF number. We also create a second three-dimensional array with the first two dimensions the same, and the third dimension indexing spatial location.\n\nfmatESNFin &lt;- array(NA, c(ensembleLen, testLen, n))\nfmatESNFinFull &lt;- array(NA,c(ensembleLen, testLen, nspatial))\n\nWe are now ready to run the ensemble of QESN models to obtain forecasts. For each ensemble, we run the function genResR, which takes arguments defined previously as input: curNh, the number of hidden units; wWidth and uWidth, the uniform distribution sampling range for \\(\\mathbf{W}\\) and \\(\\mathbf{U}\\), respectively; piW and piU, the probabilities of non-zeros in \\(\\mathbf{W}\\) and \\(\\mathbf{U}\\), respectively; curNu, the spectral-radius parameter; quadInd, the indicator on whether to include the quadratic output weights or not; DataObj, the embedding input matrices; setParObj, the initializations corresponding to the hidden state vectors and the ridge-regression matrices; and testLen, the number of test cases.\n\nfor(iEnsem in 1:ensembleLen) {\n  ## Run the QESN model for a single ensemble\n  QESNOutObj = genResR(nh = curNh,\n                       wWidth = wWidth,\n                       uWidth = uWidth,\n                       piW = piW,\n                       piU = piU,\n                       nuESN = curNu,\n                       quadInd = quadInd,\n                       DataObj = DataObj,\n                       setParObj = setParObj,\n                       testLen = testLen)\n  \n  ## save forecasts for the reduced dimension output\n  fmatESNFin[iEnsem, , ] &lt;- t(QESNOutObj$unScaledForecasts)\n  \n  ## forecasts for the full spatial field\n  fmatESNFinFull[iEnsem, , ] &lt;- fmatESNFin[iEnsem, , ] %*% t(PHI)\n}\n\n\n\nPost-Processing the Ensemble QESN Output\nIn this section, we focus on post-processing the ensemble QESN output for the Niño 3.4 region. To assess whether or not we have the correct coverage of the prediction intervals, we consider 95% (pointwise) prediction intervals.\n\nalpha &lt;- .05   # alpha-level of 1-alpha pred. intervals (P.I.s)\nlwPI &lt;- alpha/2\n\nIn the following code, we calculate the mean and the lower/upper boundaries of the 95% prediction interval for the Niño 3.4 region (across the whole ensemble of realizations from the predictive distribution).\n\nnino34AvgPreds &lt;- nino34LwPI &lt;- nino34UpPI &lt;- rep(NA, testLen)\nfor(i in 1:testLen){\n  nino34AvgPreds[i] &lt;- fmatESNFinFull[,i,nino34Index] %&gt;%\n    mean()\n  nino34LwPI[i] &lt;- fmatESNFinFull[, i, nino34Index] %&gt;%\n    rowMeans() %&gt;%\n    quantile(lwPI)\n  nino34UpPI[i] &lt;- fmatESNFinFull[,i,nino34Index] %&gt;%\n    rowMeans() %&gt;%\n    quantile(1 - lwPI)\n}\nnino34_results &lt;- data.frame(AvgPres = nino34AvgPreds,\n                             LwPI = nino34LwPI,\n                             UpPI = nino34UpPI)\n\nThese predictive-distribution summaries can be compared to the average SST at the prediction month, which we calculate as follows:\n\nnino34_results$AvgObs &lt;- SSTdata[nino34Index, yTestIndex] %&gt;%\n  colMeans()\n\nFinally, we allocate the prediction-month labels to the data frame which, for this example, is achieved as follows.\n\nnino34_results$date &lt;- seq(as.Date(\"1997-04-01\"),\n                           length.out = 10, by = \"3 months\")\n\n\n\nPlotting Results for Forecasts in the Niño 3.4 Region\nIn this last section, we focus our plots on the results for spatially averaged SST anomalies over the Niño 3.4 region for every third month (to make the plot less cluttered). Although we skip months and present the spatial average for ease of visualization, we note that the full spatial fields could easily be plotted for any of the forecast months, as shown in Lab 5.3. In Figure F.1, we plot the prediction and prediction intervals for the spatial average alongside the spatial average of the observations by month. The following code produces this figure.\n\ngresults &lt;- ggplot(nino34_results) +\n  geom_line(aes(x = date, y = AvgObs)) +\n  geom_ribbon(aes(x = date, ymin = LwPI, ymax = UpPI),\n              alpha = 0.1, fill = \"black\") +\n  geom_line(aes(x = date, y = AvgPres), col = \"red\") +\n  ylab(expression(paste(\"Ni\", tilde(n), \"o 3.4 Index\"))) +\n  xlab(\"Month\") + theme_bw()\n\n\n\n\n\n\n\nFigure F.1: Out-of-sample six-month forecasts of SST anomalies averaged over the Niño 3.4 region from April 1997 to July 1999 (every three months). The black line shows the truth and the red line shows the average of the ensemble of QESN forecasts. The point-wise 95% prediction intervals from the ensemble of QESN forecasts is shown with light-gray shading.\n\n\n\nAlthough pointwise prediction intervals are informative, it can also be helpful to plot the trajectories of individual forecasts from the QESN model. This can be done by first assigning a number (say, the first 15) of ensemble trajectories to the data frame and then putting the data frame into long format using gather. The following code produces Figure F.2.\n\n## Compute the spatial average over Nino3.4 for each ensemble\nfor(i in 1:15)\n  nino34_results[paste0(\"Sim\", i)] &lt;-\n  rowMeans(fmatESNFinFull[i, , nino34Index])\n\n## Convert to long data frame\nnino34_results_long &lt;- nino34_results %&gt;%\n  dplyr::select(-AvgPres, -LwPI,\n                -UpPI, -AvgObs) %&gt;%\n  gather(SimNum, SSTindex, -date)\n\n## Plot\ngresults2 &lt;- ggplot(nino34_results_long) +\n  geom_line(data = nino34_results, aes(x = date, y = AvgObs)) +\n  geom_line(aes(x = date, y = SSTindex, group = SimNum,\n                linetype = SimNum, colour = SimNum)) +\n  ylab(expression(paste(\"Ni\", tilde(n), \"o 3.4 Index\"))) +\n  xlab(\"Month\") + theme_bw() + theme(legend.position = \"none\")\n\n\n\n\n\n\n\nFigure F.2: Out-of-sample six-month forecasts of SST anomalies averaged over the Niño 3.4 region from April 1997 through July 1999 (every three months). The black line shows the truth, and other lines show the first 15 ensemble members from the ensemble of QESN model forecasts.\n\n\n\n\n\n\n\nJaeger, H. (2007). Echo state network. Scholarpedia, 2(9), 2330.\n\n\nMcDermott, P. L., & Wikle, C. K. (2017). An ensemble quadratic echo state network for non-linear spatio-temporal forecasting. Stat, 6(1), 315–330.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>F</span>  <span class='chapter-title'>Case Study: Quadratic Echo State Networks for Sea Surface Temperature Long-Lead Prediction</span>"
    ]
  },
  {
    "objectID": "ChapterRPackages.html",
    "href": "ChapterRPackages.html",
    "title": "List of R packages",
    "section": "",
    "text": "This book would of course not have been possible without the free availability of R (R Core Team, 2018) and some excellent packages. The book itself was compiled using knitr v1.20 (Xie, 2015) while running R v3.4.4. All other R packages that were at some point used throughout the book, together with a brief description of how they were used, are listed below.\n\n\n\n\n\n\n\n\nPackage\nReference\nHow used\n\n\n\n\nanimation v2.5\nXie (2013)\nspatio-temporal visualizations\n\n\nape v5.1\nParadis et al. (2004)\nMoran’s \\(I\\) tests in space and space-time using Moran.I\n\n\nbroom v0.5.0\nRobinson & Hayes (2018)\ncasting the summary outputs of tests into data frames using tidy\n\n\nCCA v1.2\nGonzález & Déjean (2012)\ncanonical correlation analysis of spatio-temporal data using cancor\n\n\ndevtools v1.13.6\nWickham & Chang (2017)\ninstalling STRbook\n\n\ndplyr v0.7.6\nWickham et al. (2017)\ndata-wrangling spatio-temporal data - in particular filtering, sorting, selecting variables and creating new variables\n\n\nEFDR v0.1.1\nZammit-Mangion & Huang (2015)\ncarrying out field significance tests with EFDR\n\n\nexpm v0.999-2\nGoulet et al. (2017)\nraising matrices to a power using %^%\n\n\nfields v9.6\nNychka et al. (2015)\ncomputing distances using rdist and plotting using image.show\n\n\nFRK v0.2.2\nZammit-Mangion (2018a)\nmodeling spatio-temporal data with spatio-temporal basis functions\n\n\nggplot2 v3.0.0\nWickham (2016)\nvisualizing data by plotting facets of line, contour, or raster plots\n\n\nggmap v2.6.1\nKahle & Wickham (2013)\nplotting of regional maps\n\n\nggquiver v0.1.0\nO’Hara-Wild (2017)\nvisualizing directional data with quivers\n\n\ngridExtra v2.3\nAuguie (2016)\narranging ggplot2 plots into a grid using grid.arrange or arrangeGrob\n\n\ngstat v1.1-6\nGräler et al. (2016)\ninverse distance weighting, fitting spatio-temporal semivariograms, and spatio-temporal kriging\n\n\nIDE v0.2.0\nZammit-Mangion (2018b)\nmodeling spatio-temporal data with integro-difference-equation models\n\n\nINLA v18.07.12\nLindgren & Rue (2015)\nmodeling non-Gaussian spatio-temporal data with a separable model\n\n\nlattice v0.20-35\nSarkar (2008)\nsurface plots using wireframe\n\n\nleaps v3.0\nLumley (2017)\nstepwise regression using regsubsets\n\n\nlmtest v0.9-36\nZeileis & Hothorn (2002)\nDurbin–Watson tests using dwtest\n\n\nmaps v3.3.0\nBecker & Wilks (2017)\nreading in state boundaries for plotting using map_data\n\n\nmaptools v0.9-3\nR. Bivand & Lewin-Koh (2017)\nreading in data from shapefiles using readShapePoly\n\n\nMatrix v1.2-14\nBates & Maechler (2017)\nhandling objects of class Matrix\n\n\nmgcv v1.8-23\nWood (2017)\nmodeling non-Gaussian spatio-temporal data using generalized additive models using gamm\n\n\nnlme v3.1-131\nPinheiro et al. (2017)\nfitting linear models with correlated error using gls\n\n\nplyr v1.8.4\nWickham (2011)\nfilling in missing columns during row binding using rbind.fill\n\n\npurrr v0.2.5\nHenry & Wickham (2017)\ncarrying out tests on groups of data using map\n\n\nRColorBrewer v1.1-2\nNeuwirth (2014)\ngenerating a color palette using brewer.pal\n\n\nscoringRules v0.9.5\nJordan et al. (2017)\nmultivariate probabilistic validation using es_sample and vs_sample\n\n\nsp v1.3-1\nR. S. Bivand et al. (2013)\ncreating and handling spatial objects such as SpatialPoints or SpatialPolygons\n\n\nspacetime v1.2-2\nPebesma (2012)\ncreating and handling spatio-temporal objects such as STIDF or STFDF\n\n\nSpatialVx v0.6-3\nGilleland (2018)\nfield-matching methods for assessing predictions\n\n\nSpatioTemporal v1.1.7\nLindstrom et al. (2013)\nmodeling spatio-temporal data with temporal basis functions\n\n\nSpecsVerification v0.5-2\nSiegert (2017)\nplotting verification rank histograms\n\n\nstargazer v5.2.2\nHlavac (2015)\ngenerating ~tables from the results of standard tests\n\n\nSTRbook v0.1.0\nZammit-Mangion (2018c)\ncompanion package for this book, containing several data sets and helper functions. Needs to be installed using devtools through install_github(\"andrewzm/STRbook\")\n\n\ntidyr v0.8.1\nWickham & Henry (2017)\ndata-wrangling spatio-temporal data - in particular, going from space-wide or time-wide to long formats, and nesting and unnesting data frames\n\n\nverification v1.42\nNCAR – Research Applications Laboratory (2015)\nprobabilistic validation using the continuous ranked probability score with the function crps\n\n\nxtable v1.8-2\nDahl (2016)\ngenerating ~tables from data frames\n\n\n\n\n\n\n\nAuguie, B. (2016). gridExtra: Miscellaneous functions for “grid” graphics.\n\n\nBates, D., & Maechler, M. (2017). Matrix: Sparse and dense matrix classes and methods.\n\n\nBecker, R. A., & Wilks, A. R. (2017). Maps: Draw geographical maps.\n\n\nBivand, R. S., Pebesma, E., & Gomez-Rubio, V. (2013). Applied spatial data analysis with R (second). Springer. http://www.asdar-book.org/\n\n\nBivand, R., & Lewin-Koh, N. (2017). Maptools: Tools for reading and handling spatial objects.\n\n\nDahl, D. B. (2016). Xtable: Export tables to LaTeX or HTML.\n\n\nGilleland, E. (2018). SpatialVx: Spatial forecast verification.\n\n\nGonzález, I., & Déjean, S. (2012). CCA: Canonical correlation analysis.\n\n\nGoulet, V., Dutang, C., Maechler, M., Firth, D., Shapira, M., & Stadelmann, M. (2017). Expm: Matrix exponential, log, “etc”.\n\n\nGräler, B., Pebesma, E., & Heuvelink, G. (2016). Spatio-temporal interpolation using gstat. R Journal, 8, 204–218. https://journal.r-project.org/archive/2016-1/na-pebesma-heuvelink.pdf\n\n\nHenry, L., & Wickham, H. (2017). Purrr: Functional programming tools.\n\n\nHlavac, M. (2015). Stargazer: Well-formatted regression and summary statistics tables. Harvard University.\n\n\nJordan, A., Krueger, F., & Lerch, S. (2017). scoringRules: Scoring rules for parametric and simulated distribution forecasts.\n\n\nKahle, D., & Wickham, H. (2013). Ggmap: Spatial visualization with ggplot2. R Journal, 5(1), 144–161. http://journal.r-project.org/archive/2013-1/kahle-wickham.pdf\n\n\nLindgren, F., & Rue, H. (2015). Bayesian spatial modelling with R-INLA. Journal of Statistical Software, 63(19), 1–25. http://www.jstatsoft.org/v63/i19/\n\n\nLindstrom, J., Szpiro, A., Sampson, P. D., Bergen, S., & Oron, A. P. (2013). SpatioTemporal: Spatio-temporal model estimation.\n\n\nLumley, T. (2017). Leaps: Regression subset selection.\n\n\nNCAR – Research Applications Laboratory. (2015). Verification: Weather forecast verification utilities.\n\n\nNeuwirth, E. (2014). RColorBrewer: ColorBrewer palettes.\n\n\nNychka, D., Furrer, R., Paige, J., & Sain, S. (2015). Fields: Tools for spatial data.\n\n\nO’Hara-Wild, M. (2017). Ggquiver: Quiver plots for ’ggplot2’.\n\n\nParadis, E., Claude, J., & Strimmer, K. (2004). APE: Analyses of phylogenetics and evolution in R language. Bioinformatics, 20, 289–290.\n\n\nPebesma, E. (2012). spacetime: Spatio-temporal data in R. Journal of Statistical Software, 51(7), 1–30. http://www.jstatsoft.org/v51/i07/\n\n\nPinheiro, J., Bates, D., DebRoy, S., Sarkar, D., & R Core Team. (2017). nlme: Linear and nonlinear mixed effects models.\n\n\nR Core Team. (2018). R: A language and environment for statistical computing. R Foundation for Statistical Computing. https://www.R-project.org/\n\n\nRobinson, D., & Hayes, A. (2018). Broom: Convert statistical analysis objects into tidy tibbles. https://CRAN.R-project.org/package=broom\n\n\nSarkar, D. (2008). Lattice: Multivariate data visualization with r. Springer. http://lmdvr.r-forge.r-project.org\n\n\nSiegert, S. (2017). SpecsVerification: Forecast verification routines for ensemble forecasts of weather and climate.\n\n\nWickham, H. (2011). The split-apply-combine strategy for data analysis. Journal of Statistical Software, 40(1), 1–29. http://www.jstatsoft.org/v40/i01/\n\n\nWickham, H. (2016). ggplot2: Elegant graphics for data analysis (2nd ed.). Springer.\n\n\nWickham, H., & Chang, W. (2017). Devtools: Tools to make developing r packages easier.\n\n\nWickham, H., Francois, R., Henry, L., & Müller, K. (2017). Dplyr: A grammar of data manipulation.\n\n\nWickham, H., & Henry, L. (2017). Tidyr: Easily tidy data with “spread()” and “gather()” functions.\n\n\nWood, S. N. (2017). Generalized additive models: An introduction with r (2nd ed.). Chapman & Hall/CRC.\n\n\nXie, Y. (2013). animation: An R package for creating animations and demonstrating statistical methods. Journal of Statistical Software, 53(1), 1–27. http://www.jstatsoft.org/v53/i01/\n\n\nXie, Y. (2015). Dynamic documents with r and knitr (second). Chapman & Hall/CRC.\n\n\nZammit-Mangion, A. (2018a). FRK: Fixed rank kriging.\n\n\nZammit-Mangion, A. (2018b). IDE: Integro-difference equation spatio-temporal models.\n\n\nZammit-Mangion, A. (2018c). STRbook: Supplementary package for book on ST modelling with r.\n\n\nZammit-Mangion, A., & Huang, H.-C. (2015). EFDR: Wavelet-based enhanced FDR for signal detection in noisy images. https://CRAN.R-project.org/package=EFDR\n\n\nZeileis, A., & Hothorn, T. (2002). Diagnostic checking in regression relationships. R News, 2(3), 7–10. https://CRAN.R-project.org/doc/Rnews/",
    "crumbs": [
      "Appendices",
      "List of `R` packages"
    ]
  }
]